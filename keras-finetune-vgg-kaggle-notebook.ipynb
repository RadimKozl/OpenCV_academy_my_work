{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of keras_finetune_vgg.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94741,"sourceType":"datasetVersion","datasetId":50721}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# <font style=\"color:rgb(50,120,229)\">Transfer Learning and Fine-tuning </font>\nIn this chapter, we will learn how to fine-tune a pre-trained model for a different task than it was originally trained for.\n\nWhen we train a network from scratch, we encounter the following two limitations :\n\n- Huge data required - Since the network has millions of parameters, to get an optimal set of parameters, we need to have a lot of data.\n- Huge computing power required - Even if we have a lot of data, training generally requires multiple iterations and it takes a toll on the computing resources.\n\nThe pre-trained models are trained on very large scale image classification problems. The convolutional layers act as feature extractor and the fully connected layers act as Classifiers.\n\nSince these models are very large and have seen a huge number of images, they tend to learn very good, discriminative features. We can either use the convolutional layers merely as a feature extractor and change the last layer according to our problem or we can tweak the already trained convolutional layers to suit our problem at hand. The former approach is known as **Transfer Learning** and the latter as **Fine-tuning**.\n\nThe task of fine-tuning a network is to tweak the parameters of an already trained network so that it adapts to the new task at hand. The initial layers of a network learn very general features and as we go higher up the network, the layers tend to learn patterns more specific to the task it is being trained on. Thus, for fine-tuning, we want to keep the initial layers intact ( or freeze them ) and retrain the later layers for our task.\n\nThus, fine-tuning avoids both the limitations discussed above.\n\nThe amount of data required for training is not much because of two reasons. \n- First, we are not training the entire network. Second, the part that is being trained is not trained from scratch.\n- Since the parameters that need to be updated is less, the amount of time needed will also be less.\n\nAs a rule of thumb, when we have a small training set and our problem is similar to the task for which the pre-trained models were trained, we can use transfer learning. If we have enough data, we can try and tweak the convolutional layers so that they learn more robust features relevant to our problem. You can get a detailed overview of Fine-tuning and transfer learning [here](http://cs231n.github.io/transfer-learning/).","metadata":{"id":"qfHId531-K4V"}},{"cell_type":"code","source":"\nimport tensorflow as tf\nprint(tf.__version__)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\nimport os,shutil","metadata":{"id":"JB8MoYqlLfGe","outputId":"de07a789-8d5e-48a9-f661-825bb33b94d3","execution":{"iopub.status.busy":"2024-06-18T06:53:12.692997Z","iopub.execute_input":"2024-06-18T06:53:12.693338Z","iopub.status.idle":"2024-06-18T06:53:25.720213Z","shell.execute_reply.started":"2024-06-18T06:53:12.693311Z","shell.execute_reply":"2024-06-18T06:53:25.719251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Loading kaggle dataset</font>\nWe have already discussed how to load kaggle datasets in the previous section. We will be using the [kaggle dataset](https://www.kaggle.com/sriramr/apples-bananas-oranges) on fruits. The interesting thing about this dataset is is that it contains 6 classes - 3 of which belong to normal fruits and 3 to rotten fruits!\n\nNOTE: You will need to use your API token from kaggle ( kaggle.json file ) for downloading the kaggle dataset on colab. You can also upload the dataset manually to your google drive for doing the experiment.","metadata":{"id":"yzIHAV2INW7m"}},{"cell_type":"code","source":"# This folder contians all the class folders\nmain_path = '/kaggle/input/apples-bananas-oranges/original_data_set/original_data_set/'","metadata":{"id":"H_rY_x8AMzUE","execution":{"iopub.status.busy":"2024-06-18T06:53:25.721854Z","iopub.execute_input":"2024-06-18T06:53:25.722809Z","iopub.status.idle":"2024-06-18T06:53:25.727505Z","shell.execute_reply.started":"2024-06-18T06:53:25.722773Z","shell.execute_reply":"2024-06-18T06:53:25.726511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all class names\nclassnames = os.listdir(main_path)\nprint(classnames)","metadata":{"id":"vkZ-FKuDM2Jx","outputId":"2be4dacf-72c0-4699-f367-3f1e45f4e709","execution":{"iopub.status.busy":"2024-06-18T06:53:27.197754Z","iopub.execute_input":"2024-06-18T06:53:27.198397Z","iopub.status.idle":"2024-06-18T06:53:27.221663Z","shell.execute_reply.started":"2024-06-18T06:53:27.198365Z","shell.execute_reply":"2024-06-18T06:53:27.220808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See how many images you have per class, this helps you set the required percentage of validation data.\nfor each_class in classnames:\n  print(\"Class: {}, has {} samples\".format( each_class,len(os.listdir(os.path.join(main_path,each_class )))))","metadata":{"id":"nyVpV0xwNA7k","outputId":"6ec29d26-fb8b-472f-99fa-5286cb070195","scrolled":true,"execution":{"iopub.status.busy":"2024-06-18T06:53:29.132653Z","iopub.execute_input":"2024-06-18T06:53:29.132999Z","iopub.status.idle":"2024-06-18T06:53:30.147181Z","shell.execute_reply.started":"2024-06-18T06:53:29.132973Z","shell.execute_reply":"2024-06-18T06:53:30.146281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Arrange your Data</font>\nWe have already explained in the previous section how the data should be arranged in order to feed it to a training pipeline in Keras.\n\nHere we are going to place the data neatly in train and validation folders inside a main folder called 'fruits' (change the name if you want), you can set the percentage of validation set. We have set it to 15%. So 15% of images from each class will go to validation folder and the rest to train folder.","metadata":{"id":"L2v6UvQnNHbo"}},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\n\nbase_dir= '/kaggle/working/fruits'\n\ntotal_train_images = 0\ntotal_val_images = 0\n\nos.mkdir(base_dir)\n\ntrain_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\n\nvalidation_dir = os.path.join(base_dir, 'validation') \nos.mkdir(validation_dir)\n\n# Set the percent of validation data you want.\nvalidation_percent = 15\n\nfor each_class in classnames:\n\n  source_directory = os.path.join(main_path, each_class)\n  destination_train_directory = os.path.join(train_dir, each_class)\n  destination_validation_directory = os.path.join(validation_dir, each_class)\n\n  total_image_count = len([name for name in os.listdir(source_directory) if os.path.isfile(os.path.join(source_directory, name))])\n\n  valid_image_count = int(np.floor(total_image_count * (validation_percent / 100)))\n\n  train_images_count = int(total_image_count - valid_image_count)\n\n  total_train_images += train_images_count\n  total_val_images += valid_image_count\n\n  os.mkdir(destination_train_directory)\n  os.mkdir(destination_validation_directory)\n\n  # copying the data to class's train folder\n  file_names = [name for name in os.listdir(source_directory) if os.path.isfile(os.path.join(source_directory, name))][:train_images_count]\n\n  for fname in file_names:\n      src = os.path.join(source_directory, fname)\n      dst = os.path.join(destination_train_directory, fname)\n      shutil.copyfile(src, dst)\n\n  # Copying the data to class's validation folder\n  file_names = [name for name in os.listdir(source_directory) if os.path.isfile(os.path.join(source_directory, name))][train_images_count:]\n\n  for fname in file_names:\n      src = os.path.join(source_directory, fname)\n      dst = os.path.join(destination_validation_directory, fname)\n      shutil.copyfile(src, dst)\n  \n  print('total training {} images: {}'.format(each_class, len(os.listdir(destination_train_directory))))\n  print('total validation {} images: {}'.format(each_class, len(os.listdir(destination_validation_directory))))\n","metadata":{"id":"4oDZVqpONBtA","outputId":"63483443-d5a0-4678-8cc6-72c83e556141","execution":{"iopub.status.busy":"2024-06-18T06:53:32.163655Z","iopub.execute_input":"2024-06-18T06:53:32.163997Z","iopub.status.idle":"2024-06-18T06:53:48.093293Z","shell.execute_reply.started":"2024-06-18T06:53:32.163971Z","shell.execute_reply":"2024-06-18T06:53:48.092412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Freeze the required layers</font>\nIn Keras, each layer has a parameter called “trainable”. For freezing the weights of a particular layer, we should set this parameter to `False`, indicating that this layer should not be trained. That’s it! We go over each layer and select which layers we want to train.\n\n```\n# Freeze the layers except the last 4 layers\nfor layer in vgg_conv.layers[:-4]:\n    layer.trainable = False\n```","metadata":{"id":"xhQLJGt2-K4w"}},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Experiment 1: Freezing all layers - Same as Transfer Learning</font>","metadata":{"id":"PcUHACyzLfHp"}},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16\n\nimage_size = 224\n\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\n","metadata":{"id":"odTHYm39sEJ2","execution":{"iopub.status.busy":"2024-06-18T06:54:29.992313Z","iopub.execute_input":"2024-06-18T06:54:29.992750Z","iopub.status.idle":"2024-06-18T06:54:29.998084Z","shell.execute_reply.started":"2024-06-18T06:54:29.992717Z","shell.execute_reply":"2024-06-18T06:54:29.997007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Load the VGG model\nvgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n#include_topはFully Connected Layerはロードしないという意味\n\n# Freeze all the layers\nfor layer in vgg_conv.layers[:]:\n    layer.trainable = False\n\n# Check the trainable status of the individual layers\nfor layer in vgg_conv.layers:\n    print(layer, layer.trainable)\n\n\n\n# Create the model\nmodel = models.Sequential()\n\n# Add the vgg convolutional base model\nmodel.add(vgg_conv)\n\n# Add new layers\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(len(classnames), activation='softmax'))\n\n# Show a summary of the model. Check the number of trainable parameters\nmodel.summary()","metadata":{"id":"jva8MvWuLfHq","outputId":"2054fa32-7cfe-423f-e450-095eeba4e6a2","execution":{"iopub.status.busy":"2024-06-18T06:54:34.211123Z","iopub.execute_input":"2024-06-18T06:54:34.212256Z","iopub.status.idle":"2024-06-18T06:54:38.249858Z","shell.execute_reply.started":"2024-06-18T06:54:34.212220Z","shell.execute_reply":"2024-06-18T06:54:38.248954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Setup the data generators</font>\n\nWe have already separated the data into train and validation and kept it in the “train” and “validation” folders. We can use ImageDataGenerator available in Keras to read images in batches directly from these folders and optionally perform data augmentation. We will use two different data generators for train and validation folders.","metadata":{"id":"0BjJ0uZ3LfHt"}},{"cell_type":"code","source":"# No Data augmentation \ntrain_datagen = ImageDataGenerator(rescale=1./255)\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\n\n# Change the batchsize according to your system RAM\ntrain_batchsize = 20\nval_batchsize = 20\n\n# Data Generator for Training data\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(image_size, image_size),\n        batch_size=train_batchsize,\n        class_mode='categorical')\n\n# Data Generator for Validation data\nvalidation_generator = validation_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(image_size, image_size),\n        batch_size=val_batchsize,\n        class_mode='categorical',\n        shuffle=False)","metadata":{"id":"-izOmqvbDI7H","outputId":"28d13a9e-7159-4ca6-e5e3-dd988db5f947","execution":{"iopub.status.busy":"2024-06-18T06:54:49.462572Z","iopub.execute_input":"2024-06-18T06:54:49.462941Z","iopub.status.idle":"2024-06-18T06:54:49.535750Z","shell.execute_reply.started":"2024-06-18T06:54:49.462916Z","shell.execute_reply":"2024-06-18T06:54:49.534814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Train the model</font>\nTill now, we have created the model and set up the data for training. So, we should proceed with the training and check out the performance. We will have to specify the optimizer and the learning rate and start training using the model.fit() function. After the training is over, we will save the model.","metadata":{"id":"m__N-xPIDLCl"}},{"cell_type":"code","source":"# Compile the model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n              metrics=['acc'])\n\n# Train the Model\nhistory = model.fit(\n      train_generator,\n      steps_per_epoch=int(train_generator.samples/train_generator.batch_size) ,\n      epochs=20,\n      validation_data=validation_generator,\n      validation_steps=int(validation_generator.samples/validation_generator.batch_size),\n      verbose=1)\n\n","metadata":{"id":"4sWzFSVQDCkN","outputId":"fac6e7e5-2da8-4a06-9356-4fdaef135fab","execution":{"iopub.status.busy":"2024-06-18T07:05:59.893529Z","iopub.execute_input":"2024-06-18T07:05:59.894259Z","iopub.status.idle":"2024-06-18T07:07:59.789006Z","shell.execute_reply.started":"2024-06-18T07:05:59.894230Z","shell.execute_reply":"2024-06-18T07:07:59.788102Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Save the model</font>\nKeras models are saved in h5 format. You can just use the save method along with the name of the file you want.\n","metadata":{"id":"OnwP5BnME0G5"}},{"cell_type":"code","source":"# Save the Model\nmodel.save('/kaggle/working/all_freezed.h5')","metadata":{"id":"VROrTe9jOqhF","execution":{"iopub.status.busy":"2024-06-18T07:12:01.598318Z","iopub.execute_input":"2024-06-18T07:12:01.598724Z","iopub.status.idle":"2024-06-18T07:12:01.790246Z","shell.execute_reply.started":"2024-06-18T07:12:01.598692Z","shell.execute_reply":"2024-06-18T07:12:01.789445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Check Performance</font>\n\nLet us see the loss and accuracy curves.","metadata":{"id":"dutF19eCDZe6"}},{"cell_type":"code","source":"# Plot the accuracy and loss curves\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"id":"iXKs7OdoOnUf","outputId":"75a01fde-0796-49c3-db28-62fbab240503","execution":{"iopub.status.busy":"2024-06-18T07:16:09.649430Z","iopub.execute_input":"2024-06-18T07:16:09.650273Z","iopub.status.idle":"2024-06-18T07:16:10.288052Z","shell.execute_reply.started":"2024-06-18T07:16:09.650244Z","shell.execute_reply":"2024-06-18T07:16:10.287127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Show the errors</font>\nAlso, let us visually see the errors that we got.\n","metadata":{"id":"SaF619WQLfHy"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import load_img\nimport math\n\n# Create a generator for prediction\nvalidation_generator = validation_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(image_size, image_size),\n        batch_size=val_batchsize,\n        class_mode='categorical',\n        shuffle=False)\n\n# Get the filenames from the generator\nfnames = validation_generator.filenames\n\n# Get the ground truth from generator\nground_truth = validation_generator.classes\n\n# Get the label to class mapping from the generator\nlabel2index = validation_generator.class_indices\n\n# Getting the mapping from class index to class label\nidx2label = dict((v, k) for k, v in label2index.items())\n\n# Calculate the correct number of steps\nsteps = math.ceil(validation_generator.samples / validation_generator.batch_size)\n\n# Get the predictions from the model using the generator\npredictions = model.predict(validation_generator, steps=steps, verbose=1)\npredicted_classes = np.argmax(predictions, axis=1)\n\n# Ensure the lengths match\npredicted_classes = predicted_classes[:validation_generator.samples]\n\n# Find the errors\nerrors = np.where(predicted_classes != ground_truth)[0]\nprint(\"No of errors = {}/{}\".format(len(errors), validation_generator.samples))\n\n# Show the errors\nfor i in range(len(errors)):\n    pred_class = np.argmax(predictions[errors[i]])\n    pred_label = idx2label[pred_class]\n    \n    title = 'Original label: {}, Prediction: {}, confidence: {:.3f}'.format(\n        fnames[errors[i]].split('/')[0],\n        pred_label,\n        predictions[errors[i]][pred_class])\n    \n    original = load_img('{}/{}'.format(validation_dir, fnames[errors[i]]))\n    plt.figure(figsize=[7, 7])\n    plt.axis('off')\n    plt.title(title)\n    plt.imshow(original)\n    plt.show()","metadata":{"id":"8lBv849VLfHz","outputId":"27499803-e941-4685-c605-6a20467edabf","execution":{"iopub.status.busy":"2024-06-18T07:20:42.782739Z","iopub.execute_input":"2024-06-18T07:20:42.783458Z","iopub.status.idle":"2024-06-18T07:20:48.404545Z","shell.execute_reply.started":"2024-06-18T07:20:42.783426Z","shell.execute_reply":"2024-06-18T07:20:48.403410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the test loss is not converging even thought the training loss keeps decreasing. Let us try to train some more layers (Fine-tuning)","metadata":{"id":"HUN0VMQXGkQq"}},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Experiment 2 : Train Last 4 layers without data augmentation</font>\n\nIn this experiment, we will keep the initial layers fixed and only retrain the last 4 layers. Since the initial layers learn more general features, it is a good practice to freeze the initial layers while fine-tuning the latter layers of the network for the new task.","metadata":{"id":"G-gSnfe9LfH2"}},{"cell_type":"code","source":"#Load the VGG model\nvgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n\n# Freeze all layers except the last 4\nfor layer in vgg_conv.layers[:-4]:\n    layer.trainable = False\n\n# Check the trainable status of the individual layers\nfor layer in vgg_conv.layers:\n    print(layer, layer.trainable)\n\n# Create the model\nmodel = models.Sequential()\n\n# Add the vgg convolutional base model\nmodel.add(vgg_conv)\n\n# Add new layers\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(len(classnames), activation='softmax'))\n\n# Show a summary of the model. Check the number of trainable parameters\nmodel.summary()","metadata":{"id":"J-PQPdbYLfH3","outputId":"44538fe2-3fc6-4e3a-ac54-f352cd01678f","execution":{"iopub.status.busy":"2024-06-18T07:24:29.934690Z","iopub.execute_input":"2024-06-18T07:24:29.935055Z","iopub.status.idle":"2024-06-18T07:24:30.210970Z","shell.execute_reply.started":"2024-06-18T07:24:29.935027Z","shell.execute_reply":"2024-06-18T07:24:30.210016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Train the model</font>","metadata":{"id":"71AWktPcLfH5"}},{"cell_type":"code","source":"# No Data augmentation \ntrain_datagen = ImageDataGenerator(rescale=1./255)\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\n\n# Change the batchsize according to your system RAM\ntrain_batchsize = 50\nval_batchsize = 20\n\n# Data Generator for Training data\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(image_size, image_size),\n        batch_size=train_batchsize,\n        class_mode='categorical')\n\n# Data Generator for Validation data\nvalidation_generator = validation_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(image_size, image_size),\n        batch_size=val_batchsize,\n        class_mode='categorical',\n        shuffle=False)\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n              metrics=['acc'])\n\n# Train the Model\nhistory = model.fit(\n      train_generator,\n      steps_per_epoch=int(train_generator.samples/train_generator.batch_size) ,\n      epochs=20,\n      validation_data=validation_generator,\n      validation_steps=int(validation_generator.samples/validation_generator.batch_size),\n      verbose=1)\n\n","metadata":{"id":"aF6raPLzLfH6","outputId":"0c183327-b2ad-41eb-e358-2117c3ce2a2a","execution":{"iopub.status.busy":"2024-06-18T07:24:41.174032Z","iopub.execute_input":"2024-06-18T07:24:41.174639Z","iopub.status.idle":"2024-06-18T07:28:22.694327Z","shell.execute_reply.started":"2024-06-18T07:24:41.174608Z","shell.execute_reply":"2024-06-18T07:28:22.693450Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the Model\nmodel.save('/kaggle/working/last4_layers.h5')\n\n# you can also download the model to your pc from the files tab on the left","metadata":{"id":"Xv1hUtRFR0u4","execution":{"iopub.status.busy":"2024-06-18T07:32:43.464522Z","iopub.execute_input":"2024-06-18T07:32:43.464917Z","iopub.status.idle":"2024-06-18T07:32:43.667602Z","shell.execute_reply.started":"2024-06-18T07:32:43.464886Z","shell.execute_reply":"2024-06-18T07:32:43.666624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the accuracy and loss curves\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"id":"xlf__PR7R1Ay","outputId":"4cd2d8ea-1b84-4ede-c0a2-b5fec2dca8f1","execution":{"iopub.status.busy":"2024-06-18T07:42:52.191001Z","iopub.execute_input":"2024-06-18T07:42:52.191378Z","iopub.status.idle":"2024-06-18T07:42:52.745252Z","shell.execute_reply.started":"2024-06-18T07:42:52.191349Z","shell.execute_reply":"2024-06-18T07:42:52.744264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Show the errors</font>\n\n","metadata":{"id":"9pbonc-sLfH-"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import load_img\nimport math\n\n# Create a generator for prediction\nvalidation_generator = validation_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(image_size, image_size),\n        batch_size=val_batchsize,\n        class_mode='categorical',\n        shuffle=False)\n\n# Get the filenames from the generator\nfnames = validation_generator.filenames\n\n# Get the ground truth from generator\nground_truth = validation_generator.classes\n\n# Get the label to class mapping from the generator\nlabel2index = validation_generator.class_indices\n\n# Getting the mapping from class index to class label\nidx2label = dict((v, k) for k, v in label2index.items())\n\n# Calculate the correct number of steps\nsteps = math.ceil(validation_generator.samples / validation_generator.batch_size)\n\n# Get the predictions from the model using the generator\npredictions = model.predict(validation_generator, steps=steps, verbose=1)\npredicted_classes = np.argmax(predictions, axis=1)\n\n# Ensure the lengths match\npredicted_classes = predicted_classes[:validation_generator.samples]\n\n# Find the errors\nerrors = np.where(predicted_classes != ground_truth)[0]\nprint(\"No of errors = {}/{}\".format(len(errors), validation_generator.samples))\n\n# Show the errors\nfor i in range(len(errors)):\n    pred_class = np.argmax(predictions[errors[i]])\n    pred_label = idx2label[pred_class]\n    \n    title = 'Original label: {}, Prediction: {}, confidence: {:.3f}'.format(\n        fnames[errors[i]].split('/')[0],\n        pred_label,\n        predictions[errors[i]][pred_class])\n    \n    original = load_img('{}/{}'.format(validation_dir, fnames[errors[i]]))\n    plt.figure(figsize=[7, 7])\n    plt.axis('off')\n    plt.title(title)\n    plt.imshow(original)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-18T07:45:27.658906Z","iopub.execute_input":"2024-06-18T07:45:27.659671Z","iopub.status.idle":"2024-06-18T07:45:33.832647Z","shell.execute_reply.started":"2024-06-18T07:45:27.659639Z","shell.execute_reply":"2024-06-18T07:45:33.831457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Experiment 3 : Train last 8 layers with data augmentation</font>\n\nWe do this experiment to check if we can get even better results than the previous experiment. We train more number of layers and also use data augmentation while training.","metadata":{"id":"PH2XF5EqLfIB"}},{"cell_type":"code","source":"\n#Load the VGG model\nvgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n\n# Freeze all the layers\nfor layer in vgg_conv.layers[:-8]:\n    layer.trainable = False\n\n# Check the trainable status of the individual layers\nfor layer in vgg_conv.layers:\n    print(layer, layer.trainable)\n\n# Create the model\nmodel = models.Sequential()\n\n# Add the vgg convolutional base model\nmodel.add(vgg_conv)\n\n# Add new layers\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(len(classnames), activation='softmax'))\n\n# Show a summary of the model. Check the number of trainable parameters\nmodel.summary()","metadata":{"id":"VNeQc1BXLfIC","outputId":"748ee37f-e9ae-42f6-858d-348a8b4af5b3","execution":{"iopub.status.busy":"2024-06-18T07:47:34.574058Z","iopub.execute_input":"2024-06-18T07:47:34.574816Z","iopub.status.idle":"2024-06-18T07:47:34.847578Z","shell.execute_reply.started":"2024-06-18T07:47:34.574784Z","shell.execute_reply":"2024-06-18T07:47:34.846708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Train the model</font>\nHere we will be using the imageDataGenerator for data augmentation.","metadata":{"id":"Kkg4cATSLfIF"}},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n      rescale=1./255,\n      rotation_range=20,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\n\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\n\n# Change the batchsize according to your system RAM\ntrain_batchsize = 50\nval_batchsize = 20\n\n# Data Generator for Training data\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(image_size, image_size),\n        batch_size=train_batchsize,\n        class_mode='categorical')\n\n# Data Generator for Validation data\nvalidation_generator = validation_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(image_size, image_size),\n        batch_size=val_batchsize,\n        class_mode='categorical',\n        shuffle=False)\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n              metrics=['acc'])\n\n# Train the Model\n# NOTE that we have multiplied the steps_per_epoch by 2. This is because we are using data augmentation.\nhistory = model.fit(\n      train_generator,\n      steps_per_epoch=int(train_generator.samples/train_generator.batch_size) ,\n      epochs=100,\n      validation_data=validation_generator,\n      validation_steps=int(validation_generator.samples/validation_generator.batch_size),\n      verbose=1)\n\n","metadata":{"id":"L1-oMd0nLfIG","outputId":"2104ebe8-dff4-4a5e-935c-7881703c2dd3","scrolled":true,"execution":{"iopub.status.busy":"2024-06-18T07:55:33.565257Z","iopub.execute_input":"2024-06-18T07:55:33.565894Z","iopub.status.idle":"2024-06-18T08:17:58.584165Z","shell.execute_reply.started":"2024-06-18T07:55:33.565863Z","shell.execute_reply":"2024-06-18T08:17:58.583156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the Model\nmodel.save('/kaggle/working/da_last8_layers.h5')\n\n# You can also download it from the files tab on the left","metadata":{"id":"zRal0iIVUzzW","execution":{"iopub.status.busy":"2024-06-18T08:18:18.585456Z","iopub.execute_input":"2024-06-18T08:18:18.586095Z","iopub.status.idle":"2024-06-18T08:18:18.937175Z","shell.execute_reply.started":"2024-06-18T08:18:18.586065Z","shell.execute_reply":"2024-06-18T08:18:18.935935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the accuracy and loss curves\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"id":"wLsdzZA1Uz5i","outputId":"c027e694-07ff-468a-f13a-3262f8e83105","execution":{"iopub.status.busy":"2024-06-18T08:18:24.003477Z","iopub.execute_input":"2024-06-18T08:18:24.004333Z","iopub.status.idle":"2024-06-18T08:18:24.604245Z","shell.execute_reply.started":"2024-06-18T08:18:24.004301Z","shell.execute_reply":"2024-06-18T08:18:24.603362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Show the errors</font>\n\n","metadata":{"id":"qAT21JABLfIJ"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import load_img\nimport math\n\n# Create a generator for prediction\nvalidation_generator = validation_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(image_size, image_size),\n        batch_size=val_batchsize,\n        class_mode='categorical',\n        shuffle=False)\n\n# Get the filenames from the generator\nfnames = validation_generator.filenames\n\n# Get the ground truth from generator\nground_truth = validation_generator.classes\n\n# Get the label to class mapping from the generator\nlabel2index = validation_generator.class_indices\n\n# Getting the mapping from class index to class label\nidx2label = dict((v, k) for k, v in label2index.items())\n\n# Calculate the correct number of steps\nsteps = math.ceil(validation_generator.samples / validation_generator.batch_size)\n\n# Get the predictions from the model using the generator\npredictions = model.predict(validation_generator, steps=steps, verbose=1)\npredicted_classes = np.argmax(predictions, axis=1)\n\n# Ensure the lengths match\npredicted_classes = predicted_classes[:validation_generator.samples]\n\n# Find the errors\nerrors = np.where(predicted_classes != ground_truth)[0]\nprint(\"No of errors = {}/{}\".format(len(errors), validation_generator.samples))\n\n# Show the errors\nfor i in range(len(errors)):\n    pred_class = np.argmax(predictions[errors[i]])\n    pred_label = idx2label[pred_class]\n    \n    title = 'Original label: {}, Prediction: {}, confidence: {:.3f}'.format(\n        fnames[errors[i]].split('/')[0],\n        pred_label,\n        predictions[errors[i]][pred_class])\n    \n    original = load_img('{}/{}'.format(validation_dir, fnames[errors[i]]))\n    plt.figure(figsize=[7, 7])\n    plt.axis('off')\n    plt.title(title)\n    plt.imshow(original)\n    plt.show()","metadata":{"id":"_mlqAyZULfIK","outputId":"91ad4871-8cbe-4a2a-eb1b-9c89d6093abd","execution":{"iopub.status.busy":"2024-06-18T08:19:35.341714Z","iopub.execute_input":"2024-06-18T08:19:35.342629Z","iopub.status.idle":"2024-06-18T08:19:39.875856Z","shell.execute_reply.started":"2024-06-18T08:19:35.342595Z","shell.execute_reply":"2024-06-18T08:19:39.874923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In experiment 2, there were signs of overfitting, which are reduced to an extent by using data augmentation. You should try and run for more number of epochs to see the effect of data augmentation.","metadata":{"id":"4ukYyuUiHPhT"}},{"cell_type":"markdown","source":"## <font style=\"color:rgb(50,120,229)\">Exercise</font>\n\n1. Try to use a different dataset and see if the same code and network works for the new dataset.\n1. Try unfreezing more layers and see how the accuracy improves.\n1. Try with some different model instead of VGG","metadata":{"id":"rV9NjgyEHQW9"}}]}