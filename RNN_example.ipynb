{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***<font style=\"color:black\">Base example of RNN nets</font>***\n-------------------\n---------------","metadata":{}},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Installation and Imports libraries</font>**\n-------------------","metadata":{}},{"cell_type":"markdown","source":"@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    input_size: int = 28\n    sequence_length: int = 28\n    num_layers: int = 2\n    hidden_size: int = 256\n    num_classes: int = 10\n    learning_rate: float = 0.001\n    log_interval: int = 100\n    test_interval: int = 1\n    batch_size: int = 64\n    num_epochs: int = 5\n    dir_root: str = os.path.join('/kaggle','working')\n    data_root: str = os.path.join(dir_root,'dataset')\n    num_workers: int = 10  \n    device: str = 'cuda'  ","metadata":{"execution":{"iopub.status.busy":"2025-02-18T20:19:56.528653Z","iopub.execute_input":"2025-02-18T20:19:56.529034Z","iopub.status.idle":"2025-02-18T20:19:56.616935Z","shell.execute_reply.started":"2025-02-18T20:19:56.528999Z","shell.execute_reply":"2025-02-18T20:19:56.615564Z"}}},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">System settings</font>**\n-----------------------------------------------","metadata":{}},{"cell_type":"code","source":"def setup_system(system_config: SystemConfiguration) -> None:\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:28.454350Z","iopub.execute_input":"2025-02-18T20:09:28.454756Z","iopub.status.idle":"2025-02-18T20:09:28.465628Z","shell.execute_reply.started":"2025-02-18T20:09:28.454728Z","shell.execute_reply":"2025-02-18T20:09:28.464908Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Create model</font>**\n------------------------","metadata":{}},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Recurrent neural network (many-to-one) model</font>**\n","metadata":{}},{"cell_type":"code","source":"# Create RNN Network\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, sequence_length, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n\n    def forward(self, x):\n        # Set initial hidden and cell states\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # Forward propagate RNN\n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n\n        # Decode the hidden state of the last time step\n        out = self.fc(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:28.466878Z","iopub.execute_input":"2025-02-18T20:09:28.467135Z","iopub.status.idle":"2025-02-18T20:09:28.481748Z","shell.execute_reply.started":"2025-02-18T20:09:28.467112Z","shell.execute_reply":"2025-02-18T20:09:28.480914Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## **<font style=\"color:green\">Recurrent neural network with GRU (many-to-one) model</font>**","metadata":{}},{"cell_type":"code","source":"class RNN_GRU(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, sequence_length, num_classes):\n        super(RNN_GRU, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n\n    def forward(self, x):\n        # Set initial hidden and cell states\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # Forward propagate LSTM\n        out, _ = self.gru(x, h0)\n        out = out.reshape(out.shape[0], -1)\n\n        # Decode the hidden state of the last time step\n        out = self.fc(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:28.482513Z","iopub.execute_input":"2025-02-18T20:09:28.482804Z","iopub.status.idle":"2025-02-18T20:09:28.496686Z","shell.execute_reply.started":"2025-02-18T20:09:28.482778Z","shell.execute_reply":"2025-02-18T20:09:28.495977Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Recurrent neural network with LSTM (many-to-one) model</font>**","metadata":{}},{"cell_type":"code","source":"class RNN_LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, sequence_length, num_classes):\n        super(RNN_LSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n\n    def forward(self, x):\n        # Set initial hidden and cell states\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # Forward propagate LSTM\n        out, _ = self.lstm(\n            x, (h0, c0)\n        )  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        out = out.reshape(out.shape[0], -1)\n\n        # Decode the hidden state of the last time step\n        out = self.fc(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:28.497534Z","iopub.execute_input":"2025-02-18T20:09:28.497821Z","iopub.status.idle":"2025-02-18T20:09:28.509519Z","shell.execute_reply.started":"2025-02-18T20:09:28.497794Z","shell.execute_reply":"2025-02-18T20:09:28.508701Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Bidirectional LSTM model</font>**","metadata":{}},{"cell_type":"code","source":"class BRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(BRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(\n            input_size, hidden_size, num_layers, batch_first=True, bidirectional=True\n        )\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n\n        out, _ = self.lstm(x)\n        out = self.fc(out[:, -1, :])\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:28.510381Z","iopub.execute_input":"2025-02-18T20:09:28.510649Z","iopub.status.idle":"2025-02-18T20:09:28.524829Z","shell.execute_reply.started":"2025-02-18T20:09:28.510622Z","shell.execute_reply":"2025-02-18T20:09:28.524086Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Load Data</font>**\n--------------------------------","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/dataset/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:28.525525Z","iopub.execute_input":"2025-02-18T20:09:28.525898Z","iopub.status.idle":"2025-02-18T20:09:28.656873Z","shell.execute_reply.started":"2025-02-18T20:09:28.525874Z","shell.execute_reply":"2025-02-18T20:09:28.655846Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def get_data(batch_size, data_root='/kaggle/working/', num_workers=1):\n    data_transforms = transforms.Compose([\n        transforms.ToTensor()\n    ])\n\n    # train dataloader\n    train_loader = DataLoader(\n        datasets.MNIST(root=data_root, train=True, download=True, transform=data_transforms),\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers\n    )\n\n    # test dataloader\n    test_loader = DataLoader(\n        datasets.MNIST(root=data_root, train=False, download=True, transform=data_transforms),\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers\n    )\n    return train_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:28.657947Z","iopub.execute_input":"2025-02-18T20:09:28.658234Z","iopub.status.idle":"2025-02-18T20:09:28.665106Z","shell.execute_reply.started":"2025-02-18T20:09:28.658207Z","shell.execute_reply":"2025-02-18T20:09:28.664208Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Initialize RNN networks</font>**\n--------------------------------------","metadata":{}},{"cell_type":"code","source":"config_model = TrainingConfiguration()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:28.668842Z","iopub.execute_input":"2025-02-18T20:09:28.669102Z","iopub.status.idle":"2025-02-18T20:09:28.680742Z","shell.execute_reply.started":"2025-02-18T20:09:28.669078Z","shell.execute_reply":"2025-02-18T20:09:28.679986Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Recurrent neural network (many-to-one)\nmodel_rnn = RNN(\n    config_model.input_size, \n    config_model.hidden_size, \n    config_model.num_layers, \n    config_model.sequence_length, \n    config_model.num_classes).to(device)\nprint(model_rnn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:28.682077Z","iopub.execute_input":"2025-02-18T20:09:28.682328Z","iopub.status.idle":"2025-02-18T20:09:29.020426Z","shell.execute_reply.started":"2025-02-18T20:09:28.682305Z","shell.execute_reply":"2025-02-18T20:09:29.019675Z"}},"outputs":[{"name":"stdout","text":"RNN(\n  (rnn): RNN(28, 256, num_layers=2, batch_first=True)\n  (fc): Linear(in_features=7168, out_features=10, bias=True)\n)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Recurrent neural network with GRU (many-to-one)\nmodel_rnn_gru = RNN_GRU(\n    config_model.input_size, \n    config_model.hidden_size, \n    config_model.num_layers, \n    config_model.sequence_length, \n    config_model.num_classes).to(device)\nprint(model_rnn_gru)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:29.021273Z","iopub.execute_input":"2025-02-18T20:09:29.021530Z","iopub.status.idle":"2025-02-18T20:09:29.036324Z","shell.execute_reply.started":"2025-02-18T20:09:29.021510Z","shell.execute_reply":"2025-02-18T20:09:29.035514Z"}},"outputs":[{"name":"stdout","text":"RNN_GRU(\n  (gru): GRU(28, 256, num_layers=2, batch_first=True)\n  (fc): Linear(in_features=7168, out_features=10, bias=True)\n)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Recurrent neural network with LSTM (many-to-one)\nmodel_rnn_lstm = RNN_LSTM(\n    config_model.input_size, \n    config_model.hidden_size, \n    config_model.num_layers,\n    config_model.sequence_length,\n    config_model.num_classes).to(device)\nprint(model_rnn_lstm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:29.037130Z","iopub.execute_input":"2025-02-18T20:09:29.037328Z","iopub.status.idle":"2025-02-18T20:09:29.050691Z","shell.execute_reply.started":"2025-02-18T20:09:29.037311Z","shell.execute_reply":"2025-02-18T20:09:29.050054Z"}},"outputs":[{"name":"stdout","text":"RNN_LSTM(\n  (lstm): LSTM(28, 256, num_layers=2, batch_first=True)\n  (fc): Linear(in_features=7168, out_features=10, bias=True)\n)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Bidirectional LSTM\nmodel_rnn_brnn = BRNN(\n    config_model.input_size, \n    config_model.hidden_size, \n    config_model.num_layers, \n    config_model.num_classes).to(device)\nprint(model_rnn_brnn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:29.051751Z","iopub.execute_input":"2025-02-18T20:09:29.052014Z","iopub.status.idle":"2025-02-18T20:09:29.080079Z","shell.execute_reply.started":"2025-02-18T20:09:29.051988Z","shell.execute_reply":"2025-02-18T20:09:29.079516Z"}},"outputs":[{"name":"stdout","text":"BRNN(\n  (lstm): LSTM(28, 256, num_layers=2, batch_first=True, bidirectional=True)\n  (fc): Linear(in_features=512, out_features=10, bias=True)\n)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Train Network</font>**\n---------------------------","metadata":{}},{"cell_type":"code","source":"def train(\n    train_config: TrainingConfiguration,\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    train_loader: torch.utils.data.DataLoader,\n    epoch_idx: int\n) -> Tuple[float, float]:\n    \n    # change model in training mode\n    model.train()\n\n    # to get batch loss\n    batch_loss = np.array([])\n\n    # to get batch accuracy\n    batch_acc = np.array([])\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # clone target\n        indx_target = target.clone()\n\n        # send data to device (it is mandatory if GPU has to be used)\n        data = data.to(train_config.device).squeeze(1)\n\n        # send target to device\n        target = target.to(train_config.device)\n\n        # reset parameters gradient to zero\n        optimizer.zero_grad()\n\n        # forward pass to the model\n        output = model(data)\n        # cross entropy loss\n        loss = F.cross_entropy(output, target)\n\n        # find gradients w.r.t training parameters\n        loss.backward()\n        # Update parameters using gradients\n        optimizer.step()\n\n        batch_loss = np.append(batch_loss, [loss.item()])\n\n        # get probability score using softmax\n        prob = F.softmax(output, dim=1)\n\n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1]\n\n        # correct prediction\n        correct = pred.cpu().eq(indx_target).sum()\n\n        # accuracy\n        acc = float(correct) / float(len(data))\n\n        batch_acc = np.append(batch_acc, [acc])\n\n        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:\n            print(\n                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n                )\n            )\n\n    epoch_loss = batch_loss.mean()\n    epoch_acc = batch_acc.mean()\n    \n    return epoch_loss, epoch_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:29.080878Z","iopub.execute_input":"2025-02-18T20:09:29.081163Z","iopub.status.idle":"2025-02-18T20:09:29.088057Z","shell.execute_reply.started":"2025-02-18T20:09:29.081135Z","shell.execute_reply":"2025-02-18T20:09:29.087352Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Validace</font>**\n---------------------------","metadata":{}},{"cell_type":"code","source":"def validate(\n    train_config: TrainingConfiguration,\n    model: nn.Module,\n    test_loader: torch.utils.data.DataLoader,\n) -> Tuple[float, float]:\n    \n    model.eval()\n    test_loss = 0\n    count_corect_predictions = 0\n\n    # turn off gradient-computation\n    with torch.no_grad():\n\n        for data, target in test_loader:\n            indx_target = target.clone()\n            data = data.to(train_config.device).squeeze(1)\n\n            target = target.to(train_config.device)\n\n            output = model(data)\n            # add loss for each mini batch\n            test_loss += F.cross_entropy(output, target).item()\n\n            # get probability score using softmax\n            prob = F.softmax(output, dim=1)\n\n            # get the index of the max probability\n            pred = prob.data.max(dim=1)[1]\n\n            # add correct prediction count\n            count_corect_predictions += pred.cpu().eq(indx_target).sum()\n\n        # average over number of mini-batches\n        test_loss = test_loss / len(test_loader)\n\n        # average over number of dataset\n        accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n\n        print(\n            '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n                test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n            )\n        )\n    return test_loss, accuracy/100.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:29.088911Z","iopub.execute_input":"2025-02-18T20:09:29.089150Z","iopub.status.idle":"2025-02-18T20:09:29.103364Z","shell.execute_reply.started":"2025-02-18T20:09:29.089121Z","shell.execute_reply":"2025-02-18T20:09:29.102760Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Experiment</font>**\n---------------------------","metadata":{}},{"cell_type":"code","source":"def experiment(\n    set_model,\n    model_name='model',\n    set_optimizer='Adam', # SGD\n    system_configuration=SystemConfiguration(), \n    training_configuration=TrainingConfiguration()\n):\n\n    setup_system(system_configuration)\n\n    batch_size_to_set = training_configuration.batch_size\n    num_workers_to_set = training_configuration.num_workers\n    epoch_num_to_set = training_configuration.num_epochs\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n        num_workers_to_set = 2\n    else:\n        device = \"cpu\"\n        batch_size_to_set = 16\n        num_workers_to_set = 2\n        epoch_num_to_set = 5\n\n    train_loader, test_loader = get_data(\n        batch_size=batch_size_to_set,\n        data_root=training_configuration.data_root,\n        num_workers=num_workers_to_set\n    )\n\n    training_configuration = TrainingConfiguration(\n        device=device,\n        num_epochs=epoch_num_to_set,\n        batch_size=batch_size_to_set,\n        num_workers=num_workers_to_set\n    )\n\n    model = set_model\n    model.to(training_configuration.device)\n\n    if set_optimizer=='SGD':\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=training_configuration.learning_rate\n        )\n    elif set_optimizer=='Adam':\n        optimizer = optim.Adam(\n            model.parameters(), \n            lr=training_configuration.learning_rate\n        )\n\n    best_loss = torch.tensor(np.inf)\n    epoch_train_loss = np.array([])\n    epoch_test_loss = np.array([])\n    epoch_train_acc = np.array([])\n    epoch_test_acc = np.array([])\n\n    t_begin = time.time()\n    for epoch in range(training_configuration.num_epochs):\n        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n\n        elapsed_time = time.time() - t_begin\n        speed_epoch = elapsed_time / (epoch + 1)\n        speed_batch = speed_epoch / len(train_loader)\n        eta = speed_epoch * training_configuration.num_epochs - elapsed_time\n\n        print(\n            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n                elapsed_time, speed_epoch, speed_batch, eta\n            )\n        )\n\n        if epoch % training_configuration.test_interval == 0:\n            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n\n            if current_loss < best_loss:\n                best_loss = current_loss\n                torch.save(model.state_dict(), os.path.join(training_configuration.dir_root,\"best_\" + model_name + \".pth\"))\n                print(\"Best model saved at epoch {} with loss {:.4f}\".format(epoch, best_loss))\n\n    torch.save(model.state_dict(), os.path.join(training_configuration.dir_root,\"final_\" + model_name + \".pth\"))\n    print(\"Final model saved.\")\n    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n\n    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:29.104154Z","iopub.execute_input":"2025-02-18T20:09:29.104409Z","iopub.status.idle":"2025-02-18T20:09:29.123070Z","shell.execute_reply.started":"2025-02-18T20:09:29.104381Z","shell.execute_reply":"2025-02-18T20:09:29.122378Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Function for print results</font>**\n------------------------------","metadata":{}},{"cell_type":"code","source":"def plot_results(\n    data: list,\n    label_names: list,\n    name_value: str,\n    name_title: str\n) -> None:  \n    # Plot loss\n    plt.rcParams[\"figure.figsize\"] = (10, 6)\n    x = range(len(data[0]))\n    \n    \n    plt.figure\n    plt.plot(x, data[0], color='r', label=label_names[0])\n    plt.plot(x, data[1], color='b', label=label_names[1])\n    plt.xlabel('epoch no.')\n    plt.ylabel(name_value)\n    plt.legend(loc='upper right')\n    plt.title(name_title)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:29.123788Z","iopub.execute_input":"2025-02-18T20:09:29.123983Z","iopub.status.idle":"2025-02-18T20:09:29.139702Z","shell.execute_reply.started":"2025-02-18T20:09:29.123966Z","shell.execute_reply":"2025-02-18T20:09:29.139019Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Inference on sample images</font>**\n------------------------------","metadata":{}},{"cell_type":"code","source":" def prediction(model, train_config, batch_input):\n\n    # turn off gradient-computation\n    with torch.no_grad():\n\n        # send model to cpu/cuda according to your system configuration\n        model.to(train_config.device)\n\n        # it is important to do model.eval() before prediction\n        model.eval()\n\n        data = batch_input.to(train_config.device)\n\n        output = model(data)\n\n        # get probability score using softmax\n        prob = F.softmax(output, dim=1)\n\n        # get the max probability\n        pred_prob = prob.data.max(dim=1)[0]\n\n        # get the index of the max probability\n        pred_index = prob.data.max(dim=1)[1]\n\n    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"-------------\n## ***<font style=\"color:purple\">Main part of the program</font>***\n--------------------","metadata":{}},{"cell_type":"code","source":"def main():\n    # Recurrent neural network (many-to-one) \n    rnn_model, epoch_train_loss_rnn, epoch_train_acc_rnn, epoch_test_loss_rnn, epoch_test_acc_rnn = experiment(set_model=model_rnn, model_name='rnn_model', set_optimizer='Adam')\n    \n    plot_results(\n        data=[epoch_train_loss_rnn, epoch_test_loss_rnn],\n        label_names=['train loss','validation loss'],\n        name_value='Loss',\n        name_title='Training and Validation Loss of RNN'\n    )\n    plot_results(\n        data=[epoch_train_acc_rnn, epoch_test_acc_rnn],\n        label_names=['train accuracy','validation accuracy'],\n        name_value='Accuracy',\n        name_title='Training and Validation Loss of RNN'\n    )\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:29.140384Z","iopub.execute_input":"2025-02-18T20:09:29.140593Z","iopub.status.idle":"2025-02-18T20:09:29.151185Z","shell.execute_reply.started":"2025-02-18T20:09:29.140576Z","shell.execute_reply":"2025-02-18T20:09:29.150540Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">References:</font>**\n\n- [YOUTUBE - Pytorch RNN example (Recurrent Neural Network)](https://www.youtube.com/watch?v=Gl2WXLIMvKA&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=5)\n- [YOUTUBE - Pytorch Bidirectional LSTM example](https://www.youtube.com/watch?v=jGst43P-TJA&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=6)\n- [GitHub - Machine-Learning-Collection](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master)\n- [YOUTUBE - PyTorch RNN Tutorial - Name Classification Using A Recurrent Neural Net](https://www.youtube.com/watch?v=WEV61GmmPrk)\n- [YOUTUBE - PyTorch Tutorial - RNN & LSTM & GRU - Recurrent Neural Nets](https://www.youtube.com/watch?v=0_PgWWmauHk)\n- [GitHub - pytorch-examples](https://github.com/patrickloeber/pytorch-examples/tree/master)","metadata":{}}]}