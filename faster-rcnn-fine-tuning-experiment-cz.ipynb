{"metadata":{"jupytext":{"formats":"ipynb,py:light"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Faster RCNN Fine-tuning Experiment</font>\n\nJiž známe následující stavební blokové schéma Faster RCNN.\n\n---\n![](https://www.researchgate.net/profile/Giang_Son_Tran/publication/324549019/figure/fig1/AS:649929152266241@1531966593689/Faster-R-CNN-Architecture-9.png)\n\n---\n\nChceme rychle doladit model Faster RCNN, aby fungoval pro náš detekční problém.\nPoslední vrstva (klasifikátor) ve výše uvedeném obrázku bere zvětšené vlastnosti všech navrhovaných ohraničujících boxů a předpovídá třídy a ohraničující boxy. Pro tento úkol používá síť FastRCNNPredictor. Takže pro doladění s našimi daty musíme aktualizovat počet tříd v prediktoru.\n\nAbychom získali model Faster RCNN s požadovaným počtem tříd, můžeme napsat následující metodu.\n\n```python\ndef faster_rcnn_pretrained_model(num_classes):\n    # load an instance detection model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    \n    return model\n```\n\nZačněme trénovat. Trainer použijeme k trénování a vyhodnocování našeho modelu. \n","metadata":{}},{"cell_type":"code","source":"!wget \"https://raw.githubusercontent.com/RadimKozl/OpenCV_academy_my_work/refs/heads/main/work_module/trainer_faster_rcnn.zip\" -O ./trainer.zip\n\n!ls /kaggle/working/\n\n!unzip /kaggle/working/trainer.zip\n\n!rm /kaggle/working/trainer.zip\n\n!cd ..","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:38:03.747356Z","iopub.execute_input":"2024-10-27T21:38:03.747748Z","iopub.status.idle":"2024-10-27T21:38:09.069522Z","shell.execute_reply.started":"2024-10-27T21:38:03.747682Z","shell.execute_reply":"2024-10-27T21:38:09.068224Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2024-10-27 21:38:04--  https://raw.githubusercontent.com/RadimKozl/OpenCV_academy_my_work/refs/heads/main/work_module/trainer_faster_rcnn.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11005 (11K) [application/zip]\nSaving to: './trainer.zip'\n\n./trainer.zip       100%[===================>]  10.75K  --.-KB/s    in 0s      \n\n2024-10-27 21:38:04 (81.1 MB/s) - './trainer.zip' saved [11005/11005]\n\ntrainer.zip\nArchive:  /kaggle/working/trainer.zip\n  inflating: trainer/__init__.py     \n  inflating: trainer/base_metric.py  \n  inflating: trainer/configuration.py  \n  inflating: trainer/datasets.py     \n  inflating: trainer/hooks.py        \n  inflating: trainer/matplotlib_visualizer.py  \n  inflating: trainer/metrics.py      \n  inflating: trainer/trainer.py      \n  inflating: trainer/utils.py        \n  inflating: trainer/visualizer.py   \n  inflating: trainer/voc_eval.py     \n","output_type":"stream"}]},{"cell_type":"code","source":"!wget \"https://raw.githubusercontent.com/RadimKozl/OpenCV_academy_my_work/refs/heads/main/work_module/faster_rcnn_detector.py\" -O ./faster_rcnn_detector.py\n!wget \"https://raw.githubusercontent.com/RadimKozl/OpenCV_academy_my_work/refs/heads/main/work_module/__init__.py\" -O ./__init__.py","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:38:09.072181Z","iopub.execute_input":"2024-10-27T21:38:09.072967Z","iopub.status.idle":"2024-10-27T21:38:11.387804Z","shell.execute_reply.started":"2024-10-27T21:38:09.072916Z","shell.execute_reply":"2024-10-27T21:38:11.386661Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2024-10-27 21:38:09--  https://raw.githubusercontent.com/RadimKozl/OpenCV_academy_my_work/refs/heads/main/work_module/faster_rcnn_detector.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 870 [text/plain]\nSaving to: './faster_rcnn_detector.py'\n\n./faster_rcnn_detec 100%[===================>]     870  --.-KB/s    in 0s      \n\n2024-10-27 21:38:10 (80.7 MB/s) - './faster_rcnn_detector.py' saved [870/870]\n\n--2024-10-27 21:38:11--  https://raw.githubusercontent.com/RadimKozl/OpenCV_academy_my_work/refs/heads/main/work_module/__init__.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2 [text/plain]\nSaving to: './__init__.py'\n\n./__init__.py       100%[===================>]       2  --.-KB/s    in 0s      \n\n2024-10-27 21:38:11 (38.3 KB/s) - './__init__.py' saved [2/2]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Experiment (Trénink)</font>\n\nPojďme napsat třídu experimentu pro rychlé jemné doladění RCNN.","metadata":{}},{"cell_type":"code","source":"%matplotlib notebook\n%load_ext autoreload\n%autoreload 2\n\nimport os\nimport random\nimport cv2\n\nfrom operator import itemgetter\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\n\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import MultiStepLR\n\nfrom trainer import Trainer, hooks, configuration\nfrom trainer.utils import patch_configs\nfrom trainer.utils import setup_system\n\nfrom trainer.metrics import APEstimator\nfrom trainer.datasets import ListDataset\nfrom trainer.matplotlib_visualizer import MatplotlibVisualizer\nfrom trainer.utils import collate_fn\n\nfrom faster_rcnn_detector import faster_rcnn_pretrained_model\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:38:11.389422Z","iopub.execute_input":"2024-10-27T21:38:11.389833Z","iopub.status.idle":"2024-10-27T21:38:15.924502Z","shell.execute_reply.started":"2024-10-27T21:38:11.389795Z","shell.execute_reply":"2024-10-27T21:38:15.923563Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">1. Třída Experiment</font>","metadata":{}},{"cell_type":"code","source":"class Experiment:\n    def __init__(\n        self,\n        system_config: configuration.SystemConfig = configuration.SystemConfig(),\n        dataset_config: configuration.DatasetConfig = configuration.DatasetConfig(),  \n        dataloader_config: configuration.DataloaderConfig = configuration.DataloaderConfig(),\n        optimizer_config: configuration.OptimizerConfig = configuration.OptimizerConfig(),\n    ):\n        self.system_config = system_config\n        setup_system(system_config)\n        \n        # fruit detection data has 3-classes. Anything other than these three classes is called background\n        self.classes = ['__background__', 'apple', 'banana', 'orange']\n        \n        # written custom dataset class of our dataset\n        train_csv_path = os.path.join(dataset_config.root_dir, 'labels_train.csv')\n        self.dataset_train = ListDataset(\n            csv_path=train_csv_path,\n            train = True,\n            transform=None\n        )\n\n        self.loader_train = DataLoader(\n            dataset=self.dataset_train,\n            batch_size=dataloader_config.batch_size,\n            shuffle=True,\n            collate_fn=collate_fn,\n            num_workers=dataloader_config.num_workers,\n            pin_memory=True\n        )\n        \n        test_csv_path = os.path.join(dataset_config.root_dir, 'labels_test.csv')\n\n        self.dataset_test = ListDataset(\n            csv_path=test_csv_path,\n            train=False,\n            transform=None\n        )\n        self.loader_test = DataLoader(\n            dataset=self.dataset_test,\n            batch_size=dataloader_config.batch_size,\n            shuffle=False,\n            collate_fn=collate_fn,\n            num_workers=dataloader_config.num_workers,\n            pin_memory=True\n        )\n        \n        # get faster rcnn model pretrained on coco\n        self.model = faster_rcnn_pretrained_model(len(self.classes))\n        \n        self.metric_fn = APEstimator(classes=self.classes)\n        \n        params = [p for p in self.model.parameters() if p.requires_grad]\n        self.optimizer = optim.SGD(\n            params,\n            lr=optimizer_config.learning_rate,\n            weight_decay=optimizer_config.weight_decay,\n            momentum=optimizer_config.momentum\n        )\n        self.lr_scheduler = MultiStepLR(\n            self.optimizer, milestones=optimizer_config.lr_step_milestones, gamma=optimizer_config.lr_gamma\n        )\n        self.visualizer = MatplotlibVisualizer()\n\n    def run(self, trainer_config: configuration.TrainerConfig) -> dict:  \n        setup_system(self.system_config)\n        device = torch.device(trainer_config.device)\n        self.model = self.model.to(device)\n\n        model_trainer = Trainer(\n            model=self.model,\n            loader_train=self.loader_train,\n            loader_test=self.loader_test,\n            metric_fn=self.metric_fn,\n            optimizer=self.optimizer,\n            lr_scheduler=self.lr_scheduler,\n            device=device,\n            data_getter=itemgetter(\"image\"),\n            target_getter=itemgetter(\"target\"),\n            stage_progress=trainer_config.progress_bar,\n            get_key_metric=itemgetter(\"mAP\"),\n            visualizer=self.visualizer,\n            model_save_best=trainer_config.model_save_best,\n            model_saving_frequency=trainer_config.model_saving_frequency,\n            save_dir=trainer_config.model_dir\n        )\n\n        model_trainer.register_hook(\"train\", hooks.train_hook_faster_rcnn)\n        model_trainer.register_hook(\"test\", hooks.test_hook_faster_rcnn)\n        model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_faster_rcnn)\n        self.metrics = model_trainer.fit(trainer_config.epoch_num)\n        return self.metrics\n\n    def draw_bboxes(self, rows, columns, trainer_config: configuration.TrainerConfig):\n        # load the best model\n        if trainer_config.model_save_best:\n            self.model.load_state_dict(\n                torch.\n                load(os.path.join(trainer_config.model_dir, self.model.__class__.__name__) + '_best.pth')\n            )\n        # or use the last saved\n        self.model = self.model.eval()\n\n        fig, ax = plt.subplots(\n            nrows=rows, ncols=columns, figsize=(15, 30), gridspec_kw={\n                'wspace': 0,\n                'hspace': 0.05\n            }\n        )\n        \n        colors = [(255, 0, 0), (0, 225, 0), (0, 0, 225)]\n\n        for axi in ax.flat:\n            index = random.randrange(len(self.loader_test.dataset))\n\n            image, targets = self.loader_test.dataset[index]\n\n            device = torch.device(trainer_config.device)\n            image = image.to(device).clone()\n\n            detections = self.model(image.unsqueeze(0))\n            bboxes = detections[0]['boxes'].cpu().detach().numpy()\n            labels = detections[0]['labels'].cpu().detach().numpy()\n            scores = detections[0]['scores'].cpu().detach().numpy()\n\n            with torch.no_grad():\n                img = image.cpu()\n                img = img.numpy().transpose(1, 2, 0)\n                img = (img * 255.).astype(np.uint8)\n                gt_img = img.copy()\n                pred_img = img.copy()\n\n                for i, box in enumerate(targets['boxes']):\n                    label = targets['labels'][i]\n                    cls = self.classes[label]\n                    clr = colors[label-1]\n                    gt_img = cv2.rectangle(\n                        gt_img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), clr, thickness=2)\n                    gt_img = cv2.putText(gt_img, cls, (int(box[0]), int(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, \n                                         0.9, clr, 2)\n                    \n                for i, box in enumerate(bboxes):\n                    label = labels[i]\n                    score = scores[i]\n                    cls = self.classes[label]\n                    clr = colors[label-1]\n                    cls_score = '{0}:{1:.2}'.format(cls, score)\n                    pred_img = cv2.rectangle(\n                        pred_img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), clr, thickness=2)\n                    pred_img = cv2.putText(pred_img, cls_score, (int(box[0]), int(box[1])-10), \n                                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, clr, 2)\n\n                merged_img = np.concatenate((gt_img, pred_img), axis=1)\n                axi.imshow(merged_img)\n                axi.axis('off')\n        fig.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:38:15.926043Z","iopub.execute_input":"2024-10-27T21:38:15.926795Z","iopub.status.idle":"2024-10-27T21:38:15.986769Z","shell.execute_reply.started":"2024-10-27T21:38:15.926749Z","shell.execute_reply":"2024-10-27T21:38:15.985816Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">2. Obrázky ovoce pro datovou sadu pro detekci objektů</font>\n\nZde jsme převzali datovou sadu pro detekci ovoce [Kaggle](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection). Má štítky ve formátu XML. Přidali jsme soubory štítků ve formátu CSV. Data si můžete stáhnout z **[tady](https://www.dropbox.com/sh/r2qxsaeq1otrtag/AAC1oI4g6n-upAB8M-VNYs68a?dl=1)**. \n\nSkládá se z `300` snímků (vlak `240` a testovací `60` ).\n\nMá tři třídy – jablko, banán a pomeranč. \n\nZde je jeden z příkladů dat, která datová sada poskytuje:\n\n### [Stáhnout data](https://www.dropbox.com/sh/r2qxsaeq1otrtag/AAC1oI4g6n-upAB8M-VNYs68a?dl=1)\n---\n\n<img src='https://www.dropbox.com/s/837sdq5d1f2jxz3/apple_3.jpg?dl=1' align='middle'>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:green\">3. Spustit Experiment</font>","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    dataloader_config, trainer_config = patch_configs(epoch_num_to_set=100, batch_size_to_set=2)\n    \n    path = os.path.join('/','kaggle','input','opencv-fruit-images-for-object-detection')\n    \n\n    dataset_config = configuration.DatasetConfig(root_dir=path)\n    print(dataset_config.root_dir)\n    \n    \n    optimizer_config = configuration.OptimizerConfig(\n        learning_rate=5e-3, \n        lr_step_milestones=[50], \n        lr_gamma=0.1, \n        momentum=0.9, \n        weight_decay=1e-5\n    )\n    \n    experiment = Experiment(\n        dataset_config=dataset_config, \n        dataloader_config=dataloader_config, \n        optimizer_config=optimizer_config\n    )\n    \n    # Run the experiment / start training\n    experiment.run(trainer_config)","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2024-10-27T21:39:58.347573Z","iopub.execute_input":"2024-10-27T21:39:58.347996Z","iopub.status.idle":"2024-10-27T21:39:59.847792Z","shell.execute_reply.started":"2024-10-27T21:39:58.347930Z","shell.execute_reply":"2024-10-27T21:39:59.846030Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/opencv-fruit-images-for-object-detection\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47fc6a0c000b48ae91f45cc367618b6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/120 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e57467131e2a425eb988e74f61aff4ab"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(\n\u001b[1;32m     20\u001b[0m     dataset_config\u001b[38;5;241m=\u001b[39mdataset_config, \n\u001b[1;32m     21\u001b[0m     dataloader_config\u001b[38;5;241m=\u001b[39mdataloader_config, \n\u001b[1;32m     22\u001b[0m     optimizer_config\u001b[38;5;241m=\u001b[39moptimizer_config\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Run the experiment / start training\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[4], line 91\u001b[0m, in \u001b[0;36mExperiment.run\u001b[0;34m(self, trainer_config)\u001b[0m\n\u001b[1;32m     89\u001b[0m model_trainer\u001b[38;5;241m.\u001b[39mregister_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, hooks\u001b[38;5;241m.\u001b[39mtest_hook_faster_rcnn)\n\u001b[1;32m     90\u001b[0m model_trainer\u001b[38;5;241m.\u001b[39mregister_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, hooks\u001b[38;5;241m.\u001b[39mend_epoch_hook_faster_rcnn)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\n","File \u001b[0;32m/kaggle/working/trainer/trainer.py:87\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     85\u001b[0m iterator \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m---> 87\u001b[0m     output_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstage_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_getter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_getter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_getter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_getter\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     output_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m](\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_test,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m         get_key_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_key_metric\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisualizer:\n","File \u001b[0;32m/kaggle/working/trainer/hooks.py:42\u001b[0m, in \u001b[0;36mtrain_hook_faster_rcnn\u001b[0;34m(model, loader, optimizer, device, data_getter, target_getter, iterator_type, prefix, stage_progress)\u001b[0m\n\u001b[1;32m     40\u001b[0m iterator \u001b[38;5;241m=\u001b[39m iterator_type(loader, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m stage_progress, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m loss_avg \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iterator):\n\u001b[1;32m     44\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     46\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m sample[\u001b[38;5;241m0\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1344\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/kaggle/working/trainer/datasets.py\", line 61, in __getitem__\n    img = Image.open(self.fnames[idx]).convert(\"RGB\")\n  File \"/opt/conda/lib/python3.10/site-packages/PIL/Image.py\", line 3431, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/data/train_zip/train/banana_50.jpg'\n"],"ename":"FileNotFoundError","evalue":"Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/kaggle/working/trainer/datasets.py\", line 61, in __getitem__\n    img = Image.open(self.fnames[idx]).convert(\"RGB\")\n  File \"/opt/conda/lib/python3.10/site-packages/PIL/Image.py\", line 3431, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/data/train_zip/train/banana_50.jpg'\n","output_type":"error"}]},{"cell_type":"code","source":"# how good our detector works by visualizing the results on the randomly chosen test images:\n\nif __name__ == '__main__':\n    experiment.draw_bboxes(4, 1, trainer_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:38:19.387478Z","iopub.status.idle":"2024-10-27T21:38:19.387857Z","shell.execute_reply.started":"2024-10-27T21:38:19.387648Z","shell.execute_reply":"2024-10-27T21:38:19.387680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">4. Zkuste to s jinou datovou sadou</font>\n\nVylaďte model Faster-RCNN pro své vlastní nebo veřejně dostupné datové sady. Následuje několik odkazů na veřejně dostupné datové sady:\n\n- [MCIndoor20000](https://github.com/bircatmcri/MCIndoor20000)\n\n- [The Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/)\n\n- [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)\n\n- [KITTI Vision](http://www.cvlibs.net/datasets/kitti/)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}