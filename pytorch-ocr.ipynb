{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10951507,"sourceType":"datasetVersion","datasetId":6783030}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---------------\n\n# **<font style=\"color:Black\">Create OCR by PyTorch</font>**\n-------------------\n-----------------","metadata":{}},{"cell_type":"code","source":"!pip install python-Levenshtein","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:52:26.523616Z","iopub.execute_input":"2025-03-11T20:52:26.523911Z","iopub.status.idle":"2025-03-11T20:52:35.776032Z","shell.execute_reply.started":"2025-03-11T20:52:26.523884Z","shell.execute_reply":"2025-03-11T20:52:35.774841Z"}},"outputs":[{"name":"stdout","text":"Collecting python-Levenshtein\n  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting Levenshtein==0.27.1 (from python-Levenshtein)\n  Downloading levenshtein-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n  Downloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\nSuccessfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.12.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!pip install tensorboard\n!tensorboard --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:52:35.777449Z","iopub.execute_input":"2025-03-11T20:52:35.777913Z","iopub.status.idle":"2025-03-11T20:52:55.082256Z","shell.execute_reply.started":"2025-03-11T20:52:35.777870Z","shell.execute_reply":"2025-03-11T20:52:55.080992Z"}},"outputs":[{"name":"stdout","text":"2025-03-11 20:52:40.035255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-11 20:52:40.305951: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-11 20:52:40.377680: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2.17.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport sys\nimport shutil\nimport random\nfrom PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter\nimport numpy as np\nimport cv2\n\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nfrom Levenshtein import distance as levenshtein_distance\n\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F  # Add this import\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:52:55.083752Z","iopub.execute_input":"2025-03-11T20:52:55.084214Z","iopub.status.idle":"2025-03-11T20:53:07.197041Z","shell.execute_reply.started":"2025-03-11T20:52:55.084173Z","shell.execute_reply":"2025-03-11T20:53:07.195878Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Hyperparameters</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"seed = 3\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:07.199275Z","iopub.execute_input":"2025-03-11T20:53:07.200111Z","iopub.status.idle":"2025-03-11T20:53:07.214996Z","shell.execute_reply.started":"2025-03-11T20:53:07.200072Z","shell.execute_reply":"2025-03-11T20:53:07.213813Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7beabb9efbd0>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"OUTPUT_DIR = os.path.join('/kaggle','working','synthetic_data','images')\nTENSORBOARD_DIR = os.path.join('/kaggle','working','runs')\nMODEL_DIR = os.path.join('/kaggle','working','model_dir')\nLABELS_FILE = os.path.join('/kaggle','working','synthetic_data','labels.txt')\nNUM_SAMPLES = 10240  # Number of images generated\nIMG_WIDTH = 128\nIMG_HEIGHT = 32\nBATCH_SIZE = 32\nEPOCHS = 20\nLEARNING_RATE = 1e-5\nWEIGHT_DECAY = 1e-4  \nWARMUP_STEPS = 500\nENTROPY_WEIGHT = 0.5\nTEMPERATURE = 0.3\nDROPOUT = 0.3\nBEAM_WIDTH = 5\nLABEL_SMOOTHING = 0.1\nBLANK_PENALTY_WEIGHT = 1.0\nMAX_SEQ_LENGTH = None\nCHARSET = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-*\"  # Group characters\nMAX_TEXT_LENGTH = 12  # Maximum length of text in an image\nMIN_TEXT_LENGTH = 4  # Minimum length of text in an image\nFONT_DIR = os.path.join('/kaggle','input','google-fonts','GoogleFontScripts') # Folder with TrueType fonts (.ttf)\nBACKGROUND_DIR = os.path.join('/kaggle','working','backgrounds')  # New folder for background (optional)\nNUMBER_BACKGROUND_IMAGE = 2000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:07.216857Z","iopub.execute_input":"2025-03-11T20:53:07.217377Z","iopub.status.idle":"2025-03-11T20:53:07.268350Z","shell.execute_reply.started":"2025-03-11T20:53:07.217329Z","shell.execute_reply":"2025-03-11T20:53:07.267014Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Utils support functions</font>**\n-------------------","metadata":{}},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Create output folders</font>**","metadata":{}},{"cell_type":"code","source":"os.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(BACKGROUND_DIR, exist_ok=True)\nos.makedirs(TENSORBOARD_DIR, exist_ok=True)\n\n# Creates a new file\nwith open(LABELS_FILE, 'w') as fp:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:07.269562Z","iopub.execute_input":"2025-03-11T20:53:07.270026Z","iopub.status.idle":"2025-03-11T20:53:07.290779Z","shell.execute_reply.started":"2025-03-11T20:53:07.269981Z","shell.execute_reply":"2025-03-11T20:53:07.289745Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Load the font list (add the paths to the .ttf files to the \"fonts\" folder)</font>**","metadata":{}},{"cell_type":"code","source":"font_files = [\n    os.path.join(FONT_DIR, f) for f in os.listdir(FONT_DIR) \n    if f.endswith('.ttf') and os.path.isfile(os.path.join(FONT_DIR, f))\n]\nif not font_files:\n    raise FileNotFoundError(\"No fonts found in 'fonts' folder. Add .ttf files!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:07.292083Z","iopub.execute_input":"2025-03-11T20:53:07.292467Z","iopub.status.idle":"2025-03-11T20:53:13.868987Z","shell.execute_reply.started":"2025-03-11T20:53:07.292437Z","shell.execute_reply":"2025-03-11T20:53:13.867386Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Generating a simple gradient background</font>**","metadata":{}},{"cell_type":"code","source":"def generate_gradient_background(filename, size=(128, 32)):\n    img = Image.new('L', size, color=230)  # Lighter gray as a base\n    draw = ImageDraw.Draw(img)\n    for y in range(size[1]):\n        # Soft gradient with low contrast\n        color = int(230 - 20 * (y / size[1]))  # From light gray to slightly darker\n        draw.line([(0, y), (size[0], y)], fill=color)\n    # Background blur\n    img = img.filter(ImageFilter.GaussianBlur(radius=2))\n    img.save(os.path.join(BACKGROUND_DIR, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:13.870559Z","iopub.execute_input":"2025-03-11T20:53:13.871090Z","iopub.status.idle":"2025-03-11T20:53:13.880765Z","shell.execute_reply.started":"2025-03-11T20:53:13.871037Z","shell.execute_reply":"2025-03-11T20:53:13.879056Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Generate a background with noise (paper texture)</font>**","metadata":{}},{"cell_type":"code","source":"def generate_paper_texture(filename, size=(128, 32)):\n    img = Image.new('L', size, color=220)  # Lighter gray\n    noise = np.random.normal(0, 5, size).astype(np.uint8)  # Less noise\n    noise_img = Image.fromarray(noise)\n    img.paste(noise_img, (0, 0), noise_img)\n    # Blur for a softer effect\n    img = img.filter(ImageFilter.GaussianBlur(radius=1.5))\n    img.save(os.path.join(BACKGROUND_DIR, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:13.882221Z","iopub.execute_input":"2025-03-11T20:53:13.882775Z","iopub.status.idle":"2025-03-11T20:53:13.903431Z","shell.execute_reply.started":"2025-03-11T20:53:13.882726Z","shell.execute_reply":"2025-03-11T20:53:13.901813Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Creating multiple backgrounds</font>**","metadata":{}},{"cell_type":"code","source":"for i in range(NUMBER_BACKGROUND_IMAGE):\n    generate_gradient_background(f\"gradient_{i}.png\")\n    generate_paper_texture(f\"paper_{i}.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:13.905262Z","iopub.execute_input":"2025-03-11T20:53:13.905992Z","iopub.status.idle":"2025-03-11T20:53:16.306674Z","shell.execute_reply.started":"2025-03-11T20:53:13.905937Z","shell.execute_reply":"2025-03-11T20:53:16.305373Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Load backgrounds (optional, add images to the \"backgrounds\" folder)</font>**","metadata":{}},{"cell_type":"code","source":"background_files = (\n    [os.path.join(BACKGROUND_DIR, f) for f in os.listdir(BACKGROUND_DIR) \n     if f.endswith(('.png', '.jpg', '.jpeg'))] if os.path.exists(BACKGROUND_DIR) else []\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.307862Z","iopub.execute_input":"2025-03-11T20:53:16.308320Z","iopub.status.idle":"2025-03-11T20:53:16.324070Z","shell.execute_reply.started":"2025-03-11T20:53:16.308253Z","shell.execute_reply":"2025-03-11T20:53:16.322806Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Random text generation function</font>**","metadata":{}},{"cell_type":"code","source":"def generate_random_text(min_length, max_length, charset=CHARSET):\n    \"\"\"\n    Generates random text of length between min_length and max_length from the given character set.\n    \n    Args:\n        min_length (int): Minimum length of the generated text.\n        max_length (int): Maximum length of the generated text.\n        charset (str): Character set from which characters are selected (default is global CHARSET).\n    \n    Returns:\n        str: Random text of length between min_length and max_length.\n    \"\"\"\n    # Verifying that min_length is not greater than max_length\n    if min_length > max_length:\n        raise ValueError(f\"min_length ({min_length}) must be less than or equal to max_length ({max_length})\")\n    \n    # Choosing a random length between min_length and max_length\n    length = random.randint(min_length, max_length)\n    \n    # Generating text using random.choice from charset\n    return ''.join(random.choice(charset) for _ in range(length))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.325407Z","iopub.execute_input":"2025-03-11T20:53:16.325888Z","iopub.status.idle":"2025-03-11T20:53:16.335600Z","shell.execute_reply.started":"2025-03-11T20:53:16.325812Z","shell.execute_reply":"2025-03-11T20:53:16.334328Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Functions for adding noise and distortions</font>**","metadata":{}},{"cell_type":"code","source":"def add_noise_and_distortion(img):\n    img_array = np.array(img)\n    # Zvýšený šum\n    if random.random() > 0.5:\n        noise = np.random.normal(0, random.randint(10, 25), img_array.shape).astype(np.uint8)\n        img_array = cv2.add(img_array, noise)\n    # Silnější perspektivní distorze\n    rows, cols = img_array.shape\n    src_points = np.float32([[0, 0], [cols-1, 0], [0, rows-1], [cols-1, rows-1]])\n    dst_points = np.float32([\n        [random.uniform(0, 5), random.uniform(0, 5)],\n        [cols-1-random.uniform(0, 5), random.uniform(0, 5)],\n        [random.uniform(0, 5), rows-1-random.uniform(0, 5)],\n        [cols-1-random.uniform(0, 5), rows-1-random.uniform(0, 5)]\n    ])\n    matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n    img_array = cv2.warpPerspective(img_array, matrix, (cols, rows))\n    return Image.fromarray(img_array)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.339838Z","iopub.execute_input":"2025-03-11T20:53:16.340240Z","iopub.status.idle":"2025-03-11T20:53:16.364333Z","shell.execute_reply.started":"2025-03-11T20:53:16.340204Z","shell.execute_reply":"2025-03-11T20:53:16.362850Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Improved image generation feature</font>**","metadata":{}},{"cell_type":"code","source":"def generate_synthetic_image(text, font_path, img_size=(IMG_WIDTH, IMG_HEIGHT)):\n    # Background\n    if background_files:\n        bg_path = random.choice(background_files)\n        img = Image.open(bg_path).convert('L').resize(img_size)\n    else:\n        img = Image.new('L', img_size, color=230)\n        draw = ImageDraw.Draw(img)\n        for y in range(img_size[1]):\n            color = int(230 - 20 * (y / img_size[1]))\n            draw.line([(0, y), (img_size[0], y)], fill=color)\n        img = img.filter(ImageFilter.GaussianBlur(radius=2))\n\n    draw = ImageDraw.Draw(img)\n\n    # Iterative font and text editing\n    font_size = random.randint(20, min(IMG_HEIGHT-2, 28))\n    max_attempts = 5  # Limiting the number of attempts\n    for attempt in range(max_attempts):\n        font = ImageFont.truetype(font_path, font_size)\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width, text_height = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n\n        if text_width <= IMG_WIDTH - 10:  # Text will fit\n            break\n        elif len(text) > 1:  # Shorten the text if it is too long.\n            text = text[:len(text)//2]\n        else:  # Reduce font size\n            font_size = max(10, font_size - 5)  # Minimum size 10\n\n    # If that doesn't work, use a minimal font and single-letter text.\n    if text_width > IMG_WIDTH - 10:\n        text = text[0]  # Use the first letter\n        font_size = 10\n        font = ImageFont.truetype(font_path, font_size)\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width, text_height = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n\n    # Text position\n    x = random.randint(5, max(5, IMG_WIDTH - text_width - 5))\n    y = random.randint(5, max(5, IMG_HEIGHT - text_height - 5))\n\n    # Highlighting text\n    text_color = random.randint(0, 50)\n    outline_color = 200\n    for offset_x in [-1, 0, 1]:\n        for offset_y in [-1, 0, 1]:\n            if offset_x != 0 or offset_y != 0:\n                draw.text((x + offset_x, y + offset_y), text, font=font, fill=outline_color)\n    draw.text((x, y), text, font=font, fill=text_color)\n\n    # Noise and distortion\n    img = add_noise_and_distortion(img)\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.366706Z","iopub.execute_input":"2025-03-11T20:53:16.367255Z","iopub.status.idle":"2025-03-11T20:53:16.386046Z","shell.execute_reply.started":"2025-03-11T20:53:16.367201Z","shell.execute_reply":"2025-03-11T20:53:16.384934Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Function for splitting labels</font>**","metadata":{}},{"cell_type":"code","source":"def split_labels(labels, label_lengths):\n    \"\"\"Split a flat tensor of labels into a list of label sequences based on lengths.\"\"\"\n    split_labels = []\n    start = 0\n    for length in label_lengths:\n        split_labels.append(labels[start:start + length])\n        start += length\n    return split_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.387367Z","iopub.execute_input":"2025-03-11T20:53:16.387805Z","iopub.status.idle":"2025-03-11T20:53:16.414640Z","shell.execute_reply.started":"2025-03-11T20:53:16.387767Z","shell.execute_reply":"2025-03-11T20:53:16.413088Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Function of Beam search decoding</font>**","metadata":{}},{"cell_type":"code","source":"def beam_search_decode(output, idx_to_char, target_lengths=None, beam_width=20, blank_penalty=-1.0, length_penalty=-0.5, global_step=None):\n    probs = output.softmax(2).cpu().numpy()\n    T, B, C = probs.shape\n    predictions = []\n\n    for b in range(B):\n        sequence_probs = [(0.0, [], 1.0)]  # (log_prob, sequence, probability)\n        max_length = target_lengths[b].item() * 2 if target_lengths is not None else min(T, 16)\n        for t in range(T):\n            new_sequences = []\n            for log_prob, seq, prob in sequence_probs:\n                if len(seq) >= max_length:\n                    new_sequences.append((log_prob, seq, prob))\n                    continue\n                top_k_probs, top_k_idx = torch.topk(torch.tensor(probs[t, b]), beam_width)\n                for k_prob, k_idx in zip(top_k_probs, top_k_idx):\n                    new_seq = seq + [k_idx.item()] if k_idx.item() != 0 else seq  # We only add non-blank tokens\n                    new_prob = prob * k_prob.item()\n                    new_log_prob = log_prob + np.log(k_prob.item() + 1e-10)  # Prevention logs(0)\n                    if k_idx.item() == 0:\n                        new_log_prob += blank_penalty\n                    new_log_prob += length_penalty * len(new_seq)\n                    new_sequences.append((new_log_prob, new_seq, new_prob))\n            sequence_probs = sorted(new_sequences, key=lambda x: x[0], reverse=True)[:beam_width]\n\n        best_seq = sequence_probs[0][1]\n        if all(token == 0 for token in best_seq):  # If all tokens are blank\n            decoded = \"\"\n        else:\n            decoded = []\n            prev = -1\n            for idx in best_seq:\n                if idx != 0 and idx != prev:\n                    decoded.append(idx_to_char.get(idx, ''))\n                prev = idx\n            decoded = ''.join(decoded)\n        predictions.append(decoded if decoded else '<empty>')\n\n        token_counts = Counter(best_seq)\n        pred_length = len(best_seq)\n        for token, count in token_counts.items():\n            writer.add_scalar(f'Token_Distribution/token_{token}', count, global_step)\n        writer.add_scalar('Prediction_Length/mean_length', pred_length, global_step)\n        print(f\"Token distribution (Batch {b}): {dict(token_counts)}, Pred length: {pred_length}\")\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.416027Z","iopub.execute_input":"2025-03-11T20:53:16.416506Z","iopub.status.idle":"2025-03-11T20:53:16.430370Z","shell.execute_reply.started":"2025-03-11T20:53:16.416469Z","shell.execute_reply":"2025-03-11T20:53:16.428871Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Custom collate function</font>**","metadata":{}},{"cell_type":"code","source":"def custom_collate_fn(batch):\n    images, labels, label_lengths = zip(*batch)\n    # Stack images (all same size)\n    images = torch.stack(images, dim=0)\n    # Concatenate labels into a flat tensor\n    labels = torch.cat(labels, dim=0)\n    # Convert label_lengths to tensor\n    label_lengths = torch.tensor(label_lengths, dtype=torch.long)\n    return images, labels, label_lengths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.431923Z","iopub.execute_input":"2025-03-11T20:53:16.432418Z","iopub.status.idle":"2025-03-11T20:53:16.454363Z","shell.execute_reply.started":"2025-03-11T20:53:16.432376Z","shell.execute_reply":"2025-03-11T20:53:16.452864Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Analyze dataset char frequency</font>**","metadata":{}},{"cell_type":"code","source":"def analyze_dataset_char_frequency(labels_file, charset):\n    \"\"\"\n    Analyzes the character frequency in the generated dataset and displays a progress bar.\n    \n    Args:\n        labels_file (str): Path to the labels file (e.g., LABELS_FILE).\n        charset (str): String containing all possible characters (e.g., CHARSET).\n    \n    Returns:\n        dict: Dictionary with character frequencies.\n    \"\"\"\n    if not os.path.exists(labels_file):\n        print(f\"Error: Labels file '{labels_file}' does not exist!\")\n        return {}\n\n    # Read labels and extract texts\n    with open(labels_file, 'r') as f:\n        labels = [line.split('\\t')[1].strip() for line in f if '\\t' in line]\n\n    if not labels:\n        print(\"Error: No valid labels found in the file!\")\n        return {}\n\n    # Count character frequencies\n    all_chars = ''.join(labels)\n    char_counts = Counter(all_chars)\n\n    # Prepare data for the bar plot\n    chars = list(charset)\n    frequencies = [char_counts.get(char, 0) for char in chars]\n\n    # Create a progress bar (bar plot)\n    plt.figure(figsize=(12, 6))\n    plt.bar(chars, frequencies, color='skyblue')\n    plt.xlabel('Characters')\n    plt.ylabel('Frequency')\n    plt.title('Character Frequency in Dataset')\n    plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n    plt.tight_layout()\n\n    # Display the plot\n    plt.show()\n\n    # Print summary\n    total_chars = sum(frequencies)\n    print(f\"Total characters analyzed: {total_chars}\")\n    print(\"Character frequencies:\", dict(char_counts))\n\n    return dict(char_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.455796Z","iopub.execute_input":"2025-03-11T20:53:16.456496Z","iopub.status.idle":"2025-03-11T20:53:16.470460Z","shell.execute_reply.started":"2025-03-11T20:53:16.456437Z","shell.execute_reply":"2025-03-11T20:53:16.469242Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Generování datasetu</font>**","metadata":{}},{"cell_type":"code","source":"def create_synthetic_dataset(num_samples):\n    \"\"\"\n    Creates a synthetic dataset with images and corresponding labels.\n    \n    Args:\n        num_samples (int): Number of images to generate.\n    \n    Notes:\n        It uses global variables: OUTPUT_DIR, LABELS_FILE, font_files, MAX_TEXT_LENGTH, MIN_TEXT_LENGTH.\n        It assumes the existence of the generate_synthetic_image functions and access to font_files.\n    \"\"\"\n    labels = []\n    for i in range(num_samples):\n        # Generating text with minimum and maximum length\n        text = generate_random_text(MIN_TEXT_LENGTH, MAX_TEXT_LENGTH)\n        if not text:\n            continue  # Skip if text is empty (which should not happen with min_length=3)\n        \n        # Random font selection\n        font_path = random.choice(font_files)\n        \n        # Image generation\n        img = generate_synthetic_image(text, font_path)\n        img_name = f\"img_{i:05d}.png\"\n        img_path = os.path.join(OUTPUT_DIR, img_name)\n        img.save(img_path)\n        \n        # Saving a label\n        labels.append(f\"{img_name}\\t{text}\")  # Using the tab as a separator\n        \n        # Progress report\n        if i % 100 == 0:\n            print(f\"Generated {i}/{num_samples} images\")\n\n    # Saving labels to a file\n    if labels:\n        with open(LABELS_FILE, 'w') as f:\n            f.write(\"\\n\".join(labels))\n        print(f\"Dataset generated! Images saved in '{OUTPUT_DIR}', labels in '{LABELS_FILE}'\")\n        \n        # Analyze and display character frequency\n        analyze_dataset_char_frequency(LABELS_FILE, CHARSET)\n    else:\n        print(\"No labels generated!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.471934Z","iopub.execute_input":"2025-03-11T20:53:16.472265Z","iopub.status.idle":"2025-03-11T20:53:16.495414Z","shell.execute_reply.started":"2025-03-11T20:53:16.472237Z","shell.execute_reply":"2025-03-11T20:53:16.494012Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Mapping characters to indices and back</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"char_to_idx = {char: idx + 1 for idx, char in enumerate(CHARSET)}  # 0 is reserved for blank (CTC)\nidx_to_char = {idx: char for char, idx in char_to_idx.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.496690Z","iopub.execute_input":"2025-03-11T20:53:16.497128Z","iopub.status.idle":"2025-03-11T20:53:16.520517Z","shell.execute_reply.started":"2025-03-11T20:53:16.497087Z","shell.execute_reply":"2025-03-11T20:53:16.519098Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Custom dataset</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"class OCRDataset(Dataset):\n    def __init__(self, image_dir, labels_file):\n        self.image_dir = image_dir\n        self.labels_file = labels_file\n        self.data = []\n        with open(labels_file, 'r') as f:\n            for line in f:\n                if not line.strip():  # Skip empty lines\n                    continue\n                image_path, label = line.strip().split('\\t')\n                label_length = len(label)\n                self.data.append((image_path, label, label_length))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image_path, label, label_length = self.data[idx]\n        image = Image.open(os.path.join(self.image_dir, image_path)).convert('L')\n        image = transforms.ToTensor()(image)\n        label_encoded = torch.tensor([char_to_idx[c] for c in label], dtype=torch.long)\n        return image, label_encoded, label_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.521981Z","iopub.execute_input":"2025-03-11T20:53:16.522477Z","iopub.status.idle":"2025-03-11T20:53:16.540661Z","shell.execute_reply.started":"2025-03-11T20:53:16.522432Z","shell.execute_reply":"2025-03-11T20:53:16.539438Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Custom CTC Loss with Blank Penalty</font>**\n-------------------\n> CTC Loss with Entropy Regularization","metadata":{}},{"cell_type":"code","source":"class CTCLossWithBlankPenalty(nn.Module):\n    def __init__(self, blank=0, zero_infinity=True, blank_penalty_weight=1.0, entropy_weight=0.5, label_smoothing=0.1):\n        super().__init__()\n        self.ctc_loss = nn.CTCLoss(blank=blank, zero_infinity=zero_infinity)\n        self.blank_penalty_weight = blank_penalty_weight\n        self.entropy_weight = entropy_weight  # Zvýšeno z 0.1 na 0.5\n        self.label_smoothing = label_smoothing\n        self.global_step = 0\n\n    def update_blank_penalty(self, global_step):\n        # Postupné snižování penalizace během prvních 1,000 kroků\n        self.global_step = global_step\n        decay_factor = max(0.1, 1.0 - self.global_step / 1000.0)\n        self.blank_penalty_weight = 1.0 * decay_factor\n\n    def forward(self, log_probs, targets, input_lengths, target_lengths):\n        ctc_loss = self.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n        blank_probs = log_probs[:, :, 0].exp().mean()\n        blank_penalty = torch.clamp(-torch.log(1 - blank_probs + 1e-6) * self.blank_penalty_weight, max=1.0)\n        probs = log_probs.exp()\n        entropy = -torch.sum(probs * log_probs, dim=-1).mean()\n        total_loss = ctc_loss + blank_penalty + self.entropy_weight * entropy\n        return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.541884Z","iopub.execute_input":"2025-03-11T20:53:16.542359Z","iopub.status.idle":"2025-03-11T20:53:16.560633Z","shell.execute_reply.started":"2025-03-11T20:53:16.542315Z","shell.execute_reply":"2025-03-11T20:53:16.559414Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Model definition (CNN + RNN + CTC)</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"class OCRModel(nn.Module):\n    def __init__(self, num_chars):\n        super(OCRModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.rnn = nn.LSTM(128 * (IMG_HEIGHT // 4), 256, num_layers=2, bidirectional=True, dropout=DROPOUT)\n        self.fc = nn.Linear(256 * 2, num_chars)\n        self.dropout = nn.Dropout(DROPOUT)\n\n    def forward(self, x, max_length=None):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        batch, channels, height, width = x.size()\n        x = x.view(batch, channels * height, width).permute(2, 0, 1)\n        if max_length is not None:\n            x = x[:max_length]\n        x, _ = self.rnn(x)\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.562003Z","iopub.execute_input":"2025-03-11T20:53:16.562443Z","iopub.status.idle":"2025-03-11T20:53:16.589120Z","shell.execute_reply.started":"2025-03-11T20:53:16.562402Z","shell.execute_reply":"2025-03-11T20:53:16.587859Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Training</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, criterion, optimizer, device, epoch, warmup_steps=WARMUP_STEPS):\n    model.train()\n    total_loss = 0\n    global_step = epoch * len(train_loader)\n    \n    for batch_idx, (imgs, labels, label_lengths) in enumerate(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n        label_lengths = label_lengths.to(device)\n\n        if global_step < warmup_steps:\n            lr_scale = min(1.0, float(global_step + 1) / warmup_steps)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = LEARNING_RATE * lr_scale\n\n        optimizer.zero_grad()\n        # Dynamické nastavení MAX_SEQ_LENGTH\n        max_label_length = min(label_lengths.max().item() * 2, 16)\n        outputs = model(imgs, max_length=max_label_length)\n        outputs = outputs / TEMPERATURE\n        outputs = outputs.log_softmax(2)\n\n        batch_size = imgs.size(0)\n        seq_length = outputs.size(0)\n        input_lengths = torch.full((batch_size,), seq_length, dtype=torch.long).to(device)\n\n        loss = criterion(outputs, labels, input_lengths, label_lengths)\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"Warning: NaN or Inf loss at batch {batch_idx}. Skipping...\")\n            continue\n\n        loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Sníženo\n        optimizer.step()\n        total_loss += loss.item()\n        global_step += 1\n\n        if batch_idx % 10 == 0:\n            with torch.no_grad():\n                pred_texts = beam_search_decode(outputs, idx_to_char, label_lengths, global_step=global_step)\n                raw_outputs = outputs.argmax(2).cpu().numpy()[:3]\n                blank_probs = outputs[:, :, 0].exp().mean().item()\n                \n                # Average entropy as a measure of diversity\n                probs = outputs.exp()\n                entropy = -torch.sum(probs * outputs, dim=-1).mean().item()\n                \n                label_sequences = split_labels(labels, label_lengths)\n                ground_truth = [''.join([idx_to_char.get(idx.item(), '') for idx in label_seq])\n                                for label_seq in label_sequences[:3]]\n                pred_lengths = [len(p) for p in pred_texts]\n                avg_pred_length = sum(pred_lengths) / len(pred_lengths) if pred_lengths else 0\n                \n                writer.add_scalar('Loss/train_batch', loss.item(), global_step)\n                writer.add_scalar('Blank_Probability/train_batch', blank_probs, global_step)\n                writer.add_scalar('Gradient_Norm/train_batch', grad_norm.item(), global_step)\n                writer.add_scalar('Entropy/train_batch', entropy, global_step)  \n                writer.add_scalar('Prediction_Length/avg_length', avg_pred_length, global_step)\n                \n                print(f\"Batch {batch_idx}, Gradient norm: {grad_norm.item():.4f}\")\n                print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n                print(f\"Avg Blank Probability: {blank_probs:.4f}\")\n                print(f\"Sample predictions: {pred_texts[:3]}\")\n                print(f\"Ground Truth (first 3): {ground_truth}\")\n                print(f\"Raw outputs (first 3): {raw_outputs}\")\n                print(f\"Avg Pred Length: {avg_pred_length:.2f}, Input length: {seq_length}, Label lengths: {label_lengths[:3].tolist()}\")\n\n    avg_loss = total_loss / len(train_loader)\n    writer.add_scalar('Loss/train_epoch', avg_loss, epoch)\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.590604Z","iopub.execute_input":"2025-03-11T20:53:16.591091Z","iopub.status.idle":"2025-03-11T20:53:16.615269Z","shell.execute_reply.started":"2025-03-11T20:53:16.591044Z","shell.execute_reply":"2025-03-11T20:53:16.614008Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Inference (prediction)</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"def decode_prediction(output, idx_to_char):\n    probs = output.softmax(2)\n    max_probs, preds = probs.max(dim=2)\n    preds = preds.cpu().numpy()\n    max_probs = max_probs.cpu().numpy()\n    texts = []\n    for i, (pred, prob) in enumerate(zip(preds.T, max_probs.T)):\n        print(f\"Raw prediction {i} (pre-filter): {pred}, Max probs: {prob}\")\n        # Dynamic threshold: 75th percentile of max probs in this sequence\n        threshold = np.percentile(prob, 75)\n        print(f\"Dynamic threshold for prediction {i}: {threshold:.4f}\")\n        pred_text = []\n        prev = -1\n        for idx, p in zip(pred, prob):\n            if idx != 0 and idx != prev and p > threshold:\n                pred_text.append(idx_to_char.get(idx, ''))\n            prev = idx\n        decoded = ''.join(pred_text)\n        texts.append(decoded if decoded else '<empty>')\n    return texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.616825Z","iopub.execute_input":"2025-03-11T20:53:16.617350Z","iopub.status.idle":"2025-03-11T20:53:16.637849Z","shell.execute_reply.started":"2025-03-11T20:53:16.617271Z","shell.execute_reply":"2025-03-11T20:53:16.636146Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Main launch</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Inicializace TensorBoard writeru\n    log_dir = \"runs/ocr_experiment\"\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    writer = SummaryWriter(log_dir)\n\n    if not font_files:\n        print(\"Download some TrueType fonts (.ttf) and place them in the 'fonts' folder!\")\n    else:\n        create_synthetic_dataset(NUM_SAMPLES)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    full_dataset = OCRDataset(image_dir=OUTPUT_DIR, labels_file=LABELS_FILE)\n\n    for i in range(5):\n        img, label, length = full_dataset[i]\n        plt.imshow(img.squeeze(), cmap='gray')\n        plt.title(''.join([idx_to_char.get(idx.item(), '') for idx in label[:length]]))\n        plt.show()\n    \n    if len(full_dataset) == 0:\n        print(\"Dataset is empty! Check labels.txt or image directory.\")\n    else:\n        # Curriculum phases with pre-filtering\n        model = OCRModel(num_chars=len(CHARSET)).to(device)\n        criterion = CTCLossWithBlankPenalty(\n            blank=0, zero_infinity=True, blank_penalty_weight=BLANK_PENALTY_WEIGHT,\n            entropy_weight=ENTROPY_WEIGHT, label_smoothing=LABEL_SMOOTHING\n        )\n        \n        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        #scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\n        best_loss = float('inf')\n        \n        for epoch in range(EPOCHS):\n            # Filter full dataset based on curriculum phase\n            if epoch < 5:\n                filtered_data = [(img, lbl, lbl_len) for img, lbl, lbl_len in full_dataset.data if lbl_len <= 5]\n                sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]  # lbl je řetězec\n                print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n            elif epoch < 10:\n                filtered_data = [(img, lbl, lbl_len) for img, lbl, lbl_len in full_dataset.data if lbl_len <= 7]\n                sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]\n                print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n            else:\n                filtered_data = full_dataset.data\n                sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]\n                print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n\n            # Create a new dataset with filtered data\n            curr_dataset = OCRDataset(image_dir=OUTPUT_DIR, labels_file=LABELS_FILE)\n            curr_dataset.data = filtered_data  # Overwrite with filtered data\n\n            # Split into train and validation\n            train_size = int(0.8 * len(curr_dataset))\n            val_size = len(curr_dataset) - train_size\n            train_dataset, val_dataset = torch.utils.data.random_split(curr_dataset, [train_size, val_size])\n            print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n            \n            # Create data loaders\n            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n            \n            # Training with TensorBoard logging\n            model.train()\n            total_loss = 0\n            global_step = epoch * len(train_loader)\n            for batch_idx, (imgs, labels, label_lengths) in enumerate(train_loader):\n                imgs, labels = imgs.to(device), labels.to(device)\n                label_lengths = label_lengths.to(device)\n\n                if global_step < WARMUP_STEPS:\n                    lr_scale = min(1.0, float(global_step + 1) / WARMUP_STEPS)\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = LEARNING_RATE * lr_scale\n\n                optimizer.zero_grad()\n                outputs = model(imgs)\n                outputs = outputs / TEMPERATURE\n                outputs = outputs.log_softmax(2)\n\n                batch_size = imgs.size(0)\n                seq_length = outputs.size(0)\n                input_lengths = torch.full((batch_size,), seq_length, dtype=torch.long).to(device)\n\n                loss = criterion(outputs, labels, input_lengths, label_lengths)\n                if torch.isnan(loss) or torch.isinf(loss):\n                    print(f\"Warning: NaN or Inf loss at batch {batch_idx}. Skipping...\")\n                    continue\n\n                loss.backward()\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n                optimizer.step()\n                total_loss += loss.item()\n                global_step += 1\n\n                if batch_idx % 10 == 0:\n                    with torch.no_grad():\n                        pred_texts = beam_search_decode(outputs, idx_to_char, label_lengths)\n                        raw_outputs = outputs.argmax(2).cpu().numpy()[:3]\n                        blank_probs = outputs[:, :, 0].exp().mean().item()\n                        label_sequences = split_labels(labels, label_lengths)\n                        ground_truth = [''.join([idx_to_char.get(idx.item(), '') for idx in label_seq])\n                                        for label_seq in label_sequences[:3]]\n                        \n                        # Logging into TensorBoard\n                        writer.add_scalar('Loss/train_batch', loss.item(), global_step)\n                        writer.add_scalar('Blank_Probability/train_batch', blank_probs, global_step)\n                        writer.add_scalar('Gradient_Norm/train_batch', grad_norm.item(), global_step)\n                        writer.add_histogram('Logits/train_probs', outputs.exp().flatten(), global_step)\n\n                        # Adding raw outputs\n                        writer.add_histogram('Raw_Outputs/train_argmax', raw_outputs.flatten(), global_step)\n                        writer.add_text('Raw_Outputs/train_text', f\"Raw train outputs (argmax):\\n{str(raw_outputs)}\", global_step)\n\n                        # Token frequency calculation and logging\n                        token_counts = Counter(raw_outputs.flatten())\n                        writer.add_text(\n                            'Raw_Outputs/train_token_counts',\n                            f\"Token counts: {token_counts}\",\n                            global_step\n                        )\n                        \n                        print(f\"Batch {batch_idx}, Gradient norm: {grad_norm.item():.4f}\")\n                        print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n                        print(f\"Avg Blank Probability: {blank_probs:.4f}\")\n                        print(f\"Sample predictions: {pred_texts[:3]}\")\n                        print(f\"Ground Truth (first 3): {ground_truth}\")\n                        print(f\"Raw outputs (first 3): {raw_outputs}\")\n                        print(f\"Input length: {seq_length}, Label lengths: {label_lengths[:3].tolist()}\")\n\n            avg_loss = total_loss / len(train_loader)\n            writer.add_scalar('Loss/train_epoch', avg_loss, epoch)\n            print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n\n            # Validation with TensorBoard logging\n            model.eval()\n            val_loss = 0\n            val_blank_probs = 0\n            with torch.no_grad():\n                for batch_idx, (imgs, labels, label_lengths) in enumerate(val_loader):\n                    imgs, labels = imgs.to(device), labels.to(device)\n                    label_lengths = label_lengths.to(device)\n                    outputs = model(imgs)\n                    outputs = outputs.log_softmax(2)\n                    seq_length = outputs.size(0)\n                    input_lengths = torch.full((imgs.size(0),), seq_length, dtype=torch.long).to(device)\n                    val_loss += criterion(outputs, labels, input_lengths, label_lengths).item()\n                    val_blank_probs += outputs[:, :, 0].exp().mean().item()\n\n                    # Logging raw outputs for the first batch\n                    if batch_idx == 0:  # We only log the first batch to save space\n\n                        # Control of the form and content of outputs\n                        print(f\"Outputs shape: {outputs.shape}, Outputs[0] shape: {outputs[0].shape}\")\n                        if outputs.numel() == 0 or torch.isnan(outputs).any() or torch.isinf(outputs).any():\n                            print(\"Warning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\")\n                            continue\n                        \n                        # 1. Histogram of the distribution of predicted tokens (argmax)\n                        raw_outputs = outputs.argmax(2).cpu().numpy()  # [seq_length, batch_size]\n                        writer.add_histogram(\n                            'Raw_Outputs/val_argmax', \n                            raw_outputs.flatten(),  # We convert to a 1D array for the histogram\n                            global_step=epoch\n                        )\n            \n                        # 2. Text listing of raw outputs (first 5 sequences)\n                        raw_outputs_text = str(raw_outputs[:5])  # First 5 sequences as text\n                        writer.add_text(\n                            'Raw_Outputs/val_text',\n                            f\"Raw validation outputs (argmax) for first batch:\\n{raw_outputs_text}\",\n                            global_step=epoch\n                        )\n\n                        # 3. Calculating token frequency and logging to TensorBoard\n                        token_counts = Counter(raw_outputs.flatten())\n                        writer.add_text(\n                            'Raw_Outputs/val_token_counts',\n                            f\"Token counts: {token_counts}\",\n                            global_step=epoch\n                        )\n            \n                        # 4. (Optional) Logit histogram for specific tokens (e.g. first time step)\n                        logits_first_step = outputs[0].cpu().numpy()  # [batch_size, num_chars]\n                        writer.add_histogram(\n                            'Raw_Outputs/val_logits_first_step',\n                            logits_first_step.flatten(),\n                            global_step=epoch\n                        )\n                        \n                        logits_first_step = outputs[0].cpu().numpy()\n                        writer.add_histogram('Logits/val_probs', torch.softmax(outputs[0], dim=-1).flatten(), global_step)\n                \n                val_loss /= len(val_loader)\n                val_blank_probs /= len(val_loader)\n                pred_texts = beam_search_decode(outputs, idx_to_char, label_lengths)\n                label_sequences = split_labels(labels, label_lengths)\n                ground_truth = [''.join([idx_to_char.get(idx.item(), '') for idx in label_seq])\n                                for label_seq in label_sequences[:5]]\n\n                # Calculating edit distance\n                edit_distances = []\n                for pred, gt in zip(pred_texts, ground_truth):\n                    edit_distances.append(levenshtein_distance(pred, gt))\n                avg_edit_distance = sum(edit_distances) / len(edit_distances) if edit_distances else 0\n                \n                # Validation logging to TensorBoard\n                writer.add_scalar('Loss/val_epoch', val_loss, epoch)\n                writer.add_scalar('Blank_Probability/val_epoch', val_blank_probs, epoch)\n                writer.add_text('Predictions/val', f\"Validation Predictions: {pred_texts[:5]}\", epoch)\n                writer.add_text('Ground_Truth/val', f\"Ground Truth: {ground_truth}\", epoch)\n                print(f\"Validation Loss: {val_loss:.4f}\")\n                print(\"Validation Predictions:\", pred_texts[:5])\n                print(\"Ground Truth:\", ground_truth)\n\n            #scheduler.step()\n            scheduler.step(val_loss)\n            print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']}\")\n\n            if val_loss < best_loss:\n                best_loss = val_loss\n                torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'best_ocr_model.pth'))\n\n        torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'final_ocr_model.pth'))\n    \n    # Closing the TensorBoard writer\n    writer.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:53:16.639622Z","iopub.execute_input":"2025-03-11T20:53:16.640123Z","iopub.status.idle":"2025-03-11T21:21:25.776727Z","shell.execute_reply.started":"2025-03-11T20:53:16.640072Z","shell.execute_reply":"2025-03-11T21:21:25.775535Z"}},"outputs":[{"name":"stdout","text":"Generated 0/10240 images\nGenerated 100/10240 images\nGenerated 200/10240 images\nGenerated 300/10240 images\nGenerated 400/10240 images\nGenerated 500/10240 images\nGenerated 600/10240 images\nGenerated 700/10240 images\nGenerated 800/10240 images\nGenerated 900/10240 images\nGenerated 1000/10240 images\nGenerated 1100/10240 images\nGenerated 1200/10240 images\nGenerated 1300/10240 images\nGenerated 1400/10240 images\nGenerated 1500/10240 images\nGenerated 1600/10240 images\nGenerated 1700/10240 images\nGenerated 1800/10240 images\nGenerated 1900/10240 images\nGenerated 2000/10240 images\nGenerated 2100/10240 images\nGenerated 2200/10240 images\nGenerated 2300/10240 images\nGenerated 2400/10240 images\nGenerated 2500/10240 images\nGenerated 2600/10240 images\nGenerated 2700/10240 images\nGenerated 2800/10240 images\nGenerated 2900/10240 images\nGenerated 3000/10240 images\nGenerated 3100/10240 images\nGenerated 3200/10240 images\nGenerated 3300/10240 images\nGenerated 3400/10240 images\nGenerated 3500/10240 images\nGenerated 3600/10240 images\nGenerated 3700/10240 images\nGenerated 3800/10240 images\nGenerated 3900/10240 images\nGenerated 4000/10240 images\nGenerated 4100/10240 images\nGenerated 4200/10240 images\nGenerated 4300/10240 images\nGenerated 4400/10240 images\nGenerated 4500/10240 images\nGenerated 4600/10240 images\nGenerated 4700/10240 images\nGenerated 4800/10240 images\nGenerated 4900/10240 images\nGenerated 5000/10240 images\nGenerated 5100/10240 images\nGenerated 5200/10240 images\nGenerated 5300/10240 images\nGenerated 5400/10240 images\nGenerated 5500/10240 images\nGenerated 5600/10240 images\nGenerated 5700/10240 images\nGenerated 5800/10240 images\nGenerated 5900/10240 images\nGenerated 6000/10240 images\nGenerated 6100/10240 images\nGenerated 6200/10240 images\nGenerated 6300/10240 images\nGenerated 6400/10240 images\nGenerated 6500/10240 images\nGenerated 6600/10240 images\nGenerated 6700/10240 images\nGenerated 6800/10240 images\nGenerated 6900/10240 images\nGenerated 7000/10240 images\nGenerated 7100/10240 images\nGenerated 7200/10240 images\nGenerated 7300/10240 images\nGenerated 7400/10240 images\nGenerated 7500/10240 images\nGenerated 7600/10240 images\nGenerated 7700/10240 images\nGenerated 7800/10240 images\nGenerated 7900/10240 images\nGenerated 8000/10240 images\nGenerated 8100/10240 images\nGenerated 8200/10240 images\nGenerated 8300/10240 images\nGenerated 8400/10240 images\nGenerated 8500/10240 images\nGenerated 8600/10240 images\nGenerated 8700/10240 images\nGenerated 8800/10240 images\nGenerated 8900/10240 images\nGenerated 9000/10240 images\nGenerated 9100/10240 images\nGenerated 9200/10240 images\nGenerated 9300/10240 images\nGenerated 9400/10240 images\nGenerated 9500/10240 images\nGenerated 9600/10240 images\nGenerated 9700/10240 images\nGenerated 9800/10240 images\nGenerated 9900/10240 images\nGenerated 10000/10240 images\nGenerated 10100/10240 images\nGenerated 10200/10240 images\nDataset generated! Images saved in '/kaggle/working/synthetic_data/images', labels in '/kaggle/working/synthetic_data/labels.txt'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Total characters analyzed: 82331\nCharacter frequencies: {'q': 1271, 'V': 1304, '8': 1291, 'i': 1267, 'b': 1333, 'H': 1347, 'M': 1261, 'd': 1265, 'I': 1263, '1': 1287, 'X': 1312, 'S': 1359, '0': 1306, 'D': 1230, 'R': 1223, 'J': 1291, 't': 1277, 'c': 1213, 'L': 1292, '2': 1354, 'e': 1279, 'z': 1280, 'f': 1331, 'O': 1249, 'U': 1320, 'r': 1328, 'W': 1239, 'h': 1301, 'Q': 1221, '7': 1211, 'T': 1271, 'n': 1295, 'N': 1258, 'C': 1299, 'E': 1289, 'P': 1284, 'x': 1276, '3': 1269, 'y': 1262, '9': 1320, 'p': 1281, 'v': 1280, 'a': 1293, 'l': 1310, '-': 1351, '6': 1305, 'K': 1311, 'G': 1269, 'Y': 1299, 'k': 1309, 'u': 1282, 's': 1280, 'A': 1256, 'j': 1257, 'Z': 1308, 'o': 1240, '4': 1307, 'm': 1264, 'w': 1305, 'g': 1270, '*': 1262, '5': 1326, 'B': 1315, 'F': 1323}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Filtered data size: 2176, Sample labels: ['MdI8', '0tcL2', 'eyOxJ']\nTrain size: 1740, Val size: 436\nToken distribution (Batch 0): {1: 3, 4: 1, 47: 1, 11: 1, 34: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 1): {1: 7, 10: 1}, Pred length: 8\nToken distribution (Batch 2): {1: 5, 10: 2, 34: 1, 47: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 3): {10: 3, 1: 4, 47: 1, 6: 1, 56: 1}, Pred length: 10\nToken distribution (Batch 4): {1: 7, 10: 1}, Pred length: 8\nToken distribution (Batch 5): {1: 4, 10: 3, 11: 1}, Pred length: 8\nToken distribution (Batch 6): {1: 9, 10: 1}, Pred length: 10\nToken distribution (Batch 7): {47: 2, 11: 1, 4: 1, 5: 1, 1: 2, 34: 2, 10: 1}, Pred length: 10\nToken distribution (Batch 8): {10: 2, 1: 4, 32: 3, 4: 1}, Pred length: 10\nToken distribution (Batch 9): {1: 5, 10: 1, 5: 1, 4: 1}, Pred length: 8\nToken distribution (Batch 10): {1: 3, 32: 3, 34: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 11): {1: 5, 42: 1, 10: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 12): {34: 2, 1: 4, 10: 1, 22: 1}, Pred length: 8\nToken distribution (Batch 13): {1: 4, 10: 4, 32: 2}, Pred length: 10\nToken distribution (Batch 14): {63: 1, 1: 4, 10: 1, 50: 1, 22: 1}, Pred length: 8\nToken distribution (Batch 15): {1: 5, 11: 1, 35: 1, 34: 1, 10: 2}, Pred length: 10\nToken distribution (Batch 16): {1: 9, 11: 1}, Pred length: 10\nToken distribution (Batch 17): {10: 2, 19: 1, 32: 2, 22: 1, 1: 2}, Pred length: 8\nToken distribution (Batch 18): {1: 5, 10: 1, 34: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 19): {1: 5, 47: 2, 50: 1, 10: 1, 56: 1}, Pred length: 10\nToken distribution (Batch 20): {1: 6, 56: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 21): {1: 7, 11: 1}, Pred length: 8\nToken distribution (Batch 22): {1: 7, 47: 1, 32: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 23): {10: 2, 1: 6}, Pred length: 8\nToken distribution (Batch 24): {10: 2, 32: 1, 1: 2, 11: 2, 43: 1}, Pred length: 8\nToken distribution (Batch 25): {34: 1, 1: 4, 47: 1, 4: 1, 6: 1}, Pred length: 8\nToken distribution (Batch 26): {10: 1, 34: 1, 47: 1, 1: 4, 32: 1}, Pred length: 8\nToken distribution (Batch 27): {1: 7, 10: 1}, Pred length: 8\nToken distribution (Batch 28): {10: 2, 1: 5, 47: 1, 27: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 29): {10: 1, 56: 1, 1: 4, 34: 2, 47: 1, 22: 1}, Pred length: 10\nToken distribution (Batch 30): {10: 2, 34: 2, 1: 2, 56: 1, 27: 1, 47: 2}, Pred length: 10\nToken distribution (Batch 31): {10: 3, 1: 5, 42: 1, 6: 1}, Pred length: 10\nBatch 0, Gradient norm: 20.3753\nEpoch 1, Batch 0/55, Loss: 28.5301\nAvg Blank Probability: 0.0147\nSample predictions: ['adaUkHaA', 'aja', 'ajaHUFj']\nGround Truth (first 3): ['r4zP', 'CYGO', 'Wd0xD']\nRaw outputs (first 3): [[ 1  1  1 10  1  1  1 47 10  1  1  1 34  1 63  1  1 10  1  1  1  1  1 10\n  10 34 10  1 10 10 10 10]\n [ 4  1  1  1  1  1  1 11 10  1  1 42  1 10  1  1  1 10 10  1  1  1 47  1\n  10  1 34  1  1 56 34  1]\n [ 1  1  1 10  1  1  1  4  1 10 32  1 10 10 10 11 11 19  1  1  1  1  1  1\n  32 47 47  1 10  1 34 10]]\nInput length: 32, Label lengths: [4, 4, 5]\nToken distribution (Batch 0): {1: 4, 10: 4, 19: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 1): {42: 1, 1: 6, 10: 1, 27: 1, 5: 1}, Pred length: 10\nToken distribution (Batch 2): {1: 1, 10: 4, 11: 3, 32: 2}, Pred length: 10\nToken distribution (Batch 3): {1: 4, 19: 2, 5: 1, 10: 1, 34: 2}, Pred length: 10\nToken distribution (Batch 4): {1: 2, 34: 1, 10: 2, 32: 1, 56: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 5): {1: 3, 32: 1, 47: 1, 10: 3}, Pred length: 8\nToken distribution (Batch 6): {11: 1, 1: 4, 22: 1, 32: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 7): {6: 1, 10: 2, 1: 4, 27: 1}, Pred length: 8\nToken distribution (Batch 8): {11: 2, 1: 4, 10: 1, 4: 1}, Pred length: 8\nToken distribution (Batch 9): {1: 4, 10: 2, 32: 2}, Pred length: 8\nToken distribution (Batch 10): {1: 6, 10: 2}, Pred length: 8\nToken distribution (Batch 11): {22: 2, 19: 1, 63: 1, 6: 1, 1: 2, 48: 1, 32: 2}, Pred length: 10\nToken distribution (Batch 12): {10: 3, 1: 4, 34: 1}, Pred length: 8\nToken distribution (Batch 13): {11: 1, 1: 4, 27: 1, 34: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 14): {10: 2, 1: 7, 34: 1}, Pred length: 10\nToken distribution (Batch 15): {1: 8, 32: 1, 4: 1}, Pred length: 10\nToken distribution (Batch 16): {1: 6, 10: 2, 34: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 17): {1: 8, 32: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 18): {32: 1, 6: 1, 1: 5, 10: 1}, Pred length: 8\nToken distribution (Batch 19): {1: 5, 12: 1, 10: 3, 27: 1}, Pred length: 10\nToken distribution (Batch 20): {6: 1, 1: 6, 10: 2, 12: 1}, Pred length: 10\nToken distribution (Batch 21): {1: 4, 42: 1, 22: 1, 32: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 22): {1: 5, 48: 1, 56: 1, 10: 1, 32: 2}, Pred length: 10\nToken distribution (Batch 23): {1: 7, 47: 1, 32: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 24): {1: 8, 47: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 25): {10: 3, 1: 4, 11: 1}, Pred length: 8\nToken distribution (Batch 26): {10: 1, 1: 5, 25: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 27): {1: 5, 32: 1, 27: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 28): {10: 1, 1: 7}, Pred length: 8\nToken distribution (Batch 29): {10: 2, 12: 1, 47: 1, 27: 1, 34: 1, 1: 2, 32: 2}, Pred length: 10\nToken distribution (Batch 30): {1: 8}, Pred length: 8\nToken distribution (Batch 31): {1: 3, 32: 2, 34: 1, 10: 1, 27: 1}, Pred length: 8\nBatch 10, Gradient norm: 17.6153\nEpoch 1, Batch 10/55, Loss: 28.3084\nAvg Blank Probability: 0.0147\nSample predictions: ['ajajsajH', 'PajaAae', 'ajkjkFj']\nGround Truth (first 3): ['orfUI', 'F8ti2', '4ABxD']\nRaw outputs (first 3): [[ 1 42  1  1  1  1 11  6 11  1  1 22 10 11 10  1  1  1 32  1  6  1  1  1\n   1 10 10  1 10 10  1  1]\n [ 1  1 10  1 34 32  1 10  1  1 10 19  1  1  1  1 10  1  6 12  1 42  1  1\n   1  1  1  1  1 12  1  1]\n [10  1 11 19  1  1  1  1 10  1 10 63  1 27 34  1  1  1  1 10  1  1 48  1\n   1 11  1  1  1 10  1 32]]\nInput length: 32, Label lengths: [5, 5, 5]\nToken distribution (Batch 0): {1: 7, 22: 1, 10: 2}, Pred length: 10\nToken distribution (Batch 1): {10: 1, 1: 5, 47: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 2): {1: 5, 34: 2, 11: 1}, Pred length: 8\nToken distribution (Batch 3): {1: 8}, Pred length: 8\nToken distribution (Batch 4): {1: 7, 25: 1, 34: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 5): {34: 1, 1: 6, 10: 1, 47: 1, 19: 1}, Pred length: 10\nToken distribution (Batch 6): {47: 1, 1: 5, 10: 2}, Pred length: 8\nToken distribution (Batch 7): {10: 2, 1: 6, 12: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 8): {11: 1, 1: 5, 6: 1, 34: 1, 10: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 9): {1: 8}, Pred length: 8\nToken distribution (Batch 10): {1: 6, 10: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 11): {10: 1, 34: 2, 1: 4, 47: 2, 22: 1}, Pred length: 10\nToken distribution (Batch 12): {63: 1, 34: 2, 1: 5}, Pred length: 8\nToken distribution (Batch 13): {10: 3, 1: 3, 32: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 14): {34: 1, 1: 9}, Pred length: 10\nToken distribution (Batch 15): {1: 8}, Pred length: 8\nToken distribution (Batch 16): {56: 1, 1: 6, 19: 1}, Pred length: 8\nToken distribution (Batch 17): {6: 1, 1: 6, 19: 1, 10: 2}, Pred length: 10\nToken distribution (Batch 18): {10: 2, 1: 4, 35: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 19): {1: 5, 19: 1, 4: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 20): {1: 9, 10: 1}, Pred length: 10\nToken distribution (Batch 21): {1: 5, 34: 1, 10: 2, 24: 1, 22: 1}, Pred length: 10\nToken distribution (Batch 22): {4: 1, 10: 4, 1: 5}, Pred length: 10\nToken distribution (Batch 23): {1: 4, 34: 1, 10: 2, 19: 1}, Pred length: 8\nToken distribution (Batch 24): {11: 1, 34: 2, 1: 4, 56: 1}, Pred length: 8\nToken distribution (Batch 25): {1: 6, 34: 1, 5: 1, 4: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 26): {1: 7, 10: 1}, Pred length: 8\nToken distribution (Batch 27): {1: 8, 11: 2}, Pred length: 10\nToken distribution (Batch 28): {34: 1, 1: 4, 10: 3}, Pred length: 8\nToken distribution (Batch 29): {34: 1, 1: 5, 42: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 30): {1: 8}, Pred length: 8\nToken distribution (Batch 31): {1: 4, 22: 1, 10: 1, 32: 1, 47: 1}, Pred length: 8\nBatch 20, Gradient norm: 30.2888\nEpoch 1, Batch 20/55, Loss: 28.6894\nAvg Blank Probability: 0.0148\nSample predictions: ['avja', 'jaUaFa', 'aHaHka']\nGround Truth (first 3): ['Pqwm*', 'hMt9', 'eEja']\nRaw outputs (first 3): [[ 1 10  1  1  1 34 47 10 11  1  1 10 63 10 34  1 56  6 10  1  1  1  4  1\n  11  1  1  1 34 34  1  1]\n [ 1  1 34  1  1  1  1  1  1  1 10 34 34 10  1  1  1  1 10  1 10 34 10 34\n  34  1  1  1  1  1  1  1]\n [ 1  1  1  1  1  1  1 10  6  1  1  1  1  1  1  1  1  1  1 19  1 10  1  1\n   1 34 10  1  1  1  1 22]]\nInput length: 32, Label lengths: [5, 4, 4]\nToken distribution (Batch 0): {10: 3, 1: 4, 4: 1}, Pred length: 8\nToken distribution (Batch 1): {1: 8, 10: 2}, Pred length: 10\nToken distribution (Batch 2): {56: 1, 1: 4, 27: 1, 47: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 3): {1: 6, 10: 2}, Pred length: 8\nToken distribution (Batch 4): {10: 1, 1: 7}, Pred length: 8\nToken distribution (Batch 5): {10: 3, 34: 1, 1: 5, 12: 1}, Pred length: 10\nToken distribution (Batch 6): {1: 8}, Pred length: 8\nToken distribution (Batch 7): {11: 1, 4: 1, 1: 3, 34: 1, 47: 1, 5: 1}, Pred length: 8\nToken distribution (Batch 8): {10: 2, 1: 6}, Pred length: 8\nToken distribution (Batch 9): {1: 7, 35: 1}, Pred length: 8\nToken distribution (Batch 10): {1: 4, 6: 1, 34: 1, 56: 2, 10: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 11): {1: 7, 4: 1, 32: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 12): {1: 9, 56: 1}, Pred length: 10\nToken distribution (Batch 13): {10: 2, 1: 7, 34: 1}, Pred length: 10\nToken distribution (Batch 14): {1: 4, 10: 4, 34: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 15): {34: 1, 32: 1, 6: 1, 1: 7}, Pred length: 10\nToken distribution (Batch 16): {34: 1, 1: 7, 10: 1, 56: 1}, Pred length: 10\nToken distribution (Batch 17): {10: 2, 1: 4, 34: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 18): {11: 1, 10: 2, 34: 1, 1: 6}, Pred length: 10\nToken distribution (Batch 19): {1: 6, 50: 1, 6: 1}, Pred length: 8\nToken distribution (Batch 20): {1: 7, 32: 1, 10: 2}, Pred length: 10\nToken distribution (Batch 21): {10: 3, 1: 4, 34: 1, 4: 1, 24: 1}, Pred length: 10\nToken distribution (Batch 22): {11: 1, 1: 6, 10: 1}, Pred length: 8\nToken distribution (Batch 23): {1: 7, 4: 1}, Pred length: 8\nToken distribution (Batch 24): {1: 1, 56: 1, 22: 1, 19: 3, 11: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 25): {1: 9, 10: 1}, Pred length: 10\nToken distribution (Batch 26): {11: 1, 4: 4, 1: 4, 10: 1}, Pred length: 10\nToken distribution (Batch 27): {11: 1, 1: 2, 34: 1, 10: 1, 6: 2, 32: 1}, Pred length: 8\nToken distribution (Batch 28): {1: 6, 42: 1, 34: 2, 10: 1}, Pred length: 10\nToken distribution (Batch 29): {1: 7, 10: 2, 4: 1}, Pred length: 10\nToken distribution (Batch 30): {1: 6, 11: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 31): {47: 2, 34: 2, 1: 1, 48: 1, 10: 1, 11: 1}, Pred length: 8\nBatch 30, Gradient norm: 17.4162\nEpoch 1, Batch 30/55, Loss: 27.9841\nAvg Blank Probability: 0.0150\nSample predictions: ['jajada', 'aja', '3aAUHa']\nGround Truth (first 3): ['oNhy', '7prWD', '-4dg']\nRaw outputs (first 3): [[10  1 56  1 10 10  1 11 10  1  1  1  1 10  1 34 34 10 11  1  1 10 11  1\n   1  1 11 11  1  1  1 47]\n [10 10  1 10  1 34  1  4  1  1  6  1  1  1  1 32  1  1 10 50  1  1  1  4\n  56  1  4  1 42  1 11 34]\n [ 1 10  1 10  1  1  1  1  1  1  1  4  1 10 10  6 10  1 34  1  1 10  1  1\n  22 10  1  1 34  1  1 34]]\nInput length: 32, Label lengths: [4, 5, 4]\nToken distribution (Batch 0): {11: 1, 32: 1, 1: 4, 34: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 1): {1: 3, 35: 1, 22: 1, 34: 2, 10: 1}, Pred length: 8\nToken distribution (Batch 2): {1: 6, 12: 1, 34: 1, 10: 2}, Pred length: 10\nToken distribution (Batch 3): {1: 5, 10: 3, 27: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 4): {1: 10}, Pred length: 10\nToken distribution (Batch 5): {47: 1, 1: 5, 34: 1, 10: 1, 11: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 6): {1: 6, 34: 1, 47: 1, 32: 1, 19: 1}, Pred length: 10\nToken distribution (Batch 7): {1: 10}, Pred length: 10\nToken distribution (Batch 8): {1: 6, 32: 1, 6: 1}, Pred length: 8\nToken distribution (Batch 9): {1: 4, 32: 1, 47: 1, 10: 3, 34: 1}, Pred length: 10\nToken distribution (Batch 10): {10: 3, 1: 3, 11: 1, 56: 1, 34: 2}, Pred length: 10\nToken distribution (Batch 11): {1: 6, 4: 1, 22: 1, 32: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 12): {10: 2, 11: 1, 1: 3, 6: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 13): {1: 6, 11: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 14): {11: 1, 10: 5, 1: 2}, Pred length: 8\nToken distribution (Batch 15): {1: 10}, Pred length: 10\nToken distribution (Batch 16): {1: 4, 10: 2, 32: 2, 34: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 17): {1: 4, 11: 1, 10: 1, 32: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 18): {1: 4, 34: 1, 10: 2, 5: 1}, Pred length: 8\nToken distribution (Batch 19): {1: 6, 10: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 20): {1: 6, 34: 2}, Pred length: 8\nToken distribution (Batch 21): {10: 2, 1: 5, 22: 1}, Pred length: 8\nToken distribution (Batch 22): {1: 5, 10: 2, 34: 1, 47: 1, 5: 1}, Pred length: 10\nToken distribution (Batch 23): {1: 6, 32: 2}, Pred length: 8\nToken distribution (Batch 24): {1: 6, 11: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 25): {56: 1, 1: 4, 12: 1, 63: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 26): {34: 1, 1: 3, 6: 1, 5: 2, 22: 1, 4: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 27): {1: 6, 11: 2}, Pred length: 8\nToken distribution (Batch 28): {10: 1, 6: 1, 1: 5, 4: 1}, Pred length: 8\nToken distribution (Batch 29): {1: 5, 10: 2, 34: 1}, Pred length: 8\nToken distribution (Batch 30): {1: 9, 32: 1}, Pred length: 10\nToken distribution (Batch 31): {1: 8, 10: 1, 32: 1}, Pred length: 10\nBatch 40, Gradient norm: 18.4922\nEpoch 1, Batch 40/55, Loss: 28.1484\nAvg Blank Probability: 0.0153\nSample predictions: ['kFaHaj', 'aIvHja', 'alaHja']\nGround Truth (first 3): ['BeMt', '*omn', '2laF3']\nRaw outputs (first 3): [[11  1  1  1  1 47  1  1  1  1 10  1 10  1 11  1  1  1  1  1  1 10  1  1\n   1 56 34  1 10  1  1  1]\n [32  1  1 10  1  1  1  1  1  1  1  1 11 11 10  1  1 11  1  1 34  1 10  1\n  11  1  1  1  6 10  1  1]\n [ 1 35 12  1  1  1 34  1  1 32  1  4  1  1 10  1  1 10  1  1  1  1 10  1\n  32 12  6  1  1  1  1  1]]\nInput length: 32, Label lengths: [4, 4, 5]\nToken distribution (Batch 0): {1: 6, 34: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 1): {1: 6, 11: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 2): {10: 4, 1: 4, 32: 1, 19: 1}, Pred length: 10\nToken distribution (Batch 3): {4: 1, 1: 2, 19: 1, 10: 2, 32: 1, 5: 1}, Pred length: 8\nToken distribution (Batch 4): {1: 7, 12: 1, 10: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 5): {10: 1, 1: 5, 50: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 6): {11: 2, 47: 1, 1: 5}, Pred length: 8\nToken distribution (Batch 7): {11: 1, 1: 6, 10: 1, 32: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 8): {4: 1, 1: 4, 19: 1, 47: 1, 32: 1, 50: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 9): {1: 7, 34: 1, 56: 1, 55: 1}, Pred length: 10\nToken distribution (Batch 10): {42: 1, 10: 2, 32: 1, 47: 1, 1: 3}, Pred length: 8\nToken distribution (Batch 11): {1: 6, 4: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 12): {1: 10}, Pred length: 10\nToken distribution (Batch 13): {11: 1, 22: 1, 10: 2, 1: 4}, Pred length: 8\nToken distribution (Batch 14): {34: 3, 56: 1, 1: 2, 19: 1, 11: 1}, Pred length: 8\nToken distribution (Batch 15): {1: 6, 22: 1, 11: 1}, Pred length: 8\nToken distribution (Batch 16): {1: 7, 32: 1, 10: 1, 19: 1}, Pred length: 10\nToken distribution (Batch 17): {1: 6, 10: 3, 22: 1}, Pred length: 10\nToken distribution (Batch 18): {1: 4, 34: 3, 22: 1}, Pred length: 8\nToken distribution (Batch 19): {1: 6, 10: 2, 32: 2}, Pred length: 10\nToken distribution (Batch 20): {1: 8, 10: 1, 22: 1}, Pred length: 10\nToken distribution (Batch 21): {11: 1, 1: 7, 4: 1, 42: 1}, Pred length: 10\nToken distribution (Batch 22): {1: 5, 10: 2, 11: 1}, Pred length: 8\nToken distribution (Batch 23): {1: 6, 10: 2, 19: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 24): {1: 4, 32: 1, 47: 2, 10: 1, 4: 1, 55: 1}, Pred length: 10\nToken distribution (Batch 25): {1: 9, 34: 1}, Pred length: 10\nToken distribution (Batch 26): {1: 9, 47: 1}, Pred length: 10\nToken distribution (Batch 27): {34: 2, 1: 3, 4: 2, 32: 2, 47: 1}, Pred length: 10\nToken distribution (Batch 28): {1: 7, 56: 1}, Pred length: 8\nToken distribution (Batch 29): {11: 1, 56: 1, 1: 8}, Pred length: 10\nToken distribution (Batch 30): {10: 3, 1: 6, 32: 1}, Pred length: 10\nToken distribution (Batch 31): {1: 4, 11: 1, 32: 1, 34: 1, 47: 1}, Pred length: 8\nBatch 50, Gradient norm: 18.6116\nEpoch 1, Batch 50/55, Loss: 27.5534\nAvg Blank Probability: 0.0155\nSample predictions: ['aHaj', 'akaAa', 'jaFjasaj']\nGround Truth (first 3): ['HLY-', 'IQcC', 'DsQpq']\nRaw outputs (first 3): [[ 1  1 10  4  1 10 11 11  4  1 42  1  1 11 34  1  1  1  1  1  1 11  1  1\n   1  1  1 34  1 11 10  1]\n [ 1 11  1  1 12  1 47  1  1 34 10  1  1 22 34 22 32 10  1  1  1  1  1  1\n  32 34  1  1  1 56  1  1]\n [ 1  1 32  1  1  1  1  1  1  1 32  1  1 10 56  1 10 22 34  1 10  1  1 10\n  47  1 47  4  1  1 10 11]]\nInput length: 32, Label lengths: [4, 4, 5]\nEpoch 1/20, Loss: 27.9357\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nToken distribution (Batch 0): {1: 10}, Pred length: 10\nToken distribution (Batch 1): {1: 8}, Pred length: 8\nToken distribution (Batch 2): {1: 10}, Pred length: 10\nToken distribution (Batch 3): {1: 10}, Pred length: 10\nToken distribution (Batch 4): {1: 10}, Pred length: 10\nToken distribution (Batch 5): {1: 10}, Pred length: 10\nToken distribution (Batch 6): {1: 10}, Pred length: 10\nToken distribution (Batch 7): {1: 10}, Pred length: 10\nToken distribution (Batch 8): {1: 10}, Pred length: 10\nToken distribution (Batch 9): {1: 10}, Pred length: 10\nToken distribution (Batch 10): {1: 10}, Pred length: 10\nToken distribution (Batch 11): {1: 8}, Pred length: 8\nToken distribution (Batch 12): {1: 10}, Pred length: 10\nToken distribution (Batch 13): {1: 8}, Pred length: 8\nToken distribution (Batch 14): {1: 10}, Pred length: 10\nToken distribution (Batch 15): {1: 10}, Pred length: 10\nToken distribution (Batch 16): {1: 8}, Pred length: 8\nToken distribution (Batch 17): {1: 10}, Pred length: 10\nToken distribution (Batch 18): {1: 8}, Pred length: 8\nToken distribution (Batch 19): {1: 8}, Pred length: 8\nValidation Loss: 27.9811\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['Zff2n', 'ehql', '8xNCa', 'xYM55', '2f6Np']\nCurrent Learning Rate: 1.1e-06\nEpoch 2, Filtered data size: 2176, Sample labels: ['MdI8', '0tcL2', 'eyOxJ']\nTrain size: 1740, Val size: 436\nToken distribution (Batch 0): {11: 1, 47: 1, 1: 3, 22: 1, 56: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 1): {1: 8, 22: 2}, Pred length: 10\nToken distribution (Batch 2): {1: 4, 10: 2, 50: 1, 6: 2, 34: 1}, Pred length: 10\nToken distribution (Batch 3): {32: 1, 1: 7, 34: 1, 56: 1}, Pred length: 10\nToken distribution (Batch 4): {1: 4, 11: 1, 25: 1, 10: 2}, Pred length: 8\nToken distribution (Batch 5): {1: 5, 10: 1, 47: 2}, Pred length: 8\nToken distribution (Batch 6): {1: 10}, Pred length: 10\nToken distribution (Batch 7): {1: 4, 10: 2, 47: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 8): {1: 6, 47: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 9): {1: 8}, Pred length: 8\nToken distribution (Batch 10): {1: 6, 10: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 11): {10: 2, 43: 1, 34: 1, 1: 4}, Pred length: 8\nToken distribution (Batch 12): {1: 7, 6: 1}, Pred length: 8\nToken distribution (Batch 13): {1: 6, 10: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 14): {6: 1, 1: 5, 10: 2, 34: 1, 5: 1}, Pred length: 10\nToken distribution (Batch 15): {32: 1, 1: 3, 10: 4}, Pred length: 8\nToken distribution (Batch 16): {1: 7, 10: 1, 34: 1, 42: 1}, Pred length: 10\nToken distribution (Batch 17): {1: 4, 10: 1, 47: 1, 11: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 18): {32: 1, 1: 6, 11: 1, 10: 2}, Pred length: 10\nToken distribution (Batch 19): {1: 3, 10: 2, 47: 1, 43: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 20): {1: 7, 10: 1}, Pred length: 8\nToken distribution (Batch 21): {10: 2, 1: 7, 32: 1}, Pred length: 10\nToken distribution (Batch 22): {1: 6, 32: 2}, Pred length: 8\nToken distribution (Batch 23): {1: 8, 32: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 24): {1: 8}, Pred length: 8\nToken distribution (Batch 25): {34: 1, 1: 7, 10: 2}, Pred length: 10\nToken distribution (Batch 26): {1: 5, 10: 2, 34: 1, 11: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 27): {1: 10}, Pred length: 10\nToken distribution (Batch 28): {1: 6, 10: 3, 32: 1}, Pred length: 10\nToken distribution (Batch 29): {10: 2, 11: 1, 1: 5, 32: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 30): {1: 3, 56: 1, 10: 3, 34: 1}, Pred length: 8\nToken distribution (Batch 31): {1: 5, 32: 1, 27: 1, 4: 1}, Pred length: 8\nBatch 0, Gradient norm: 18.8188\nEpoch 2, Batch 0/55, Loss: 28.2882\nAvg Blank Probability: 0.0156\nSample predictions: ['kUav3aj', 'ava', 'ajXfjaHa']\nGround Truth (first 3): ['fg1P', '3Gv0i', '93VjQ']\nRaw outputs (first 3): [[11  1  1 32  1  1  1  1  1  1  1 10  1  1  6 32  1  1 32  1  1 10  1  1\n   1 34  1  1  1 10  1  1]\n [47  1 10  1 11  1  1  1  1  1 10 43  6  1  1  1 10 10  1 10  1  1  1  1\n   1  1 10  1 10 11 56 32]\n [ 1  1 50  1 25 10  1  1  1  1  1 34  1  1  1 10  1 47  1 10  1  1  1  1\n   1 10  1  1  1 10  1  1]]\nInput length: 32, Label lengths: [4, 5, 5]\nToken distribution (Batch 0): {1: 4, 10: 3, 34: 2, 47: 1}, Pred length: 10\nToken distribution (Batch 1): {11: 2, 1: 3, 10: 1, 4: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 2): {1: 5, 10: 2, 11: 1}, Pred length: 8\nToken distribution (Batch 3): {1: 4, 47: 2, 6: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 4): {1: 8, 56: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 5): {1: 7, 34: 1}, Pred length: 8\nToken distribution (Batch 6): {1: 7, 6: 1, 32: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 7): {1: 7, 34: 1}, Pred length: 8\nToken distribution (Batch 8): {1: 8, 10: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 9): {1: 5, 10: 2, 32: 1, 27: 1, 12: 1}, Pred length: 10\nToken distribution (Batch 10): {1: 6, 10: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 11): {1: 7, 19: 1, 10: 1, 43: 1}, Pred length: 10\nToken distribution (Batch 12): {47: 2, 1: 2, 10: 3, 43: 1}, Pred length: 8\nToken distribution (Batch 13): {1: 7, 10: 1}, Pred length: 8\nToken distribution (Batch 14): {1: 6, 6: 1, 24: 1, 32: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 15): {1: 7, 12: 1}, Pred length: 8\nToken distribution (Batch 16): {1: 8, 19: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 17): {1: 6, 10: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 18): {11: 1, 1: 6, 32: 2, 5: 1}, Pred length: 10\nToken distribution (Batch 19): {1: 7, 34: 1}, Pred length: 8\nToken distribution (Batch 20): {12: 1, 1: 4, 10: 2, 22: 1}, Pred length: 8\nToken distribution (Batch 21): {1: 7, 10: 1, 32: 1, 7: 1}, Pred length: 10\nToken distribution (Batch 22): {1: 7, 56: 1}, Pred length: 8\nToken distribution (Batch 23): {10: 2, 1: 4, 34: 1, 32: 2, 6: 1}, Pred length: 10\nToken distribution (Batch 24): {10: 1, 1: 6, 32: 1}, Pred length: 8\nToken distribution (Batch 25): {1: 9, 6: 1}, Pred length: 10\nToken distribution (Batch 26): {11: 2, 1: 4, 10: 1, 42: 1}, Pred length: 8\nToken distribution (Batch 27): {47: 1, 1: 6, 10: 1}, Pred length: 8\nToken distribution (Batch 28): {1: 6, 32: 1, 22: 1}, Pred length: 8\nToken distribution (Batch 29): {1: 10}, Pred length: 10\nToken distribution (Batch 30): {6: 1, 1: 6, 56: 1}, Pred length: 8\nToken distribution (Batch 31): {1: 4, 11: 1, 27: 1, 32: 2}, Pred length: 8\nBatch 10, Gradient norm: 21.4322\nEpoch 2, Batch 10/55, Loss: 28.4861\nAvg Blank Probability: 0.0160\nSample predictions: ['ajaHaUHja', 'kajdFa', 'ajakaja']\nGround Truth (first 3): ['eC9N3', 'YOHQ', '4tbY']\nRaw outputs (first 3): [[ 1 11  1  1  1  1  1  1  1  1  1  1 47  1  1  1  1  1 11  1 12  1  1 10\n  10  1 11 47  1  1  6  1]\n [10 11  1  1 56  1  6  1 10  1  1 19  1  1  1  1  1  1  1  1  1  1 56  1\n   1  1 11  1  1  1  1 11]\n [10  1 10  1  1  1  1  1  1 10 10  1 10  1  1  1 19 10  1  1 10  1  1  1\n  32  1  1  1  1  1 56  1]]\nInput length: 32, Label lengths: [5, 4, 4]\nToken distribution (Batch 0): {1: 7, 6: 1}, Pred length: 8\nToken distribution (Batch 1): {1: 7, 10: 1}, Pred length: 8\nToken distribution (Batch 2): {1: 5, 10: 1, 12: 1, 27: 1, 5: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 3): {1: 7, 32: 2, 11: 1}, Pred length: 10\nToken distribution (Batch 4): {6: 1, 11: 1, 34: 1, 22: 1, 1: 4, 32: 2}, Pred length: 10\nToken distribution (Batch 5): {1: 10}, Pred length: 10\nToken distribution (Batch 6): {34: 1, 1: 5, 10: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 7): {1: 8, 19: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 8): {1: 7, 10: 1}, Pred length: 8\nToken distribution (Batch 9): {1: 7, 34: 1, 11: 1, 6: 1}, Pred length: 10\nToken distribution (Batch 10): {10: 1, 4: 1, 1: 8}, Pred length: 10\nToken distribution (Batch 11): {1: 9, 4: 1}, Pred length: 10\nToken distribution (Batch 12): {1: 7, 48: 1, 47: 1, 25: 1}, Pred length: 10\nToken distribution (Batch 13): {10: 1, 1: 8, 47: 1}, Pred length: 10\nToken distribution (Batch 14): {1: 6, 10: 1, 48: 1}, Pred length: 8\nToken distribution (Batch 15): {1: 5, 32: 1, 35: 1, 4: 1}, Pred length: 8\nToken distribution (Batch 16): {1: 8, 19: 2}, Pred length: 10\nToken distribution (Batch 17): {1: 6, 10: 2, 56: 1, 6: 1}, Pred length: 10\nToken distribution (Batch 18): {1: 5, 10: 1, 32: 2}, Pred length: 8\nToken distribution (Batch 19): {47: 1, 1: 6, 34: 1}, Pred length: 8\nToken distribution (Batch 20): {34: 1, 10: 2, 1: 4, 27: 1}, Pred length: 8\nToken distribution (Batch 21): {34: 1, 1: 6, 47: 1, 19: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 22): {1: 5, 4: 1, 11: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 23): {56: 1, 1: 6, 47: 1, 10: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 24): {10: 1, 1: 8, 42: 1}, Pred length: 10\nToken distribution (Batch 25): {1: 5, 10: 2, 11: 1}, Pred length: 8\nToken distribution (Batch 26): {1: 7, 10: 2, 4: 1}, Pred length: 10\nToken distribution (Batch 27): {10: 1, 1: 7}, Pred length: 8\nToken distribution (Batch 28): {4: 1, 1: 3, 10: 2, 34: 1, 22: 1}, Pred length: 8\nToken distribution (Batch 29): {1: 8, 34: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 30): {43: 1, 1: 5, 11: 1, 47: 1, 34: 1, 4: 1}, Pred length: 10\nToken distribution (Batch 31): {12: 1, 1: 6, 47: 1, 10: 2}, Pred length: 10\nBatch 20, Gradient norm: 21.0718\nEpoch 2, Batch 20/55, Loss: 27.1594\nAvg Blank Probability: 0.0164\nSample predictions: ['afa', 'aja', 'ajalaAeF']\nGround Truth (first 3): ['e9L8', 'eQm1', 'p6g9W']\nRaw outputs (first 3): [[ 1  1  1  1  6  1 34  1  1  1 10  1  1 10  1  1  1  1  1 47 34 34  1 56\n  10  1  1 10  4  1 43 12]\n [ 1  1 10  1 11  1  1  1  1  1  4  1  1  1  1 32 19 10  1  1 10  1  4  1\n   1 10  1  1  1  1  1  1]\n [ 1  1  1  1 34  1 10  1 10  1  1  4 48  1  1 35 19 56 10 34  1  1  1  1\n   1  1  1  1  1  1 11  1]]\nInput length: 32, Label lengths: [4, 4, 5]\nToken distribution (Batch 0): {1: 4, 4: 1, 32: 2, 19: 1, 10: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 1): {1: 5, 32: 1, 34: 1, 22: 1}, Pred length: 8\nToken distribution (Batch 2): {10: 2, 1: 8}, Pred length: 10\nToken distribution (Batch 3): {10: 2, 32: 3, 1: 3}, Pred length: 8\nToken distribution (Batch 4): {1: 9, 10: 1}, Pred length: 10\nToken distribution (Batch 5): {1: 6, 10: 3, 34: 1}, Pred length: 10\nToken distribution (Batch 6): {10: 3, 1: 4, 35: 1}, Pred length: 8\nToken distribution (Batch 7): {1: 6, 63: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 8): {1: 6, 32: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 9): {1: 7, 4: 1, 11: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 10): {1: 8, 5: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 11): {1: 5, 10: 2, 11: 1}, Pred length: 8\nToken distribution (Batch 12): {1: 8, 34: 2}, Pred length: 10\nToken distribution (Batch 13): {34: 2, 6: 1, 1: 7}, Pred length: 10\nToken distribution (Batch 14): {34: 1, 1: 6, 10: 1}, Pred length: 8\nToken distribution (Batch 15): {12: 1, 10: 2, 34: 1, 1: 6}, Pred length: 10\nToken distribution (Batch 16): {10: 4, 47: 1, 32: 1, 1: 2, 50: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 17): {1: 5, 32: 2, 10: 1}, Pred length: 8\nToken distribution (Batch 18): {34: 1, 1: 8, 22: 1}, Pred length: 10\nToken distribution (Batch 19): {4: 1, 1: 4, 32: 1, 19: 1, 34: 1, 10: 2}, Pred length: 10\nToken distribution (Batch 20): {1: 7, 32: 1}, Pred length: 8\nToken distribution (Batch 21): {1: 6, 32: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 22): {11: 2, 1: 3, 6: 1, 47: 3, 32: 1}, Pred length: 10\nToken distribution (Batch 23): {1: 7, 6: 1}, Pred length: 8\nToken distribution (Batch 24): {1: 6, 32: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 25): {1: 6, 10: 2, 19: 1, 22: 1}, Pred length: 10\nToken distribution (Batch 26): {1: 5, 32: 1, 34: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 27): {34: 1, 11: 1, 1: 7, 10: 1}, Pred length: 10\nToken distribution (Batch 28): {1: 5, 47: 1, 32: 2}, Pred length: 8\nToken distribution (Batch 29): {1: 7, 32: 1}, Pred length: 8\nToken distribution (Batch 30): {47: 2, 34: 1, 1: 4, 12: 1}, Pred length: 8\nToken distribution (Batch 31): {11: 1, 35: 1, 32: 1, 1: 4, 27: 1}, Pred length: 8\nBatch 30, Gradient norm: 24.4873\nEpoch 2, Batch 30/55, Loss: 27.8557\nAvg Blank Probability: 0.0169\nSample predictions: ['adaFasFjH', 'aFHva', 'jaja']\nGround Truth (first 3): ['fXiMU', 'z*Jq', 'lEIfr']\nRaw outputs (first 3): [[ 1  1 10 10  1  1 10  1  1  1  1  1 32 34 34 12 10  1 34  4  1  1 11  1\n   1  1  1 34  1  1 47 11]\n [ 4  1  1 32  1  1  1  1  1  1  5 10  1  6  1 10 47  1  1  1  1  1  1  1\n   1  1 32 11 47  1 34 35]\n [ 1  1  1 32  1 10 10 63  1  4 27  1 34  1  1 34 32 32  1  1  1  1  1  1\n   1  1 34  1 32  1  1 32]]\nInput length: 32, Label lengths: [5, 4, 5]\nToken distribution (Batch 0): {10: 1, 34: 1, 1: 6}, Pred length: 8\nToken distribution (Batch 1): {34: 1, 12: 1, 10: 1, 1: 4, 56: 1}, Pred length: 8\nToken distribution (Batch 2): {1: 3, 19: 2, 32: 3, 47: 1, 6: 1}, Pred length: 10\nToken distribution (Batch 3): {1: 7, 19: 1, 32: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 4): {56: 1, 1: 7, 11: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 5): {1: 1, 47: 2, 4: 2, 10: 2, 25: 1, 32: 1, 42: 1}, Pred length: 10\nToken distribution (Batch 6): {1: 8, 6: 1, 22: 1}, Pred length: 10\nToken distribution (Batch 7): {1: 7, 10: 1, 35: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 8): {1: 8, 43: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 9): {1: 6, 43: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 10): {1: 5, 12: 1, 4: 1, 22: 1, 34: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 11): {4: 4, 1: 1, 47: 1, 34: 1, 11: 1}, Pred length: 8\nToken distribution (Batch 12): {1: 5, 10: 1, 11: 1, 32: 2, 34: 1}, Pred length: 10\nToken distribution (Batch 13): {1: 4, 10: 2, 19: 1, 32: 1, 22: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 14): {1: 4, 32: 1, 4: 1, 5: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 15): {1: 4, 6: 2, 34: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 16): {10: 2, 1: 6, 34: 2}, Pred length: 10\nToken distribution (Batch 17): {1: 7, 10: 2, 34: 1}, Pred length: 10\nToken distribution (Batch 18): {1: 9, 34: 1}, Pred length: 10\nToken distribution (Batch 19): {1: 5, 10: 1, 19: 2}, Pred length: 8\nToken distribution (Batch 20): {10: 2, 11: 1, 1: 3, 56: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 21): {10: 1, 34: 1, 1: 5, 32: 3}, Pred length: 10\nToken distribution (Batch 22): {1: 10}, Pred length: 10\nToken distribution (Batch 23): {1: 5, 47: 1, 10: 1, 34: 1, 27: 1, 4: 1}, Pred length: 10\nToken distribution (Batch 24): {1: 5, 56: 1, 19: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 25): {34: 1, 1: 5, 35: 1, 42: 1, 50: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 26): {34: 2, 12: 1, 10: 1, 19: 1, 1: 5}, Pred length: 10\nToken distribution (Batch 27): {10: 3, 1: 3, 34: 2}, Pred length: 8\nToken distribution (Batch 28): {1: 9, 34: 1}, Pred length: 10\nToken distribution (Batch 29): {1: 5, 10: 1, 32: 1, 6: 1}, Pred length: 8\nToken distribution (Batch 30): {11: 1, 1: 6, 12: 1}, Pred length: 8\nToken distribution (Batch 31): {1: 6, 11: 1, 6: 1, 34: 1, 10: 1}, Pred length: 10\nBatch 40, Gradient norm: 22.1567\nEpoch 2, Batch 40/55, Loss: 26.7066\nAvg Blank Probability: 0.0176\nSample predictions: ['jHa', 'Hlja3a', 'asFaUafFs']\nGround Truth (first 3): ['xG7S', '7TWf', 'UTOoC']\nRaw outputs (first 3): [[10 34  1  1 56  1  1  1  1  1 11  4 10  1  1  1 10  1  1  1 10 10  1  1\n   1 34 34 10  1  1 11  1]\n [34 12  1  1  1 47  1  1  1 43  1  4  1 10 32  6  1  1  1  1 11 34  1  1\n   1  1 12  1  1 10  1  1]\n [ 1 10 19 19  1 47  6 10 43  1 12  1 10  1  4 34  1  1  1 10  1  1  1  1\n  56  1 10 10  1  1 12  1]]\nInput length: 32, Label lengths: [4, 4, 5]\nToken distribution (Batch 0): {1: 6, 32: 3, 10: 1}, Pred length: 10\nToken distribution (Batch 1): {10: 3, 1: 7}, Pred length: 10\nToken distribution (Batch 2): {1: 7, 34: 1}, Pred length: 8\nToken distribution (Batch 3): {1: 4, 10: 2, 32: 1, 47: 2, 6: 1}, Pred length: 10\nToken distribution (Batch 4): {1: 8, 11: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 5): {1: 7, 10: 1}, Pred length: 8\nToken distribution (Batch 6): {1: 7, 10: 1, 47: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 7): {1: 5, 10: 2, 47: 1}, Pred length: 8\nToken distribution (Batch 8): {1: 7, 34: 2, 4: 1}, Pred length: 10\nToken distribution (Batch 9): {32: 2, 1: 4, 34: 2}, Pred length: 8\nToken distribution (Batch 10): {47: 1, 4: 1, 1: 6, 34: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 11): {11: 1, 32: 1, 1: 4, 22: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 12): {10: 3, 1: 3, 32: 1, 4: 1}, Pred length: 8\nToken distribution (Batch 13): {1: 6, 10: 2}, Pred length: 8\nToken distribution (Batch 14): {6: 1, 10: 1, 32: 2, 47: 2, 1: 4}, Pred length: 10\nToken distribution (Batch 15): {34: 1, 22: 1, 32: 3, 56: 1, 1: 4}, Pred length: 10\nToken distribution (Batch 16): {1: 4, 32: 1, 19: 1, 47: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 17): {11: 2, 1: 4, 47: 1, 32: 2, 27: 1}, Pred length: 10\nToken distribution (Batch 18): {1: 6, 12: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 19): {1: 9, 47: 1}, Pred length: 10\nToken distribution (Batch 20): {10: 2, 1: 6, 5: 1, 12: 1}, Pred length: 10\nToken distribution (Batch 21): {6: 1, 10: 3, 1: 4}, Pred length: 8\nToken distribution (Batch 22): {1: 6, 34: 1, 11: 1, 4: 1, 56: 1}, Pred length: 10\nToken distribution (Batch 23): {10: 1, 1: 5, 32: 1, 47: 1, 4: 1, 22: 1}, Pred length: 10\nToken distribution (Batch 24): {4: 1, 10: 1, 1: 7, 47: 1}, Pred length: 10\nToken distribution (Batch 25): {1: 7, 34: 1}, Pred length: 8\nToken distribution (Batch 26): {1: 4, 4: 2, 27: 1, 11: 1}, Pred length: 8\nToken distribution (Batch 27): {34: 3, 1: 5, 10: 2}, Pred length: 10\nToken distribution (Batch 28): {1: 10}, Pred length: 10\nToken distribution (Batch 29): {1: 8, 10: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 30): {1: 9, 34: 1}, Pred length: 10\nToken distribution (Batch 31): {1: 5, 34: 1, 27: 1, 32: 1, 47: 1, 10: 1}, Pred length: 10\nBatch 50, Gradient norm: 23.4904\nEpoch 2, Batch 50/55, Loss: 26.5843\nAvg Blank Probability: 0.0183\nSample predictions: ['aFaFj', 'jajaja', 'aHa']\nGround Truth (first 3): ['vpcr*', 'dcoTQ', 'SF2E']\nRaw outputs (first 3): [[ 1 10  1  1  1 10  1  1  1 32  1  4 10  1  6 34  1 11  1  1 10  6  1 10\n   4  1  1 34  1  1  1  1]\n [ 1  1  1 10  1  1  1  1  1  1 47 11 10  1 10 22 32  1  1  1 10 10 34  1\n  10  1  4  1  1  1  1  1]\n [ 1  1  1  1  1  0 10 10  1 34  4 32  1  1 32 32  1 47  1  1  1  1  1 32\n   1 34  1  1  1  1  1 34]]\nInput length: 32, Label lengths: [5, 5, 4]\nEpoch 2/20, Loss: 27.5738\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nToken distribution (Batch 0): {1: 8}, Pred length: 8\nToken distribution (Batch 1): {1: 8}, Pred length: 8\nToken distribution (Batch 2): {1: 10}, Pred length: 10\nToken distribution (Batch 3): {1: 8}, Pred length: 8\nToken distribution (Batch 4): {1: 10}, Pred length: 10\nToken distribution (Batch 5): {1: 10}, Pred length: 10\nToken distribution (Batch 6): {1: 8}, Pred length: 8\nToken distribution (Batch 7): {1: 10}, Pred length: 10\nToken distribution (Batch 8): {1: 8}, Pred length: 8\nToken distribution (Batch 9): {1: 10}, Pred length: 10\nToken distribution (Batch 10): {1: 8}, Pred length: 8\nToken distribution (Batch 11): {1: 10}, Pred length: 10\nToken distribution (Batch 12): {1: 10}, Pred length: 10\nToken distribution (Batch 13): {1: 8}, Pred length: 8\nToken distribution (Batch 14): {1: 10}, Pred length: 10\nToken distribution (Batch 15): {1: 10}, Pred length: 10\nToken distribution (Batch 16): {1: 10}, Pred length: 10\nToken distribution (Batch 17): {1: 8}, Pred length: 8\nToken distribution (Batch 18): {1: 10}, Pred length: 10\nToken distribution (Batch 19): {1: 10}, Pred length: 10\nValidation Loss: 27.7920\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['HvRE', 'fpl-', 'n-e5Z', 'bstJ', 'o3kWf']\nCurrent Learning Rate: 2.2e-06\nEpoch 3, Filtered data size: 2176, Sample labels: ['MdI8', '0tcL2', 'eyOxJ']\nTrain size: 1740, Val size: 436\nToken distribution (Batch 0): {1: 7, 5: 1}, Pred length: 8\nToken distribution (Batch 1): {1: 6, 47: 2, 27: 1, 12: 1}, Pred length: 10\nToken distribution (Batch 2): {11: 1, 4: 1, 1: 7, 19: 1}, Pred length: 10\nToken distribution (Batch 3): {1: 5, 10: 2, 32: 2, 19: 1}, Pred length: 10\nToken distribution (Batch 4): {10: 3, 1: 5, 27: 2}, Pred length: 10\nToken distribution (Batch 5): {1: 8, 10: 1, 4: 1}, Pred length: 10\nToken distribution (Batch 6): {34: 1, 19: 1, 1: 6, 10: 1, 56: 1}, Pred length: 10\nToken distribution (Batch 7): {10: 2, 32: 1, 27: 2, 1: 2, 34: 1}, Pred length: 8\nToken distribution (Batch 8): {11: 3, 1: 5}, Pred length: 8\nToken distribution (Batch 9): {34: 2, 12: 1, 47: 1, 32: 1, 1: 4, 42: 1}, Pred length: 10\nToken distribution (Batch 10): {11: 1, 1: 5, 10: 2, 4: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 11): {1: 6, 10: 2, 4: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 12): {1: 6, 34: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 13): {10: 1, 1: 6, 34: 1}, Pred length: 8\nToken distribution (Batch 14): {34: 2, 1: 7, 6: 1}, Pred length: 10\nToken distribution (Batch 15): {1: 6, 32: 1, 6: 1, 11: 2}, Pred length: 10\nToken distribution (Batch 16): {34: 1, 63: 1, 1: 5, 10: 1}, Pred length: 8\nToken distribution (Batch 17): {1: 5, 10: 3, 47: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 18): {1: 10}, Pred length: 10\nToken distribution (Batch 19): {11: 1, 19: 1, 1: 7, 12: 1}, Pred length: 10\nToken distribution (Batch 20): {4: 1, 1: 6, 11: 1, 32: 2}, Pred length: 10\nToken distribution (Batch 21): {34: 1, 47: 1, 1: 7, 10: 1}, Pred length: 10\nToken distribution (Batch 22): {1: 6, 32: 2, 25: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 23): {1: 6, 34: 1, 6: 1}, Pred length: 8\nToken distribution (Batch 24): {1: 6, 32: 3, 56: 1}, Pred length: 10\nToken distribution (Batch 25): {34: 1, 1: 8, 27: 1}, Pred length: 10\nToken distribution (Batch 26): {1: 8, 32: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 27): {34: 1, 1: 5, 10: 1, 11: 1}, Pred length: 8\nToken distribution (Batch 28): {1: 6, 6: 1, 12: 1}, Pred length: 8\nToken distribution (Batch 29): {1: 4, 12: 2, 19: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 30): {12: 1, 1: 6, 47: 1}, Pred length: 8\nToken distribution (Batch 31): {10: 1, 42: 1, 1: 6}, Pred length: 8\nBatch 0, Gradient norm: 23.9464\nEpoch 3, Batch 0/55, Loss: 26.4268\nAvg Blank Probability: 0.0187\nSample predictions: ['aea', 'aUAal', 'kdas']\nGround Truth (first 3): ['eJgm', 'w5JxV', 'aVQSi']\nRaw outputs (first 3): [[ 1  1 11  1 10  1 34  1 11 34 11  1  1 10 34  1 34  1  1 11  4 34  1  1\n   1 34  1 34  1 47 12 10]\n [ 1  1  4  1 10  1 19 10  1 12  1  1  1  1  1  1 63  1  1 19  1 47  1  1\n  32  1  1  1  1  1  0 42]\n [ 1  1  1  1  1 10  1 32 11 47  1  1  1  1  1  1  1  0  1  1 11  1 32  1\n   1  1  1  1  1  1  1  1]]\nInput length: 32, Label lengths: [4, 5, 5]\nToken distribution (Batch 0): {1: 6, 10: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 1): {1: 8, 34: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 2): {1: 7, 19: 1}, Pred length: 8\nToken distribution (Batch 3): {11: 1, 1: 5, 5: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 4): {1: 5, 32: 1, 27: 1, 4: 1, 12: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 5): {1: 7, 32: 1}, Pred length: 8\nToken distribution (Batch 6): {6: 1, 1: 5, 19: 1, 32: 1, 34: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 7): {24: 1, 10: 1, 1: 7, 19: 1}, Pred length: 10\nToken distribution (Batch 8): {10: 2, 1: 2, 12: 1, 34: 1, 47: 1, 32: 1, 22: 1, 5: 1}, Pred length: 10\nToken distribution (Batch 9): {1: 6, 12: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 10): {34: 3, 1: 5}, Pred length: 8\nToken distribution (Batch 11): {1: 8}, Pred length: 8\nToken distribution (Batch 12): {34: 1, 10: 2, 11: 3, 1: 4}, Pred length: 10\nToken distribution (Batch 13): {1: 4, 47: 2, 6: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 14): {1: 9, 32: 1}, Pred length: 10\nToken distribution (Batch 15): {1: 5, 34: 1, 11: 2, 10: 1, 5: 1}, Pred length: 10\nToken distribution (Batch 16): {1: 6, 10: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 17): {1: 6, 32: 1, 11: 1, 34: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 18): {1: 6, 4: 1, 12: 1}, Pred length: 8\nToken distribution (Batch 19): {1: 5, 19: 1, 42: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 20): {11: 1, 12: 1, 1: 8}, Pred length: 10\nToken distribution (Batch 21): {10: 2, 32: 1, 34: 3, 27: 2}, Pred length: 8\nToken distribution (Batch 22): {1: 5, 6: 1, 10: 1, 27: 1, 56: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 23): {1: 8}, Pred length: 8\nToken distribution (Batch 24): {10: 2, 1: 5, 32: 2, 34: 1}, Pred length: 10\nToken distribution (Batch 25): {1: 5, 50: 1, 32: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 26): {1: 6, 32: 1, 34: 1, 27: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 27): {1: 4, 32: 2, 4: 2}, Pred length: 8\nToken distribution (Batch 28): {10: 1, 1: 5, 19: 1, 34: 2, 11: 1}, Pred length: 10\nToken distribution (Batch 29): {1: 6, 11: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 30): {1: 8}, Pred length: 8\nToken distribution (Batch 31): {1: 4, 47: 1, 32: 1, 5: 1, 27: 1}, Pred length: 8\nBatch 10, Gradient norm: 27.0540\nEpoch 3, Batch 10/55, Loss: 27.3760\nAvg Blank Probability: 0.0197\nSample predictions: ['ajas', 'aHaF', 'asa']\nGround Truth (first 3): ['pEl6', 'SYePT', 'RQhe']\nRaw outputs (first 3): [[10 42 12  1  1  1  6 24 10  1 63  0 34  1  1  1  1  1  1  1 11 10  1  1\n  10  1  1 10 10  1  1  1]\n [ 1  1  1 11  1  1  1 10  1  1 34  1 10  1 32  1  1  1  4  1 12 32  6  1\n   1  1  1  1  1  1  1  1]\n [ 1  1  1  1  0  1  1  1 12  1  1  1 11  1  1  1 10  1  1  1  1 34  1  1\n   1 50  1  0 19  1  1 47]]\nInput length: 32, Label lengths: [4, 5, 4]\nToken distribution (Batch 0): {1: 9, 11: 1}, Pred length: 10\nToken distribution (Batch 1): {10: 1, 1: 5, 34: 1, 11: 1}, Pred length: 8\nToken distribution (Batch 2): {10: 1, 11: 1, 1: 4, 34: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 3): {47: 1, 1: 6, 10: 1}, Pred length: 8\nToken distribution (Batch 4): {1: 7, 47: 1}, Pred length: 8\nToken distribution (Batch 5): {1: 8, 6: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 6): {1: 4, 12: 1, 34: 3, 10: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 7): {47: 2, 1: 5, 34: 1}, Pred length: 8\nToken distribution (Batch 8): {1: 5, 4: 1, 10: 2, 56: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 9): {1: 10}, Pred length: 10\nToken distribution (Batch 10): {1: 8, 4: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 11): {1: 3, 32: 2, 11: 1, 27: 1, 5: 1}, Pred length: 8\nToken distribution (Batch 12): {10: 1, 47: 1, 11: 1, 1: 4, 5: 1}, Pred length: 8\nToken distribution (Batch 13): {1: 7, 19: 1}, Pred length: 8\nToken distribution (Batch 14): {1: 7, 47: 2, 12: 1}, Pred length: 10\nToken distribution (Batch 15): {1: 9, 47: 1}, Pred length: 10\nToken distribution (Batch 16): {1: 6, 35: 1, 11: 1}, Pred length: 8\nToken distribution (Batch 17): {1: 7, 32: 2, 27: 1}, Pred length: 10\nToken distribution (Batch 18): {10: 2, 47: 2, 1: 5, 32: 1}, Pred length: 10\nToken distribution (Batch 19): {10: 1, 4: 1, 1: 4, 32: 3, 56: 1}, Pred length: 10\nToken distribution (Batch 20): {10: 2, 1: 4, 47: 1, 4: 1}, Pred length: 8\nToken distribution (Batch 21): {1: 7, 10: 1, 47: 1, 4: 1}, Pred length: 10\nToken distribution (Batch 22): {1: 4, 47: 3, 32: 1}, Pred length: 8\nToken distribution (Batch 23): {1: 7, 32: 1, 48: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 24): {10: 1, 1: 5, 12: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 25): {10: 1, 11: 1, 1: 6, 12: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 26): {1: 5, 10: 1, 22: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 27): {1: 7, 47: 1, 32: 1, 55: 1}, Pred length: 10\nToken distribution (Batch 28): {4: 2, 1: 6}, Pred length: 8\nToken distribution (Batch 29): {1: 5, 35: 1, 11: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 30): {1: 9, 32: 1}, Pred length: 10\nToken distribution (Batch 31): {10: 2, 34: 1, 1: 3, 11: 1, 12: 1}, Pred length: 8\nBatch 20, Gradient norm: 29.0249\nEpoch 3, Batch 20/55, Loss: 26.7743\nAvg Blank Probability: 0.0206\nSample predictions: ['aka', 'jaHka', 'jkaHas']\nGround Truth (first 3): ['Bi72z', 'vxrj', 'yJY7']\nRaw outputs (first 3): [[10 10 10 47  1  1  1  1  1  1  1 11 10  1  1  1  1 47 10  1  1  1  1  1\n  10 10  1  1  1 11  1 10]\n [ 1  1 11  1 47  1 12 47  4  1  1  1 47  1  1  1  1  1 47 10 10  1 47  1\n   1 10  1  1  4  1  1 34]\n [ 0  1  1  1  1  1 34  1 10  0  1  0 11  1 47  1  1  1  1  4  1  0  0  1\n   1 11  0  1  0  1  1  1]]\nInput length: 32, Label lengths: [5, 4, 4]\nToken distribution (Batch 0): {19: 1, 1: 9}, Pred length: 10\nToken distribution (Batch 1): {47: 1, 1: 7, 34: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 2): {1: 6, 35: 2, 47: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 3): {34: 2, 1: 6}, Pred length: 8\nToken distribution (Batch 4): {1: 4, 19: 1, 34: 2, 47: 1}, Pred length: 8\nToken distribution (Batch 5): {1: 7, 12: 1, 34: 2}, Pred length: 10\nToken distribution (Batch 6): {6: 1, 1: 5, 34: 1, 4: 1}, Pred length: 8\nToken distribution (Batch 7): {1: 5, 47: 2, 34: 1}, Pred length: 8\nToken distribution (Batch 8): {34: 1, 47: 1, 1: 6}, Pred length: 8\nToken distribution (Batch 9): {1: 6, 47: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 10): {1: 9, 47: 1}, Pred length: 10\nToken distribution (Batch 11): {1: 6, 34: 1, 47: 3}, Pred length: 10\nToken distribution (Batch 12): {10: 1, 1: 6, 34: 1, 4: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 13): {1: 8, 11: 1, 6: 1}, Pred length: 10\nToken distribution (Batch 14): {47: 3, 1: 4, 34: 2, 32: 1}, Pred length: 10\nToken distribution (Batch 15): {1: 8, 47: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 16): {1: 6, 25: 1, 34: 2, 19: 1}, Pred length: 10\nToken distribution (Batch 17): {1: 7, 34: 2, 19: 1}, Pred length: 10\nToken distribution (Batch 18): {1: 7, 27: 1}, Pred length: 8\nToken distribution (Batch 19): {1: 7, 34: 1}, Pred length: 8\nToken distribution (Batch 20): {1: 7, 47: 1}, Pred length: 8\nToken distribution (Batch 21): {1: 7, 34: 1}, Pred length: 8\nToken distribution (Batch 22): {10: 1, 1: 5, 6: 1, 19: 1, 11: 1, 22: 1}, Pred length: 10\nToken distribution (Batch 23): {1: 7, 32: 1, 10: 1, 5: 1}, Pred length: 10\nToken distribution (Batch 24): {1: 6, 6: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 25): {1: 9, 32: 1}, Pred length: 10\nToken distribution (Batch 26): {1: 6, 22: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 27): {10: 1, 1: 7, 22: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 28): {1: 7, 32: 1, 47: 1, 4: 1}, Pred length: 10\nToken distribution (Batch 29): {1: 8, 32: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 30): {1: 6, 47: 2, 34: 2}, Pred length: 10\nToken distribution (Batch 31): {1: 6, 42: 1, 32: 1, 10: 1, 11: 1}, Pred length: 10\nBatch 30, Gradient norm: 30.2757\nEpoch 3, Batch 30/55, Loss: 25.7889\nAvg Blank Probability: 0.0221\nSample predictions: ['sa', 'UaHja', 'aIaUHIa']\nGround Truth (first 3): ['zqN8J', '4OO0K', 'vezPg']\nRaw outputs (first 3): [[ 0 56  1 12 12 11 34 34 34  1  1  1 10  1  1  1  1 10  1  1 10  0  1 34\n   1  1 12  0  0  1  1 11]\n [ 0  0  0 34  1  1  0  1 34 47  1  0  0  0  0  1  0  1  1  1  0  1 10  1\n   1  0  1 10  1  1 47  1]\n [ 0  1  0  0  0  0  0 47  0 34  1  0  0  1  0  0  1  1  1  1  0  1  1  1\n   0  1  0  1  0  1  1 42]]\nInput length: 32, Label lengths: [5, 5, 5]\nToken distribution (Batch 0): {34: 3, 1: 3, 47: 2}, Pred length: 8\nToken distribution (Batch 1): {1: 6, 4: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 2): {1: 7, 47: 1}, Pred length: 8\nToken distribution (Batch 3): {11: 1, 1: 5, 27: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 4): {1: 7, 32: 1, 11: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 5): {1: 8, 6: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 6): {1: 5, 11: 1, 47: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 7): {10: 3, 1: 3, 19: 1, 32: 2, 6: 1}, Pred length: 10\nToken distribution (Batch 8): {34: 1, 19: 1, 22: 1, 1: 5}, Pred length: 8\nToken distribution (Batch 9): {34: 3, 47: 2, 1: 5}, Pred length: 10\nToken distribution (Batch 10): {1: 2, 34: 4, 47: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 11): {1: 9, 34: 1}, Pred length: 10\nToken distribution (Batch 12): {1: 6, 47: 2, 10: 2}, Pred length: 10\nToken distribution (Batch 13): {34: 2, 11: 1, 1: 4, 19: 1}, Pred length: 8\nToken distribution (Batch 14): {10: 1, 1: 8, 32: 1}, Pred length: 10\nToken distribution (Batch 15): {6: 1, 1: 6, 5: 1}, Pred length: 8\nToken distribution (Batch 16): {34: 2, 1: 7, 32: 1}, Pred length: 10\nToken distribution (Batch 17): {1: 8, 47: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 18): {1: 8, 10: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 19): {1: 4, 10: 1, 5: 1, 34: 2}, Pred length: 8\nToken distribution (Batch 20): {1: 8, 34: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 21): {1: 6, 11: 1, 27: 1, 47: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 22): {1: 5, 34: 2, 47: 1}, Pred length: 8\nToken distribution (Batch 23): {10: 3, 1: 4, 27: 1}, Pred length: 8\nToken distribution (Batch 24): {1: 5, 47: 3, 4: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 25): {1: 7, 10: 1, 34: 1, 4: 1}, Pred length: 10\nToken distribution (Batch 26): {1: 5, 10: 1, 47: 1, 4: 1}, Pred length: 8\nToken distribution (Batch 27): {1: 4, 47: 2, 19: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 28): {1: 5, 47: 1, 19: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 29): {1: 9, 11: 1}, Pred length: 10\nToken distribution (Batch 30): {1: 4, 32: 2, 4: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 31): {1: 6, 47: 1, 32: 2, 5: 1}, Pred length: 10\nBatch 40, Gradient norm: 37.2785\nEpoch 3, Batch 40/55, Loss: 26.1710\nAvg Blank Probability: 0.0234\nSample predictions: ['HaUaH', 'adaH', 'aUa']\nGround Truth (first 3): ['S4pz', 'ahJ0', 'GAx7']\nRaw outputs (first 3): [[11  1  1  1  1  1 11  1  0  1  1 10  1 11  1 11 10  1  0 11  1 11  1 10\n   0 11 34  1  0  1 10 34]\n [34  1  1  0  0  0  1 10  0  0  0  0  1 34  0  6 34  0  1  1  0  0  0  0\n   0  0  0  0  1  0  0  0]\n [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  1  0  0  0  0  0\n  47  0  0  0  1 11  1  0]]\nInput length: 32, Label lengths: [4, 4, 4]\nToken distribution (Batch 0): {1: 8}, Pred length: 8\nToken distribution (Batch 1): {32: 1, 1: 4, 27: 3}, Pred length: 8\nToken distribution (Batch 2): {10: 1, 32: 1, 1: 4, 34: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 3): {1: 3, 34: 4, 4: 1}, Pred length: 8\nToken distribution (Batch 4): {1: 6, 34: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 5): {1: 8, 12: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 6): {12: 1, 32: 2, 1: 3, 4: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 7): {1: 8, 6: 2}, Pred length: 10\nToken distribution (Batch 8): {10: 1, 34: 1, 12: 1, 1: 4, 32: 1}, Pred length: 8\nToken distribution (Batch 9): {1: 9, 10: 1}, Pred length: 10\nToken distribution (Batch 10): {47: 2, 1: 2, 19: 1, 10: 2, 32: 2, 12: 1}, Pred length: 10\nToken distribution (Batch 11): {10: 1, 1: 3, 34: 3, 32: 1, 12: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 12): {1: 7, 42: 1, 10: 2}, Pred length: 10\nToken distribution (Batch 13): {1: 8, 35: 1, 12: 1}, Pred length: 10\nToken distribution (Batch 14): {42: 1, 1: 5, 47: 1, 12: 1}, Pred length: 8\nToken distribution (Batch 15): {1: 8, 10: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 16): {1: 6, 11: 1, 42: 1, 32: 1, 25: 1}, Pred length: 10\nToken distribution (Batch 17): {47: 2, 1: 4, 5: 1, 22: 1}, Pred length: 8\nToken distribution (Batch 18): {1: 8, 32: 2}, Pred length: 10\nToken distribution (Batch 19): {34: 1, 1: 8, 47: 1}, Pred length: 10\nToken distribution (Batch 20): {1: 8}, Pred length: 8\nToken distribution (Batch 21): {1: 6, 32: 1, 34: 1, 11: 1, 6: 1}, Pred length: 10\nToken distribution (Batch 22): {1: 6, 4: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 23): {1: 5, 10: 1, 32: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 24): {1: 9, 34: 1}, Pred length: 10\nToken distribution (Batch 25): {1: 6, 47: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 26): {1: 4, 11: 2, 10: 1, 34: 3}, Pred length: 10\nToken distribution (Batch 27): {10: 1, 47: 1, 1: 5, 34: 2, 11: 1}, Pred length: 10\nToken distribution (Batch 28): {22: 1, 1: 5, 6: 1, 32: 1, 34: 1, 19: 1}, Pred length: 10\nToken distribution (Batch 29): {1: 6, 34: 1, 12: 1}, Pred length: 8\nToken distribution (Batch 30): {34: 2, 1: 7, 47: 1}, Pred length: 10\nToken distribution (Batch 31): {1: 8, 34: 1, 4: 1}, Pred length: 10\nBatch 50, Gradient norm: 32.6011\nEpoch 3, Batch 50/55, Loss: 25.5002\nAvg Blank Probability: 0.0251\nSample predictions: ['a', 'FaAaAaAa', 'jFaHUa']\nGround Truth (first 3): ['iMPp', 'x0Xw', 'RT0a']\nRaw outputs (first 3): [[ 1  0  0  0  0  0  0  0  1  1 47  0  1  0  0  0  0  1  1 34 22  1  0  1\n  10  1 10 10  1 10  0  0]\n [ 1 32 10  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n   0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [4, 4, 4]\nEpoch 3/20, Loss: 26.5450\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nToken distribution (Batch 0): {1: 8}, Pred length: 8\nToken distribution (Batch 1): {1: 8}, Pred length: 8\nToken distribution (Batch 2): {1: 10}, Pred length: 10\nToken distribution (Batch 3): {1: 8}, Pred length: 8\nToken distribution (Batch 4): {1: 10}, Pred length: 10\nToken distribution (Batch 5): {1: 10}, Pred length: 10\nToken distribution (Batch 6): {1: 10}, Pred length: 10\nToken distribution (Batch 7): {1: 10}, Pred length: 10\nToken distribution (Batch 8): {1: 8}, Pred length: 8\nToken distribution (Batch 9): {1: 8}, Pred length: 8\nToken distribution (Batch 10): {1: 8}, Pred length: 8\nToken distribution (Batch 11): {1: 8}, Pred length: 8\nToken distribution (Batch 12): {1: 8}, Pred length: 8\nToken distribution (Batch 13): {1: 10}, Pred length: 10\nToken distribution (Batch 14): {1: 8}, Pred length: 8\nToken distribution (Batch 15): {1: 10}, Pred length: 10\nToken distribution (Batch 16): {1: 8}, Pred length: 8\nToken distribution (Batch 17): {1: 10}, Pred length: 10\nToken distribution (Batch 18): {1: 10}, Pred length: 10\nToken distribution (Batch 19): {1: 8}, Pred length: 8\nValidation Loss: 27.1873\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['qynH', 'oDES', 'p4kXP', 'P5Ma', 'WjcZv']\nCurrent Learning Rate: 3.3000000000000006e-06\nEpoch 4, Filtered data size: 2176, Sample labels: ['MdI8', '0tcL2', 'eyOxJ']\nTrain size: 1740, Val size: 436\nToken distribution (Batch 0): {1: 9, 34: 1}, Pred length: 10\nToken distribution (Batch 1): {11: 1, 1: 4, 34: 1, 12: 1, 47: 1, 32: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 2): {1: 7, 42: 1, 32: 2}, Pred length: 10\nToken distribution (Batch 3): {1: 5, 12: 2, 22: 1, 34: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 4): {1: 5, 50: 1, 34: 1, 47: 1, 22: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 5): {34: 1, 1: 8, 27: 1}, Pred length: 10\nToken distribution (Batch 6): {1: 6, 32: 1, 19: 1}, Pred length: 8\nToken distribution (Batch 7): {1: 7, 5: 1}, Pred length: 8\nToken distribution (Batch 8): {1: 4, 34: 2, 47: 2}, Pred length: 8\nToken distribution (Batch 9): {5: 1, 47: 1, 1: 4, 11: 2, 34: 1, 35: 1}, Pred length: 10\nToken distribution (Batch 10): {1: 6, 32: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 11): {1: 5, 34: 1, 12: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 12): {47: 2, 12: 1, 1: 3, 34: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 13): {1: 4, 34: 2, 19: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 14): {47: 1, 34: 2, 1: 3, 4: 1, 6: 1}, Pred length: 8\nToken distribution (Batch 15): {10: 1, 1: 7, 11: 1, 27: 1}, Pred length: 10\nToken distribution (Batch 16): {1: 5, 34: 3, 4: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 17): {1: 10}, Pred length: 10\nToken distribution (Batch 18): {12: 1, 1: 7, 34: 2}, Pred length: 10\nToken distribution (Batch 19): {1: 5, 47: 1, 34: 2}, Pred length: 8\nToken distribution (Batch 20): {1: 6, 10: 1, 22: 1, 27: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 21): {4: 1, 1: 3, 11: 1, 10: 1, 12: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 22): {10: 1, 1: 5, 34: 1, 43: 1}, Pred length: 8\nToken distribution (Batch 23): {1: 8, 32: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 24): {1: 5, 4: 1, 35: 1, 47: 1, 19: 1, 32: 1}, Pred length: 10\nToken distribution (Batch 25): {1: 7, 47: 2, 11: 1}, Pred length: 10\nToken distribution (Batch 26): {1: 7, 19: 1, 32: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 27): {1: 7, 34: 1, 47: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 28): {11: 2, 1: 5, 34: 2, 22: 1}, Pred length: 10\nToken distribution (Batch 29): {47: 2, 1: 5, 34: 2, 6: 1}, Pred length: 10\nToken distribution (Batch 30): {1: 6, 11: 1, 4: 1}, Pred length: 8\nToken distribution (Batch 31): {12: 1, 10: 1, 1: 5, 34: 1}, Pred length: 8\nBatch 0, Gradient norm: 34.9811\nEpoch 4, Batch 0/55, Loss: 25.1252\nAvg Blank Probability: 0.0260\nSample predictions: ['aHa', 'kaHlaUaFA', 'aPaFaFa']\nGround Truth (first 3): ['7GTbt', 'y3hkh', 'emzwz']\nRaw outputs (first 3): [[ 0  0  1  1  0  4  0  0  1  0  0  0  0  1  1  1  0  0 11  0  1  0  0  0\n   1  0  0  0 10  0  0  0]\n [ 0  0  1  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1\n   0  0  0  0  0  0  0  0]\n [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [5, 5, 5]\nToken distribution (Batch 0): {1: 6, 32: 1, 12: 1, 27: 1, 47: 1}, Pred length: 10\nToken distribution (Batch 1): {1: 6, 47: 1, 12: 1}, Pred length: 8\nToken distribution (Batch 2): {1: 5, 34: 1, 12: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 3): {47: 1, 1: 7, 32: 1, 11: 1}, Pred length: 10\nToken distribution (Batch 4): {1: 10}, Pred length: 10\nToken distribution (Batch 5): {1: 9, 34: 1}, Pred length: 10\nToken distribution (Batch 6): {1: 5, 32: 1, 5: 1, 12: 1}, Pred length: 8\nToken distribution (Batch 7): {1: 7, 12: 1}, Pred length: 8\nToken distribution (Batch 8): {32: 3, 1: 3, 22: 1, 34: 1}, Pred length: 8\nToken distribution (Batch 9): {34: 2, 1: 4, 4: 1, 22: 1}, Pred length: 8\nToken distribution (Batch 10): {12: 2, 1: 7, 19: 1}, Pred length: 10\nToken distribution (Batch 11): {1: 7, 32: 1}, Pred length: 8\nToken distribution (Batch 12): {34: 1, 1: 4, 4: 1, 19: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 13): {1: 3, 47: 3, 34: 3, 5: 1}, Pred length: 10\nToken distribution (Batch 14): {5: 1, 1: 7, 47: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 15): {10: 1, 1: 6, 34: 1, 47: 1, 22: 1}, Pred length: 10\nToken distribution (Batch 16): {1: 8, 47: 2}, Pred length: 10\nToken distribution (Batch 17): {32: 1, 1: 4, 4: 1, 6: 1, 47: 1}, Pred length: 8\nToken distribution (Batch 18): {34: 1, 1: 7}, Pred length: 8\nToken distribution (Batch 19): {12: 1, 34: 2, 1: 4, 47: 1, 32: 1, 35: 1}, Pred length: 10\nToken distribution (Batch 20): {1: 6, 32: 2, 27: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 21): {1: 4, 6: 1, 34: 2, 10: 1}, Pred length: 8\nToken distribution (Batch 22): {11: 1, 1: 7}, Pred length: 8\nToken distribution (Batch 23): {1: 7, 47: 1}, Pred length: 8\nToken distribution (Batch 24): {11: 2, 10: 1, 1: 5, 19: 1, 34: 1}, Pred length: 10\nToken distribution (Batch 25): {34: 1, 10: 3, 12: 1, 47: 1, 1: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 26): {1: 7, 4: 1, 27: 1, 10: 1}, Pred length: 10\nToken distribution (Batch 27): {35: 2, 34: 1, 1: 6, 12: 1}, Pred length: 10\nToken distribution (Batch 28): {10: 1, 1: 4, 32: 2, 11: 1}, Pred length: 8\nToken distribution (Batch 29): {11: 1, 12: 1, 27: 1, 1: 4, 32: 1}, Pred length: 8\nToken distribution (Batch 30): {1: 6, 12: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 31): {11: 1, 1: 6, 34: 1}, Pred length: 8\nBatch 10, Gradient norm: 39.5053\nEpoch 4, Batch 10/55, Loss: 25.5672\nAvg Blank Probability: 0.0286\nSample predictions: ['aFalAUa', 'aUla', 'aHlF']\nGround Truth (first 3): ['KNQo8', 'DYK8', 'CSBo']\nRaw outputs (first 3): [[ 0  0 34  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  1\n   0  0  0  0  0  0  0  1]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0\n   0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [5, 4, 4]\nToken distribution (Batch 0): {1: 1}, Pred length: 1\nToken distribution (Batch 1): {34: 1, 1: 2, 32: 1}, Pred length: 4\nToken distribution (Batch 2): {1: 2}, Pred length: 2\nToken distribution (Batch 3): {1: 3, 5: 1}, Pred length: 4\nToken distribution (Batch 4): {1: 1}, Pred length: 1\nToken distribution (Batch 5): {1: 3, 10: 1}, Pred length: 4\nToken distribution (Batch 6): {5: 1, 1: 1}, Pred length: 2\nToken distribution (Batch 7): {1: 3, 27: 1}, Pred length: 4\nToken distribution (Batch 8): {1: 1, 32: 1}, Pred length: 2\nToken distribution (Batch 9): {34: 2, 47: 2, 1: 3, 32: 1}, Pred length: 8\nToken distribution (Batch 10): {11: 1, 1: 1}, Pred length: 2\nToken distribution (Batch 11): {1: 4, 11: 1, 27: 1}, Pred length: 6\nToken distribution (Batch 12): {1: 4, 35: 1}, Pred length: 5\nToken distribution (Batch 13): {47: 1, 1: 4, 10: 1, 12: 1, 32: 1}, Pred length: 8\nToken distribution (Batch 14): {32: 2, 1: 6}, Pred length: 8\nToken distribution (Batch 15): {4: 1, 32: 1, 1: 3, 5: 1, 34: 1, 10: 1}, Pred length: 8\nToken distribution (Batch 16): {34: 1, 47: 1, 1: 2}, Pred length: 4\nToken distribution (Batch 17): {1: 5, 47: 2, 27: 1}, Pred length: 8\nToken distribution (Batch 18): {1: 1}, Pred length: 1\nToken distribution (Batch 19): {47: 1, 1: 5, 4: 1}, Pred length: 7\nToken distribution (Batch 20): {32: 1, 1: 2, 47: 1, 27: 1}, Pred length: 5\nToken distribution (Batch 21): {1: 1, 11: 1}, Pred length: 2\nToken distribution (Batch 22): {1: 3, 12: 1, 32: 1}, Pred length: 5\nToken distribution (Batch 23): {1: 1, 10: 1, 5: 1}, Pred length: 3\nToken distribution (Batch 24): {1: 1, 34: 1}, Pred length: 2\nToken distribution (Batch 25): {1: 5, 12: 1, 32: 1}, Pred length: 7\nToken distribution (Batch 26): {1: 5, 5: 2, 32: 2, 47: 1}, Pred length: 10\nToken distribution (Batch 27): {10: 1, 1: 1, 4: 1}, Pred length: 3\nToken distribution (Batch 28): {10: 1, 1: 5, 22: 1, 27: 1}, Pred length: 8\nToken distribution (Batch 29): {1: 4, 10: 1, 5: 1}, Pred length: 6\nToken distribution (Batch 30): {1: 2, 32: 1, 5: 1}, Pred length: 4\nToken distribution (Batch 31): {11: 1, 47: 1, 1: 3, 34: 1, 32: 1, 10: 1}, Pred length: 8\nBatch 20, Gradient norm: 40.7931\nEpoch 4, Batch 20/55, Loss: 25.5813\nAvg Blank Probability: 0.0319\nSample predictions: ['a', 'HaF', 'a']\nGround Truth (first 3): ['ZSdZ', 'pqAf', '76PJ']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 4, 4]\nToken distribution (Batch 0): {1: 1}, Pred length: 1\nToken distribution (Batch 1): {1: 1, 32: 1}, Pred length: 2\nToken distribution (Batch 2): {32: 1}, Pred length: 1\nToken distribution (Batch 3): {1: 1}, Pred length: 1\nToken distribution (Batch 4): {19: 1}, Pred length: 1\nToken distribution (Batch 5): {42: 1}, Pred length: 1\nToken distribution (Batch 6): {1: 1}, Pred length: 1\nToken distribution (Batch 7): {1: 1}, Pred length: 1\nToken distribution (Batch 8): {5: 1}, Pred length: 1\nToken distribution (Batch 9): {1: 2}, Pred length: 2\nToken distribution (Batch 10): {1: 1}, Pred length: 1\nToken distribution (Batch 11): {27: 1}, Pred length: 1\nToken distribution (Batch 12): {32: 1}, Pred length: 1\nToken distribution (Batch 13): {1: 1}, Pred length: 1\nToken distribution (Batch 14): {5: 1}, Pred length: 1\nToken distribution (Batch 15): {5: 1}, Pred length: 1\nToken distribution (Batch 16): {19: 1}, Pred length: 1\nToken distribution (Batch 17): {1: 1, 19: 1}, Pred length: 2\nToken distribution (Batch 18): {32: 1, 1: 3, 47: 1, 34: 1}, Pred length: 6\nToken distribution (Batch 19): {34: 5, 1: 1}, Pred length: 6\nToken distribution (Batch 20): {1: 1, 5: 1}, Pred length: 2\nToken distribution (Batch 21): {34: 1}, Pred length: 1\nToken distribution (Batch 22): {34: 1, 1: 4}, Pred length: 5\nToken distribution (Batch 23): {19: 1}, Pred length: 1\nToken distribution (Batch 24): {34: 1}, Pred length: 1\nToken distribution (Batch 25): {1: 1}, Pred length: 1\nToken distribution (Batch 26): {1: 1, 32: 1}, Pred length: 2\nToken distribution (Batch 27): {1: 3}, Pred length: 3\nToken distribution (Batch 28): {32: 1}, Pred length: 1\nToken distribution (Batch 29): {1: 1, 27: 1}, Pred length: 2\nToken distribution (Batch 30): {10: 1}, Pred length: 1\nToken distribution (Batch 31): {1: 1}, Pred length: 1\nBatch 30, Gradient norm: 40.2277\nEpoch 4, Batch 30/55, Loss: 23.8459\nAvg Blank Probability: 0.0355\nSample predictions: ['a', 'aF', 'F']\nGround Truth (first 3): ['iqziy', 'y8hT', 'wZ5X']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 4]\nToken distribution (Batch 0): {34: 1}, Pred length: 1\nToken distribution (Batch 1): {34: 1, 1: 1}, Pred length: 2\nToken distribution (Batch 2): {1: 1}, Pred length: 1\nToken distribution (Batch 3): {1: 1}, Pred length: 1\nToken distribution (Batch 4): {1: 1}, Pred length: 1\nToken distribution (Batch 5): {11: 1}, Pred length: 1\nToken distribution (Batch 6): {1: 1}, Pred length: 1\nToken distribution (Batch 7): {32: 1}, Pred length: 1\nToken distribution (Batch 8): {1: 1}, Pred length: 1\nToken distribution (Batch 9): {34: 1}, Pred length: 1\nToken distribution (Batch 10): {1: 1}, Pred length: 1\nToken distribution (Batch 11): {34: 1}, Pred length: 1\nToken distribution (Batch 12): {1: 1}, Pred length: 1\nToken distribution (Batch 13): {19: 1}, Pred length: 1\nToken distribution (Batch 14): {5: 1}, Pred length: 1\nToken distribution (Batch 15): {1: 1}, Pred length: 1\nToken distribution (Batch 16): {5: 1}, Pred length: 1\nToken distribution (Batch 17): {1: 1}, Pred length: 1\nToken distribution (Batch 18): {32: 1}, Pred length: 1\nToken distribution (Batch 19): {1: 1}, Pred length: 1\nToken distribution (Batch 20): {32: 1}, Pred length: 1\nToken distribution (Batch 21): {1: 1}, Pred length: 1\nToken distribution (Batch 22): {34: 1}, Pred length: 1\nToken distribution (Batch 23): {10: 1}, Pred length: 1\nToken distribution (Batch 24): {10: 1}, Pred length: 1\nToken distribution (Batch 25): {1: 2}, Pred length: 2\nToken distribution (Batch 26): {32: 1}, Pred length: 1\nToken distribution (Batch 27): {47: 1}, Pred length: 1\nToken distribution (Batch 28): {34: 1}, Pred length: 1\nToken distribution (Batch 29): {1: 1}, Pred length: 1\nToken distribution (Batch 30): {1: 1}, Pred length: 1\nToken distribution (Batch 31): {1: 1}, Pred length: 1\nBatch 40, Gradient norm: 76.9527\nEpoch 4, Batch 40/55, Loss: 23.0485\nAvg Blank Probability: 0.0402\nSample predictions: ['H', 'Ha', 'a']\nGround Truth (first 3): ['rAUE', 'ZBg52', 'SBM1']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 5, 4]\nToken distribution (Batch 0): {1: 1}, Pred length: 1\nToken distribution (Batch 1): {19: 1}, Pred length: 1\nToken distribution (Batch 2): {19: 1}, Pred length: 1\nToken distribution (Batch 3): {1: 1}, Pred length: 1\nToken distribution (Batch 4): {5: 1}, Pred length: 1\nToken distribution (Batch 5): {1: 1}, Pred length: 1\nToken distribution (Batch 6): {1: 1}, Pred length: 1\nToken distribution (Batch 7): {1: 1}, Pred length: 1\nToken distribution (Batch 8): {27: 1}, Pred length: 1\nToken distribution (Batch 9): {27: 1}, Pred length: 1\nToken distribution (Batch 10): {1: 1}, Pred length: 1\nToken distribution (Batch 11): {32: 1}, Pred length: 1\nToken distribution (Batch 12): {1: 1}, Pred length: 1\nToken distribution (Batch 13): {27: 1}, Pred length: 1\nToken distribution (Batch 14): {1: 1}, Pred length: 1\nToken distribution (Batch 15): {5: 1}, Pred length: 1\nToken distribution (Batch 16): {19: 1}, Pred length: 1\nToken distribution (Batch 17): {32: 1}, Pred length: 1\nToken distribution (Batch 18): {5: 1}, Pred length: 1\nToken distribution (Batch 19): {1: 1}, Pred length: 1\nToken distribution (Batch 20): {34: 1}, Pred length: 1\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {1: 1}, Pred length: 1\nToken distribution (Batch 23): {4: 1}, Pred length: 1\nToken distribution (Batch 24): {19: 1}, Pred length: 1\nToken distribution (Batch 25): {1: 1}, Pred length: 1\nToken distribution (Batch 26): {1: 1}, Pred length: 1\nToken distribution (Batch 27): {1: 1}, Pred length: 1\nToken distribution (Batch 28): {1: 1}, Pred length: 1\nToken distribution (Batch 29): {10: 1}, Pred length: 1\nToken distribution (Batch 30): {27: 1}, Pred length: 1\nToken distribution (Batch 31): {1: 1}, Pred length: 1\nBatch 50, Gradient norm: 63.1817\nEpoch 4, Batch 50/55, Loss: 22.6568\nAvg Blank Probability: 0.0465\nSample predictions: ['a', 's', 's']\nGround Truth (first 3): ['u-MI', '*EHo', 'SMAE6']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 4, 5]\nEpoch 4/20, Loss: 24.0988\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nToken distribution (Batch 0): {1: 10}, Pred length: 10\nToken distribution (Batch 1): {1: 10}, Pred length: 10\nToken distribution (Batch 2): {1: 8}, Pred length: 8\nToken distribution (Batch 3): {1: 8}, Pred length: 8\nToken distribution (Batch 4): {1: 8}, Pred length: 8\nToken distribution (Batch 5): {1: 8}, Pred length: 8\nToken distribution (Batch 6): {1: 8}, Pred length: 8\nToken distribution (Batch 7): {1: 8}, Pred length: 8\nToken distribution (Batch 8): {1: 8}, Pred length: 8\nToken distribution (Batch 9): {1: 10}, Pred length: 10\nToken distribution (Batch 10): {1: 10}, Pred length: 10\nToken distribution (Batch 11): {1: 10}, Pred length: 10\nToken distribution (Batch 12): {1: 8}, Pred length: 8\nToken distribution (Batch 13): {1: 8}, Pred length: 8\nToken distribution (Batch 14): {1: 10}, Pred length: 10\nToken distribution (Batch 15): {1: 10}, Pred length: 10\nToken distribution (Batch 16): {1: 8}, Pred length: 8\nToken distribution (Batch 17): {1: 8}, Pred length: 8\nToken distribution (Batch 18): {1: 8}, Pred length: 8\nToken distribution (Batch 19): {1: 8}, Pred length: 8\nValidation Loss: 26.4255\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['36eFK', 'ZLqaH', 'dwE6', 'Wxfs', 'XxTJ']\nCurrent Learning Rate: 4.4e-06\nEpoch 5, Filtered data size: 2176, Sample labels: ['MdI8', '0tcL2', 'eyOxJ']\nTrain size: 1740, Val size: 436\nToken distribution (Batch 0): {32: 1}, Pred length: 1\nToken distribution (Batch 1): {1: 1}, Pred length: 1\nToken distribution (Batch 2): {5: 1}, Pred length: 1\nToken distribution (Batch 3): {61: 1}, Pred length: 1\nToken distribution (Batch 4): {1: 1}, Pred length: 1\nToken distribution (Batch 5): {5: 1}, Pred length: 1\nToken distribution (Batch 6): {34: 1}, Pred length: 1\nToken distribution (Batch 7): {32: 1}, Pred length: 1\nToken distribution (Batch 8): {5: 1}, Pred length: 1\nToken distribution (Batch 9): {1: 1}, Pred length: 1\nToken distribution (Batch 10): {34: 1}, Pred length: 1\nToken distribution (Batch 11): {4: 1}, Pred length: 1\nToken distribution (Batch 12): {1: 1}, Pred length: 1\nToken distribution (Batch 13): {19: 1}, Pred length: 1\nToken distribution (Batch 14): {1: 1}, Pred length: 1\nToken distribution (Batch 15): {1: 1}, Pred length: 1\nToken distribution (Batch 16): {32: 1}, Pred length: 1\nToken distribution (Batch 17): {34: 1}, Pred length: 1\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {42: 1}, Pred length: 1\nToken distribution (Batch 20): {27: 1}, Pred length: 1\nToken distribution (Batch 21): {19: 1}, Pred length: 1\nToken distribution (Batch 22): {1: 1}, Pred length: 1\nToken distribution (Batch 23): {1: 1}, Pred length: 1\nToken distribution (Batch 24): {4: 1}, Pred length: 1\nToken distribution (Batch 25): {1: 1}, Pred length: 1\nToken distribution (Batch 26): {1: 1}, Pred length: 1\nToken distribution (Batch 27): {27: 1}, Pred length: 1\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {1: 1}, Pred length: 1\nToken distribution (Batch 30): {1: 1}, Pred length: 1\nToken distribution (Batch 31): {34: 1}, Pred length: 1\nBatch 0, Gradient norm: 53.8955\nEpoch 5, Batch 0/55, Loss: 22.4128\nAvg Blank Probability: 0.0498\nSample predictions: ['F', 'a', 'e']\nGround Truth (first 3): ['SHJX', '0u-Z', 'ls9yh']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 4, 5]\nToken distribution (Batch 0): {34: 1}, Pred length: 1\nToken distribution (Batch 1): {1: 1}, Pred length: 1\nToken distribution (Batch 2): {1: 1}, Pred length: 1\nToken distribution (Batch 3): {1: 1}, Pred length: 1\nToken distribution (Batch 4): {4: 1}, Pred length: 1\nToken distribution (Batch 5): {27: 1}, Pred length: 1\nToken distribution (Batch 6): {5: 1}, Pred length: 1\nToken distribution (Batch 7): {32: 1}, Pred length: 1\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {1: 1}, Pred length: 1\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {32: 1}, Pred length: 1\nToken distribution (Batch 13): {27: 1}, Pred length: 1\nToken distribution (Batch 14): {1: 1}, Pred length: 1\nToken distribution (Batch 15): {34: 1}, Pred length: 1\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {1: 1}, Pred length: 1\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {1: 1}, Pred length: 1\nToken distribution (Batch 21): {1: 1}, Pred length: 1\nToken distribution (Batch 22): {32: 1}, Pred length: 1\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {32: 1}, Pred length: 1\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {1: 1}, Pred length: 1\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {34: 1}, Pred length: 1\nToken distribution (Batch 29): {5: 1}, Pred length: 1\nToken distribution (Batch 30): {1: 1}, Pred length: 1\nToken distribution (Batch 31): {1: 1}, Pred length: 1\nBatch 10, Gradient norm: 79.4529\nEpoch 5, Batch 10/55, Loss: 21.7729\nAvg Blank Probability: 0.0596\nSample predictions: ['H', 'a', 'a']\nGround Truth (first 3): ['OCL2N', 'w7su', 'EGr2']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {34: 1}, Pred length: 1\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {1: 1}, Pred length: 1\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {34: 1}, Pred length: 1\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {32: 1}, Pred length: 1\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {34: 1}, Pred length: 1\nToken distribution (Batch 16): {1: 1}, Pred length: 1\nToken distribution (Batch 17): {34: 1}, Pred length: 1\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {1: 1}, Pred length: 1\nToken distribution (Batch 20): {1: 1}, Pred length: 1\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {32: 1}, Pred length: 1\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {1: 1}, Pred length: 1\nToken distribution (Batch 26): {1: 1}, Pred length: 1\nToken distribution (Batch 27): {1: 1}, Pred length: 1\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {1: 1}, Pred length: 1\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {1: 1}, Pred length: 1\nBatch 20, Gradient norm: 192.6443\nEpoch 5, Batch 20/55, Loss: 19.3731\nAvg Blank Probability: 0.0709\nSample predictions: ['<empty>', 'H', '<empty>']\nGround Truth (first 3): ['cQyY', 'EnIm6', 'bSk2T']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 5, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {1: 1}, Pred length: 1\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 30, Gradient norm: 375.8732\nEpoch 5, Batch 30/55, Loss: 18.2970\nAvg Blank Probability: 0.0935\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['CLAK4', 'gCWj', 'pEl6']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 40, Gradient norm: 57.8806\nEpoch 5, Batch 40/55, Loss: 16.7944\nAvg Blank Probability: 0.1162\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['q1O9j', '4xt5b', 'jBLS']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 5, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 50, Gradient norm: 68.1143\nEpoch 5, Batch 50/55, Loss: 15.7653\nAvg Blank Probability: 0.1515\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['p29HH', 'gZsf', 'Pwps2']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 5]\nEpoch 5/20, Loss: 18.8470\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 100, Gradient norm: 540360768.0000\nEpoch 6, Batch 100/113, Loss: 6.2613\nAvg Blank Probability: 0.9499\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['oSZ4', 'Q*oL', 'pcS2Hq']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 4, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 110, Gradient norm: 13.8155\nEpoch 6, Batch 110/113, Loss: 6.6853\nAvg Blank Probability: 0.9672\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['K3Gq2d', '7inVbrN', 'Vao3a']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 7, 5]\nEpoch 6/20, Loss: 7.6855\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nValidation Loss: 11.8119\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['avtaJ', 'qmG-8', 'qb21Mt', 'RJbl0j', 'WdE7yfJ']\nCurrent Learning Rate: 5.500000000000001e-06\nEpoch 7, Filtered data size: 4521, Sample labels: ['qV8ib8H', 'MdI8', '0tcL2']\nTrain size: 3616, Val size: 905\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 0, Gradient norm: 122423504.0000\nEpoch 7, Batch 0/113, Loss: 6.7989\nAvg Blank Probability: 0.9684\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['OEKQFQ', 'ATmF80', 'wboT']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 6, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 10, Gradient norm: 19127.9844\nEpoch 7, Batch 10/113, Loss: 6.9452\nAvg Blank Probability: 0.9766\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['DrTbSD', 'LQIDp', 'S9gcM']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 5, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 20, Gradient norm: 18769.1895\nEpoch 7, Batch 20/113, Loss: 7.5054\nAvg Blank Probability: 0.9832\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['owoM-6', 'BSQDqRb', 'QpJvjOY']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 7, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 30, Gradient norm: 62871820042240.0000\nEpoch 7, Batch 30/113, Loss: 7.4316\nAvg Blank Probability: 0.9867\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['4s38hm0', 'ZU2Q', '9pEgR']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 4, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 40, Gradient norm: 75365.4766\nEpoch 7, Batch 40/113, Loss: 7.9625\nAvg Blank Probability: 0.9875\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['zr*5ez', 'NvgOap', 'mFqS']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 6, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 50, Gradient norm: 1461202714624.0000\nEpoch 7, Batch 50/113, Loss: 7.5542\nAvg Blank Probability: 0.9881\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Uulj3fs', 'PIC*', '96d3BaQ']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 4, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 60, Gradient norm: 23.0443\nEpoch 7, Batch 60/113, Loss: 8.3302\nAvg Blank Probability: 0.9904\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['VsWmX', 'FbcG', 'zlOOUX']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 70, Gradient norm: 22.4695\nEpoch 7, Batch 70/113, Loss: 8.1737\nAvg Blank Probability: 0.9899\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['FUR1S', 'TPpo', '0j0Hqh*']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 80, Gradient norm: 23.1706\nEpoch 7, Batch 80/113, Loss: 8.3033\nAvg Blank Probability: 0.9902\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['XCoTIWa', 'foVm', 'QJj1L']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 4, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 90, Gradient norm: 77808856.0000\nEpoch 7, Batch 90/113, Loss: 8.0936\nAvg Blank Probability: 0.9901\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['GG8jpuM', 'wRBI', '9AMYPt']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 4, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 100, Gradient norm: 149980.2812\nEpoch 7, Batch 100/113, Loss: 8.5871\nAvg Blank Probability: 0.9924\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['PQ1B65', 'VptZ3n', 'zUgGS']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 6, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 110, Gradient norm: 23.8030\nEpoch 7, Batch 110/113, Loss: 8.6925\nAvg Blank Probability: 0.9940\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['-bldKE', 'Cbsj', 'avtaJ']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 4, 5]\nEpoch 7/20, Loss: 7.9343\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nValidation Loss: 8.8732\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['9Ireq', 'oGDK', 'xx8R', 'BLssOo', 'FUAnv']\nCurrent Learning Rate: 5.500000000000001e-06\nEpoch 8, Filtered data size: 4521, Sample labels: ['qV8ib8H', 'MdI8', '0tcL2']\nTrain size: 3616, Val size: 905\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 0, Gradient norm: 16.0238\nEpoch 8, Batch 0/113, Loss: 8.8124\nAvg Blank Probability: 0.9945\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['O*0vL6', 'PqzT', 'x36t*A']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 4, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 10, Gradient norm: 1270434496512.0000\nEpoch 8, Batch 10/113, Loss: 9.3009\nAvg Blank Probability: 0.9957\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['ov2C65k', 'Sy1pI', 'STSuY']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 5, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 20, Gradient norm: 1599299125248.0000\nEpoch 8, Batch 20/113, Loss: 9.1110\nAvg Blank Probability: 0.9960\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['dz*4', 'dRm5PD', 'KnBrWLY']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 6, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 30, Gradient norm: 18080688128.0000\nEpoch 8, Batch 30/113, Loss: 9.4067\nAvg Blank Probability: 0.9968\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Wu5r', 'a1IPv', 'G1Bkkib']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 5, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 40, Gradient norm: 25.9069\nEpoch 8, Batch 40/113, Loss: 9.8469\nAvg Blank Probability: 0.9970\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['bem1yX', 'W7y5', 'J2Ook0p']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 4, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 50, Gradient norm: 4621.0845\nEpoch 8, Batch 50/113, Loss: 9.8212\nAvg Blank Probability: 0.9973\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Lall6kp', 'N6uxqB', 'dxl7jh6']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 6, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 60, Gradient norm: 5832.3335\nEpoch 8, Batch 60/113, Loss: 9.5216\nAvg Blank Probability: 0.9973\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['kp9P', '12egVP', 'uqd56lf']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 6, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 70, Gradient norm: 291725737984.0000\nEpoch 8, Batch 70/113, Loss: 9.6411\nAvg Blank Probability: 0.9978\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['YdI5p', 'cGkefuh', '-cQ6']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 7, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 80, Gradient norm: 1247.0078\nEpoch 8, Batch 80/113, Loss: 10.2177\nAvg Blank Probability: 0.9985\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['J4K--', 'dwEXAOG', '*lPzx']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 7, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 90, Gradient norm: 17.0310\nEpoch 8, Batch 90/113, Loss: 10.8327\nAvg Blank Probability: 0.9986\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['pEoG09', 'PhatZ', '*Bxn6pY']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 5, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 100, Gradient norm: 628492253869899776.0000\nEpoch 8, Batch 100/113, Loss: 10.8882\nAvg Blank Probability: 0.9990\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['IVnkA8', 'QZzPJ', 'ACP4z']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 5, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 110, Gradient norm: 17369094.0000\nEpoch 8, Batch 110/113, Loss: 10.7029\nAvg Blank Probability: 0.9990\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['x-tVOPU', 'pld*', 'KwlR3']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 4, 5]\nEpoch 8/20, Loss: 9.9143\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nValidation Loss: 7.1393\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['D2BEq', '*rc2T1', 'ywASN', 'tJOu', 'wTXv']\nCurrent Learning Rate: 5.500000000000001e-06\nEpoch 9, Filtered data size: 4521, Sample labels: ['qV8ib8H', 'MdI8', '0tcL2']\nTrain size: 3616, Val size: 905\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 0, Gradient norm: 109658712.0000\nEpoch 9, Batch 0/113, Loss: 11.3151\nAvg Blank Probability: 0.9992\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['s2Cy3', 'QYrK', 'Y0s9uqv']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 10, Gradient norm: 70778955497472.0000\nEpoch 9, Batch 10/113, Loss: 11.5655\nAvg Blank Probability: 0.9993\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['x-tVOPU', 'pRw1p', 'rmLX']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 5, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 20, Gradient norm: inf\nEpoch 9, Batch 20/113, Loss: 11.5752\nAvg Blank Probability: 0.9995\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['szTCVQs', 'HJotgZ', '9la6hc']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 6, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 30, Gradient norm: 39.2907\nEpoch 9, Batch 30/113, Loss: 11.9856\nAvg Blank Probability: 0.9996\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['6xWU', 'rYAXt', '-jwvv2i']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 5, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 40, Gradient norm: 28.8919\nEpoch 9, Batch 40/113, Loss: 12.1955\nAvg Blank Probability: 0.9996\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['dgIG', 'S8kT', 'ijZoDn']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 4, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 50, Gradient norm: 28.5578\nEpoch 9, Batch 50/113, Loss: 12.1422\nAvg Blank Probability: 0.9997\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['O9gv', 'bcS0q3I', '6JI5']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 7, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 60, Gradient norm: 917258649993216.0000\nEpoch 9, Batch 60/113, Loss: 12.2122\nAvg Blank Probability: 0.9997\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['fHSRR7', 't*CaYQZ', 'jYvlaa']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 7, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 70, Gradient norm: 39042700.0000\nEpoch 9, Batch 70/113, Loss: 12.2700\nAvg Blank Probability: 0.9997\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['z1iKMQ', 'TZIk3k', 'xtkx']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 6, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 80, Gradient norm: 29.4763\nEpoch 9, Batch 80/113, Loss: 12.6925\nAvg Blank Probability: 0.9997\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['XI25b', 'U5legq', 'oBsRy']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 6, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 90, Gradient norm: 224.8174\nEpoch 9, Batch 90/113, Loss: 12.9093\nAvg Blank Probability: 0.9997\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['jfC0p', 'v1WYEIW', 'wBXrL2R']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 7, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 100, Gradient norm: 25620157300736.0000\nEpoch 9, Batch 100/113, Loss: 12.4777\nAvg Blank Probability: 0.9998\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['1fzWw', 'V1w6N1', 'I43Qel']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 6, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 110, Gradient norm: 30.2363\nEpoch 9, Batch 110/113, Loss: 13.3600\nAvg Blank Probability: 0.9998\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['VOirgX', 'nq9Qa', 'A4A79sF']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 5, 7]\nEpoch 9/20, Loss: 12.2985\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nValidation Loss: 6.2278\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['zZj2P', 'Ic1Ps3y', '0-hQw', '0uw6-v', 'TUPz']\nCurrent Learning Rate: 5.500000000000001e-06\nEpoch 10, Filtered data size: 4521, Sample labels: ['qV8ib8H', 'MdI8', '0tcL2']\nTrain size: 3616, Val size: 905\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 0, Gradient norm: 44166244.0000\nEpoch 10, Batch 0/113, Loss: 12.8891\nAvg Blank Probability: 0.9998\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['XlvF5Er', 'xeZXsAh', 'DrTbSD']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 7, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 10, Gradient norm: 29.7806\nEpoch 10, Batch 10/113, Loss: 13.0697\nAvg Blank Probability: 0.9998\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['gar0tEw', 'zd5SsnF', 'NgWfST*']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 7, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 20, Gradient norm: 29.6779\nEpoch 10, Batch 20/113, Loss: 12.8843\nAvg Blank Probability: 0.9998\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Zy6xFpg', '4Wv-1', 'gNqsDj7']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 5, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 30, Gradient norm: 7626072064.0000\nEpoch 10, Batch 30/113, Loss: 13.0073\nAvg Blank Probability: 0.9998\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['8rGJ', 'l2BmAJ', 'BLssOo']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 6, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 40, Gradient norm: inf\nEpoch 10, Batch 40/113, Loss: 13.0246\nAvg Blank Probability: 0.9998\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['zfngIZV', 'zLC-F', 'w0PyCiu']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 5, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 50, Gradient norm: 1958699.2500\nEpoch 10, Batch 50/113, Loss: 13.1184\nAvg Blank Probability: 0.9999\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['ZrZ1J0', 'xqNOY4s', '9LkP']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 7, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 60, Gradient norm: 30.6370\nEpoch 10, Batch 60/113, Loss: 13.8251\nAvg Blank Probability: 0.9999\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['j4Z8xp', '7prWD', 'cbhvS']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 5, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 70, Gradient norm: 628471040.0000\nEpoch 10, Batch 70/113, Loss: 13.7270\nAvg Blank Probability: 0.9999\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['ou5C5', '6uEXose', '*Bxn6pY']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 7, 7]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 80, Gradient norm: 31.2270\nEpoch 10, Batch 80/113, Loss: 14.2173\nAvg Blank Probability: 0.9999\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['bhOkii-', 'NSTZ6We', 'zqN8J']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 7, 5]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 90, Gradient norm: 2716.0352\nEpoch 10, Batch 90/113, Loss: 13.9187\nAvg Blank Probability: 0.9999\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['8qQd', 'u3kPJ-', 'KbvwMp']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 6, 6]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 100, Gradient norm: inf\nEpoch 10, Batch 100/113, Loss: 13.1983\nAvg Blank Probability: 0.9999\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['ea3cBX', 'yobLjOT', 'oMqs']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 7, 4]\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 110, Gradient norm: 72856956171190272.0000\nEpoch 10, Batch 110/113, Loss: 13.5932\nAvg Blank Probability: 0.9999\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['qCciEv', 'FD4P', 'vx*hB-1']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 4, 7]\nEpoch 10/20, Loss: 13.4892\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nValidation Loss: 5.9979\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['zs1eQ', 'xJ8urDB', 'SiZI12', 'HiESJ', 'VE5z2dD']\nCurrent Learning Rate: 5.500000000000001e-06\nEpoch 11, Filtered data size: 10240, Sample labels: ['qV8ib8H', 'MdI8', '1XS0DRdJ']\nTrain size: 8192, Val size: 2048\nToken distribution (Batch 0): {}, Pred length: 0\nToken distribution (Batch 1): {}, Pred length: 0\nToken distribution (Batch 2): {}, Pred length: 0\nToken distribution (Batch 3): {}, Pred length: 0\nToken distribution (Batch 4): {}, Pred length: 0\nToken distribution (Batch 5): {}, Pred length: 0\nToken distribution (Batch 6): {}, Pred length: 0\nToken distribution (Batch 7): {}, Pred length: 0\nToken distribution (Batch 8): {}, Pred length: 0\nToken distribution (Batch 9): {}, Pred length: 0\nToken distribution (Batch 10): {}, Pred length: 0\nToken distribution (Batch 11): {}, Pred length: 0\nToken distribution (Batch 12): {}, Pred length: 0\nToken distribution (Batch 13): {}, Pred length: 0\nToken distribution (Batch 14): {}, Pred length: 0\nToken distribution (Batch 15): {}, Pred length: 0\nToken distribution (Batch 16): {}, Pred length: 0\nToken distribution (Batch 17): {}, Pred length: 0\nToken distribution (Batch 18): {}, Pred length: 0\nToken distribution (Batch 19): {}, Pred length: 0\nToken distribution (Batch 20): {}, Pred length: 0\nToken distribution (Batch 21): {}, Pred length: 0\nToken distribution (Batch 22): {}, Pred length: 0\nToken distribution (Batch 23): {}, Pred length: 0\nToken distribution (Batch 24): {}, Pred length: 0\nToken distribution (Batch 25): {}, Pred length: 0\nToken distribution (Batch 26): {}, Pred length: 0\nToken distribution (Batch 27): {}, Pred length: 0\nToken distribution (Batch 28): {}, Pred length: 0\nToken distribution (Batch 29): {}, Pred length: 0\nToken distribution (Batch 30): {}, Pred length: 0\nToken distribution (Batch 31): {}, Pred length: 0\nBatch 0, Gradient norm: 32.1699\nEpoch 11, Batch 0/256, Loss: 14.6861\nAvg Blank Probability: 0.9999\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['xtkx', 'EbQY', 'fYFC']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 4, 4]\nWarning: NaN or Inf loss at batch 2. Skipping...\nWarning: NaN or Inf loss at batch 3. Skipping...\nWarning: NaN or Inf loss at batch 4. Skipping...\nWarning: NaN or Inf loss at batch 5. Skipping...\nWarning: NaN or Inf loss at batch 6. Skipping...\nWarning: NaN or Inf loss at batch 7. Skipping...\nWarning: NaN or Inf loss at batch 8. Skipping...\nWarning: NaN or Inf loss at batch 9. Skipping...\nWarning: NaN or Inf loss at batch 10. Skipping...\nWarning: NaN or Inf loss at batch 11. Skipping...\nWarning: NaN or Inf loss at batch 12. Skipping...\nWarning: NaN or Inf loss at batch 13. Skipping...\nWarning: NaN or Inf loss at batch 14. Skipping...\nWarning: NaN or Inf loss at batch 15. Skipping...\nWarning: NaN or Inf loss at batch 16. Skipping...\nWarning: NaN or Inf loss at batch 17. Skipping...\nWarning: NaN or Inf loss at batch 18. Skipping...\nWarning: NaN or Inf loss at batch 19. Skipping...\nWarning: NaN or Inf loss at batch 20. Skipping...\nWarning: NaN or Inf loss at batch 21. Skipping...\nWarning: NaN or Inf loss at batch 22. Skipping...\nWarning: NaN or Inf loss at batch 23. Skipping...\nWarning: NaN or Inf loss at batch 24. Skipping...\nWarning: NaN or Inf loss at batch 25. Skipping...\nWarning: NaN or Inf loss at batch 26. Skipping...\nWarning: NaN or Inf loss at batch 27. Skipping...\nWarning: NaN or Inf loss at batch 28. Skipping...\nWarning: NaN or Inf loss at batch 29. Skipping...\nWarning: NaN or Inf loss at batch 30. Skipping...\nWarning: NaN or Inf loss at batch 31. Skipping...\nWarning: NaN or Inf loss at batch 32. Skipping...\nWarning: NaN or Inf loss at batch 33. Skipping...\nWarning: NaN or Inf loss at batch 34. Skipping...\nWarning: NaN or Inf loss at batch 35. Skipping...\nWarning: NaN or Inf loss at batch 36. Skipping...\nWarning: NaN or Inf loss at batch 37. Skipping...\nWarning: NaN or Inf loss at batch 38. Skipping...\nWarning: NaN or Inf loss at batch 39. Skipping...\nWarning: NaN or Inf loss at batch 40. Skipping...\nWarning: NaN or Inf loss at batch 41. Skipping...\nWarning: NaN or Inf loss at batch 42. Skipping...\nWarning: NaN or Inf loss at batch 43. Skipping...\nWarning: NaN or Inf loss at batch 44. Skipping...\nWarning: NaN or Inf loss at batch 45. Skipping...\nWarning: NaN or Inf loss at batch 46. Skipping...\nWarning: NaN or Inf loss at batch 47. Skipping...\nWarning: NaN or Inf loss at batch 48. Skipping...\nWarning: NaN or Inf loss at batch 49. Skipping...\nWarning: NaN or Inf loss at batch 50. Skipping...\nWarning: NaN or Inf loss at batch 51. Skipping...\nWarning: NaN or Inf loss at batch 52. Skipping...\nWarning: NaN or Inf loss at batch 53. Skipping...\nWarning: NaN or Inf loss at batch 54. Skipping...\nWarning: NaN or Inf loss at batch 55. Skipping...\nWarning: NaN or Inf loss at batch 56. Skipping...\nWarning: NaN or Inf loss at batch 57. Skipping...\nWarning: NaN or Inf loss at batch 58. Skipping...\nWarning: NaN or Inf loss at batch 59. Skipping...\nWarning: NaN or Inf loss at batch 60. Skipping...\nWarning: NaN or Inf loss at batch 61. Skipping...\nWarning: NaN or Inf loss at batch 62. Skipping...\nWarning: NaN or Inf loss at batch 63. Skipping...\nWarning: NaN or Inf loss at batch 64. Skipping...\nWarning: NaN or Inf loss at batch 65. Skipping...\nWarning: NaN or Inf loss at batch 66. Skipping...\nWarning: NaN or Inf loss at batch 67. Skipping...\nWarning: NaN or Inf loss at batch 68. Skipping...\nWarning: NaN or Inf loss at batch 69. Skipping...\nWarning: NaN or Inf loss at batch 70. Skipping...\nWarning: NaN or Inf loss at batch 71. Skipping...\nWarning: NaN or Inf loss at batch 72. Skipping...\nWarning: NaN or Inf loss at batch 73. Skipping...\nWarning: NaN or Inf loss at batch 74. Skipping...\nWarning: NaN or Inf loss at batch 75. Skipping...\nWarning: NaN or Inf loss at batch 76. Skipping...\nWarning: NaN or Inf loss at batch 77. Skipping...\nWarning: NaN or Inf loss at batch 78. Skipping...\nWarning: NaN or Inf loss at batch 79. Skipping...\nWarning: NaN or Inf loss at batch 80. Skipping...\nWarning: NaN or Inf loss at batch 81. Skipping...\nWarning: NaN or Inf loss at batch 82. Skipping...\nWarning: NaN or Inf loss at batch 83. Skipping...\nWarning: NaN or Inf loss at batch 84. Skipping...\nWarning: NaN or Inf loss at batch 85. Skipping...\nWarning: NaN or Inf loss at batch 86. Skipping...\nWarning: NaN or Inf loss at batch 87. Skipping...\nWarning: NaN or Inf loss at batch 88. Skipping...\nWarning: NaN or Inf loss at batch 89. Skipping...\nWarning: NaN or Inf loss at batch 90. Skipping...\nWarning: NaN or Inf loss at batch 91. Skipping...\nWarning: NaN or Inf loss at batch 92. Skipping...\nWarning: NaN or Inf loss at batch 93. Skipping...\nWarning: NaN or Inf loss at batch 94. Skipping...\nWarning: NaN or Inf loss at batch 95. Skipping...\nWarning: NaN or Inf loss at batch 96. Skipping...\nWarning: NaN or Inf loss at batch 97. Skipping...\nWarning: NaN or Inf loss at batch 98. Skipping...\nWarning: NaN or Inf loss at batch 99. Skipping...\nWarning: NaN or Inf loss at batch 100. Skipping...\nWarning: NaN or Inf loss at batch 101. Skipping...\nWarning: NaN or Inf loss at batch 102. Skipping...\nWarning: NaN or Inf loss at batch 103. Skipping...\nWarning: NaN or Inf loss at batch 104. Skipping...\nWarning: NaN or Inf loss at batch 105. Skipping...\nWarning: NaN or Inf loss at batch 106. Skipping...\nWarning: NaN or Inf loss at batch 107. Skipping...\nWarning: NaN or Inf loss at batch 108. Skipping...\nWarning: NaN or Inf loss at batch 109. Skipping...\nWarning: NaN or Inf loss at batch 110. Skipping...\nWarning: NaN or Inf loss at batch 111. Skipping...\nWarning: NaN or Inf loss at batch 112. Skipping...\nWarning: NaN or Inf loss at batch 113. Skipping...\nWarning: NaN or Inf loss at batch 114. Skipping...\nWarning: NaN or Inf loss at batch 115. Skipping...\nWarning: NaN or Inf loss at batch 116. Skipping...\nWarning: NaN or Inf loss at batch 117. Skipping...\nWarning: NaN or Inf loss at batch 118. Skipping...\nWarning: NaN or Inf loss at batch 119. Skipping...\nWarning: NaN or Inf loss at batch 120. Skipping...\nWarning: NaN or Inf loss at batch 121. Skipping...\nWarning: NaN or Inf loss at batch 122. Skipping...\nWarning: NaN or Inf loss at batch 123. Skipping...\nWarning: NaN or Inf loss at batch 124. Skipping...\nWarning: NaN or Inf loss at batch 125. Skipping...\nWarning: NaN or Inf loss at batch 126. Skipping...\nWarning: NaN or Inf loss at batch 127. Skipping...\nWarning: NaN or Inf loss at batch 128. Skipping...\nWarning: NaN or Inf loss at batch 129. Skipping...\nWarning: NaN or Inf loss at batch 130. Skipping...\nWarning: NaN or Inf loss at batch 131. Skipping...\nWarning: NaN or Inf loss at batch 132. Skipping...\nWarning: NaN or Inf loss at batch 133. Skipping...\nWarning: NaN or Inf loss at batch 134. Skipping...\nWarning: NaN or Inf loss at batch 135. Skipping...\nWarning: NaN or Inf loss at batch 136. Skipping...\nWarning: NaN or Inf loss at batch 137. Skipping...\nWarning: NaN or Inf loss at batch 138. Skipping...\nWarning: NaN or Inf loss at batch 139. Skipping...\nWarning: NaN or Inf loss at batch 140. Skipping...\nWarning: NaN or Inf loss at batch 141. Skipping...\nWarning: NaN or Inf loss at batch 142. Skipping...\nWarning: NaN or Inf loss at batch 143. Skipping...\nWarning: NaN or Inf loss at batch 144. Skipping...\nWarning: NaN or Inf loss at batch 145. Skipping...\nWarning: NaN or Inf loss at batch 146. Skipping...\nWarning: NaN or Inf loss at batch 147. Skipping...\nWarning: NaN or Inf loss at batch 148. Skipping...\nWarning: NaN or Inf loss at batch 149. Skipping...\nWarning: NaN or Inf loss at batch 150. Skipping...\nWarning: NaN or Inf loss at batch 151. Skipping...\nWarning: NaN or Inf loss at batch 152. Skipping...\nWarning: NaN or Inf loss at batch 153. Skipping...\nWarning: NaN or Inf loss at batch 154. Skipping...\nWarning: NaN or Inf loss at batch 155. Skipping...\nWarning: NaN or Inf loss at batch 156. Skipping...\nWarning: NaN or Inf loss at batch 157. Skipping...\nWarning: NaN or Inf loss at batch 158. Skipping...\nWarning: NaN or Inf loss at batch 159. Skipping...\nWarning: NaN or Inf loss at batch 160. Skipping...\nWarning: NaN or Inf loss at batch 161. Skipping...\nWarning: NaN or Inf loss at batch 162. Skipping...\nWarning: NaN or Inf loss at batch 163. Skipping...\nWarning: NaN or Inf loss at batch 164. Skipping...\nWarning: NaN or Inf loss at batch 165. Skipping...\nWarning: NaN or Inf loss at batch 166. Skipping...\nWarning: NaN or Inf loss at batch 167. Skipping...\nWarning: NaN or Inf loss at batch 168. Skipping...\nWarning: NaN or Inf loss at batch 169. Skipping...\nWarning: NaN or Inf loss at batch 170. Skipping...\nWarning: NaN or Inf loss at batch 171. Skipping...\nWarning: NaN or Inf loss at batch 172. Skipping...\nWarning: NaN or Inf loss at batch 173. Skipping...\nWarning: NaN or Inf loss at batch 174. Skipping...\nWarning: NaN or Inf loss at batch 175. Skipping...\nWarning: NaN or Inf loss at batch 176. Skipping...\nWarning: NaN or Inf loss at batch 177. Skipping...\nWarning: NaN or Inf loss at batch 178. Skipping...\nWarning: NaN or Inf loss at batch 179. Skipping...\nWarning: NaN or Inf loss at batch 180. Skipping...\nWarning: NaN or Inf loss at batch 181. Skipping...\nWarning: NaN or Inf loss at batch 182. Skipping...\nWarning: NaN or Inf loss at batch 183. Skipping...\nWarning: NaN or Inf loss at batch 184. Skipping...\nWarning: NaN or Inf loss at batch 185. Skipping...\nWarning: NaN or Inf loss at batch 186. Skipping...\nWarning: NaN or Inf loss at batch 187. Skipping...\nWarning: NaN or Inf loss at batch 188. Skipping...\nWarning: NaN or Inf loss at batch 189. Skipping...\nWarning: NaN or Inf loss at batch 190. Skipping...\nWarning: NaN or Inf loss at batch 191. Skipping...\nWarning: NaN or Inf loss at batch 192. Skipping...\nWarning: NaN or Inf loss at batch 193. Skipping...\nWarning: NaN or Inf loss at batch 194. Skipping...\nWarning: NaN or Inf loss at batch 195. Skipping...\nWarning: NaN or Inf loss at batch 196. Skipping...\nWarning: NaN or Inf loss at batch 197. Skipping...\nWarning: NaN or Inf loss at batch 198. Skipping...\nWarning: NaN or Inf loss at batch 199. Skipping...\nWarning: NaN or Inf loss at batch 200. Skipping...\nWarning: NaN or Inf loss at batch 201. Skipping...\nWarning: NaN or Inf loss at batch 202. Skipping...\nWarning: NaN or Inf loss at batch 203. Skipping...\nWarning: NaN or Inf loss at batch 204. Skipping...\nWarning: NaN or Inf loss at batch 205. Skipping...\nWarning: NaN or Inf loss at batch 206. Skipping...\nWarning: NaN or Inf loss at batch 207. Skipping...\nWarning: NaN or Inf loss at batch 208. Skipping...\nWarning: NaN or Inf loss at batch 209. Skipping...\nWarning: NaN or Inf loss at batch 210. Skipping...\nWarning: NaN or Inf loss at batch 211. Skipping...\nWarning: NaN or Inf loss at batch 212. Skipping...\nWarning: NaN or Inf loss at batch 213. Skipping...\nWarning: NaN or Inf loss at batch 214. Skipping...\nWarning: NaN or Inf loss at batch 215. Skipping...\nWarning: NaN or Inf loss at batch 216. Skipping...\nWarning: NaN or Inf loss at batch 217. Skipping...\nWarning: NaN or Inf loss at batch 218. Skipping...\nWarning: NaN or Inf loss at batch 219. Skipping...\nWarning: NaN or Inf loss at batch 220. Skipping...\nWarning: NaN or Inf loss at batch 221. Skipping...\nWarning: NaN or Inf loss at batch 222. Skipping...\nWarning: NaN or Inf loss at batch 223. Skipping...\nWarning: NaN or Inf loss at batch 224. Skipping...\nWarning: NaN or Inf loss at batch 225. Skipping...\nWarning: NaN or Inf loss at batch 226. Skipping...\nWarning: NaN or Inf loss at batch 227. Skipping...\nWarning: NaN or Inf loss at batch 228. Skipping...\nWarning: NaN or Inf loss at batch 229. Skipping...\nWarning: NaN or Inf loss at batch 230. Skipping...\nWarning: NaN or Inf loss at batch 231. Skipping...\nWarning: NaN or Inf loss at batch 232. Skipping...\nWarning: NaN or Inf loss at batch 233. Skipping...\nWarning: NaN or Inf loss at batch 234. Skipping...\nWarning: NaN or Inf loss at batch 235. Skipping...\nWarning: NaN or Inf loss at batch 236. Skipping...\nWarning: NaN or Inf loss at batch 237. Skipping...\nWarning: NaN or Inf loss at batch 238. Skipping...\nWarning: NaN or Inf loss at batch 239. Skipping...\nWarning: NaN or Inf loss at batch 240. Skipping...\nWarning: NaN or Inf loss at batch 241. Skipping...\nWarning: NaN or Inf loss at batch 242. Skipping...\nWarning: NaN or Inf loss at batch 243. Skipping...\nWarning: NaN or Inf loss at batch 244. Skipping...\nWarning: NaN or Inf loss at batch 245. Skipping...\nWarning: NaN or Inf loss at batch 246. Skipping...\nWarning: NaN or Inf loss at batch 247. Skipping...\nWarning: NaN or Inf loss at batch 248. Skipping...\nWarning: NaN or Inf loss at batch 249. Skipping...\nWarning: NaN or Inf loss at batch 250. Skipping...\nWarning: NaN or Inf loss at batch 251. Skipping...\nWarning: NaN or Inf loss at batch 252. Skipping...\nWarning: NaN or Inf loss at batch 253. Skipping...\nWarning: NaN or Inf loss at batch 254. Skipping...\nWarning: NaN or Inf loss at batch 255. Skipping...\nEpoch 11/20, Loss: 0.1142\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nWarning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\nToken distribution (Batch 0): {41: 16}, Pred length: 16\nToken distribution (Batch 1): {41: 20}, Pred length: 20\nToken distribution (Batch 2): {41: 22}, Pred length: 22\nToken distribution (Batch 3): {41: 20}, Pred length: 20\nToken distribution (Batch 4): {41: 24}, Pred length: 24\nToken distribution (Batch 5): {41: 14}, Pred length: 14\nToken distribution (Batch 6): {41: 20}, Pred length: 20\nToken distribution (Batch 7): {41: 20}, Pred length: 20\nToken distribution (Batch 8): {41: 18}, Pred length: 18\nToken distribution (Batch 9): {41: 24}, Pred length: 24\nToken distribution (Batch 10): {41: 14}, Pred length: 14\nToken distribution (Batch 11): {41: 18}, Pred length: 18\nToken distribution (Batch 12): {41: 8}, Pred length: 8\nToken distribution (Batch 13): {41: 14}, Pred length: 14\nToken distribution (Batch 14): {41: 8}, Pred length: 8\nToken distribution (Batch 15): {41: 14}, Pred length: 14\nToken distribution (Batch 16): {41: 8}, Pred length: 8\nToken distribution (Batch 17): {41: 14}, Pred length: 14\nToken distribution (Batch 18): {41: 22}, Pred length: 22\nToken distribution (Batch 19): {41: 22}, Pred length: 22\nToken distribution (Batch 20): {41: 8}, Pred length: 8\nToken distribution (Batch 21): {41: 14}, Pred length: 14\nToken distribution (Batch 22): {41: 10}, Pred length: 10\nToken distribution (Batch 23): {41: 22}, Pred length: 22\nToken distribution (Batch 24): {41: 14}, Pred length: 14\nToken distribution (Batch 25): {41: 22}, Pred length: 22\nToken distribution (Batch 26): {41: 22}, Pred length: 22\nToken distribution (Batch 27): {41: 22}, Pred length: 22\nToken distribution (Batch 28): {41: 24}, Pred length: 24\nToken distribution (Batch 29): {41: 10}, Pred length: 10\nToken distribution (Batch 30): {41: 14}, Pred length: 14\nToken distribution (Batch 31): {41: 16}, Pred length: 16\nValidation Loss: nan\nValidation Predictions: ['O', 'O', 'O', 'O', 'O']\nGround Truth: ['RVec3hb5', '0CzcXgaIWt', '*ZwDEB-Fo5X', 'eUiPBDnUV2', 'tt-5faJEn6xp']\nCurrent Learning Rate: 5.500000000000001e-06\nEpoch 12, Filtered data size: 10240, Sample labels: ['qV8ib8H', 'MdI8', '1XS0DRdJ']\nTrain size: 8192, Val size: 2048\nWarning: NaN or Inf loss at batch 0. Skipping...\nWarning: NaN or Inf loss at batch 1. Skipping...\nWarning: NaN or Inf loss at batch 2. Skipping...\nWarning: NaN or Inf loss at batch 3. Skipping...\nWarning: NaN or Inf loss at batch 4. Skipping...\nWarning: NaN or Inf loss at batch 5. Skipping...\nWarning: NaN or Inf loss at batch 6. Skipping...\nWarning: NaN or Inf loss at batch 7. Skipping...\nWarning: NaN or Inf loss at batch 8. Skipping...\nWarning: NaN or Inf loss at batch 9. Skipping...\nWarning: NaN or Inf loss at batch 10. Skipping...\nWarning: NaN or Inf loss at batch 11. Skipping...\nWarning: NaN or Inf loss at batch 12. Skipping...\nWarning: NaN or Inf loss at batch 13. Skipping...\nWarning: NaN or Inf loss at batch 14. Skipping...\nWarning: NaN or Inf loss at batch 15. Skipping...\nWarning: NaN or Inf loss at batch 16. Skipping...\nWarning: NaN or Inf loss at batch 17. Skipping...\nWarning: NaN or Inf loss at batch 18. Skipping...\nWarning: NaN or Inf loss at batch 19. Skipping...\nWarning: NaN or Inf loss at batch 20. Skipping...\nWarning: NaN or Inf loss at batch 21. Skipping...\nWarning: NaN or Inf loss at batch 22. Skipping...\nWarning: NaN or Inf loss at batch 23. Skipping...\nWarning: NaN or Inf loss at batch 24. Skipping...\nWarning: NaN or Inf loss at batch 25. Skipping...\nWarning: NaN or Inf loss at batch 26. Skipping...\nWarning: NaN or Inf loss at batch 27. Skipping...\nWarning: NaN or Inf loss at batch 28. Skipping...\nWarning: NaN or Inf loss at batch 29. Skipping...\nWarning: NaN or Inf loss at batch 30. Skipping...\nWarning: NaN or Inf loss at batch 31. Skipping...\nWarning: NaN or Inf loss at batch 32. Skipping...\nWarning: NaN or Inf loss at batch 33. Skipping...\nWarning: NaN or Inf loss at batch 34. Skipping...\nWarning: NaN or Inf loss at batch 35. Skipping...\nWarning: NaN or Inf loss at batch 36. Skipping...\nWarning: NaN or Inf loss at batch 37. Skipping...\nWarning: NaN or Inf loss at batch 38. Skipping...\nWarning: NaN or Inf loss at batch 39. Skipping...\nWarning: NaN or Inf loss at batch 40. Skipping...\nWarning: NaN or Inf loss at batch 41. Skipping...\nWarning: NaN or Inf loss at batch 42. Skipping...\nWarning: NaN or Inf loss at batch 43. Skipping...\nWarning: NaN or Inf loss at batch 44. Skipping...\nWarning: NaN or Inf loss at batch 45. Skipping...\nWarning: NaN or Inf loss at batch 46. Skipping...\nWarning: NaN or Inf loss at batch 47. Skipping...\nWarning: NaN or Inf loss at batch 48. Skipping...\nWarning: NaN or Inf loss at batch 49. Skipping...\nWarning: NaN or Inf loss at batch 50. Skipping...\nWarning: NaN or Inf loss at batch 51. Skipping...\nWarning: NaN or Inf loss at batch 52. Skipping...\nWarning: NaN or Inf loss at batch 53. Skipping...\nWarning: NaN or Inf loss at batch 54. Skipping...\nWarning: NaN or Inf loss at batch 55. Skipping...\nWarning: NaN or Inf loss at batch 56. Skipping...\nWarning: NaN or Inf loss at batch 57. Skipping...\nWarning: NaN or Inf loss at batch 58. Skipping...\nWarning: NaN or Inf loss at batch 59. Skipping...\nWarning: NaN or Inf loss at batch 60. Skipping...\nWarning: NaN or Inf loss at batch 61. Skipping...\nWarning: NaN or Inf loss at batch 62. Skipping...\nWarning: NaN or Inf loss at batch 63. Skipping...\nWarning: NaN or Inf loss at batch 64. Skipping...\nWarning: NaN or Inf loss at batch 65. Skipping...\nWarning: NaN or Inf loss at batch 66. Skipping...\nWarning: NaN or Inf loss at batch 67. Skipping...\nWarning: NaN or Inf loss at batch 68. Skipping...\nWarning: NaN or Inf loss at batch 69. Skipping...\nWarning: NaN or Inf loss at batch 70. Skipping...\nWarning: NaN or Inf loss at batch 71. Skipping...\nWarning: NaN or Inf loss at batch 72. Skipping...\nWarning: NaN or Inf loss at batch 73. Skipping...\nWarning: NaN or Inf loss at batch 74. Skipping...\nWarning: NaN or Inf loss at batch 75. Skipping...\nWarning: NaN or Inf loss at batch 76. Skipping...\nWarning: NaN or Inf loss at batch 77. Skipping...\nWarning: NaN or Inf loss at batch 78. Skipping...\nWarning: NaN or Inf loss at batch 79. Skipping...\nWarning: NaN or Inf loss at batch 80. Skipping...\nWarning: NaN or Inf loss at batch 81. Skipping...\nWarning: NaN or Inf loss at batch 82. Skipping...\nWarning: NaN or Inf loss at batch 83. Skipping...\nWarning: NaN or Inf loss at batch 84. Skipping...\nWarning: NaN or Inf loss at batch 85. Skipping...\nWarning: NaN or Inf loss at batch 86. Skipping...\nWarning: NaN or Inf loss at batch 87. Skipping...\nWarning: NaN or Inf loss at batch 88. Skipping...\nWarning: NaN or Inf loss at batch 89. Skipping...\nWarning: NaN or Inf loss at batch 90. Skipping...\nWarning: NaN or Inf loss at batch 91. Skipping...\nWarning: NaN or Inf loss at batch 92. Skipping...\nWarning: NaN or Inf loss at batch 93. Skipping...\nWarning: NaN or Inf loss at batch 94. Skipping...\nWarning: NaN or Inf loss at batch 95. Skipping...\nWarning: NaN or Inf loss at batch 96. Skipping...\nWarning: NaN or Inf loss at batch 97. Skipping...\nWarning: NaN or Inf loss at batch 98. Skipping...\nWarning: NaN or Inf loss at batch 99. Skipping...\nWarning: NaN or Inf loss at batch 100. Skipping...\nWarning: NaN or Inf loss at batch 101. Skipping...\nWarning: NaN or Inf loss at batch 102. Skipping...\nWarning: NaN or Inf loss at batch 103. Skipping...\nWarning: NaN or Inf loss at batch 104. Skipping...\nWarning: NaN or Inf loss at batch 105. Skipping...\nWarning: NaN or Inf loss at batch 106. Skipping...\nWarning: NaN or Inf loss at batch 107. Skipping...\nWarning: NaN or Inf loss at batch 108. Skipping...\nWarning: NaN or Inf loss at batch 109. Skipping...\nWarning: NaN or Inf loss at batch 110. Skipping...\nWarning: NaN or Inf loss at batch 111. Skipping...\nWarning: NaN or Inf loss at batch 112. Skipping...\nWarning: NaN or Inf loss at batch 113. Skipping...\nWarning: NaN or Inf loss at batch 114. Skipping...\nWarning: NaN or Inf loss at batch 115. Skipping...\nWarning: NaN or Inf loss at batch 116. Skipping...\nWarning: NaN or Inf loss at batch 117. Skipping...\nWarning: NaN or Inf loss at batch 118. Skipping...\nWarning: NaN or Inf loss at batch 119. Skipping...\nWarning: NaN or Inf loss at batch 120. Skipping...\nWarning: NaN or Inf loss at batch 121. Skipping...\nWarning: NaN or Inf loss at batch 122. Skipping...\nWarning: NaN or Inf loss at batch 123. Skipping...\nWarning: NaN or Inf loss at batch 124. Skipping...\nWarning: NaN or Inf loss at batch 125. Skipping...\nWarning: NaN or Inf loss at batch 126. Skipping...\nWarning: NaN or Inf loss at batch 127. Skipping...\nWarning: NaN or Inf loss at batch 128. Skipping...\nWarning: NaN or Inf loss at batch 129. Skipping...\nWarning: NaN or Inf loss at batch 130. Skipping...\nWarning: NaN or Inf loss at batch 131. Skipping...\nWarning: NaN or Inf loss at batch 132. Skipping...\nWarning: NaN or Inf loss at batch 133. Skipping...\nWarning: NaN or Inf loss at batch 134. Skipping...\nWarning: NaN or Inf loss at batch 135. Skipping...\nWarning: NaN or Inf loss at batch 136. Skipping...\nWarning: NaN or Inf loss at batch 137. Skipping...\nWarning: NaN or Inf loss at batch 138. Skipping...\nWarning: NaN or Inf loss at batch 139. Skipping...\nWarning: NaN or Inf loss at batch 140. Skipping...\nWarning: NaN or Inf loss at batch 141. Skipping...\nWarning: NaN or Inf loss at batch 142. Skipping...\nWarning: NaN or Inf loss at batch 143. Skipping...\nWarning: NaN or Inf loss at batch 144. Skipping...\nWarning: NaN or Inf loss at batch 145. Skipping...\nWarning: NaN or Inf loss at batch 146. Skipping...\nWarning: NaN or Inf loss at batch 147. Skipping...\nWarning: NaN or Inf loss at batch 148. Skipping...\nWarning: NaN or Inf loss at batch 149. Skipping...\nWarning: NaN or Inf loss at batch 150. Skipping...\nWarning: NaN or Inf loss at batch 151. Skipping...\nWarning: NaN or Inf loss at batch 152. Skipping...\nWarning: NaN or Inf loss at batch 153. Skipping...\nWarning: NaN or Inf loss at batch 154. Skipping...\nWarning: NaN or Inf loss at batch 155. Skipping...\nWarning: NaN or Inf loss at batch 156. Skipping...\nWarning: NaN or Inf loss at batch 157. Skipping...\nWarning: NaN or Inf loss at batch 158. Skipping...\nWarning: NaN or Inf loss at batch 159. Skipping...\nWarning: NaN or Inf loss at batch 160. Skipping...\nWarning: NaN or Inf loss at batch 161. Skipping...\nWarning: NaN or Inf loss at batch 162. Skipping...\nWarning: NaN or Inf loss at batch 163. Skipping...\nWarning: NaN or Inf loss at batch 164. Skipping...\nWarning: NaN or Inf loss at batch 165. Skipping...\nWarning: NaN or Inf loss at batch 166. Skipping...\nWarning: NaN or Inf loss at batch 167. Skipping...\nWarning: NaN or Inf loss at batch 168. Skipping...\nWarning: NaN or Inf loss at batch 169. Skipping...\nWarning: NaN or Inf loss at batch 170. Skipping...\nWarning: NaN or Inf loss at batch 171. Skipping...\nWarning: NaN or Inf loss at batch 172. Skipping...\nWarning: NaN or Inf loss at batch 173. Skipping...\nWarning: NaN or Inf loss at batch 174. Skipping...\nWarning: NaN or Inf loss at batch 175. Skipping...\nWarning: NaN or Inf loss at batch 176. Skipping...\nWarning: NaN or Inf loss at batch 177. Skipping...\nWarning: NaN or Inf loss at batch 178. Skipping...\nWarning: NaN or Inf loss at batch 179. Skipping...\nWarning: NaN or Inf loss at batch 180. Skipping...\nWarning: NaN or Inf loss at batch 181. Skipping...\nWarning: NaN or Inf loss at batch 182. Skipping...\nWarning: NaN or Inf loss at batch 183. Skipping...\nWarning: NaN or Inf loss at batch 184. Skipping...\nWarning: NaN or Inf loss at batch 185. Skipping...\nWarning: NaN or Inf loss at batch 186. Skipping...\nWarning: NaN or Inf loss at batch 187. Skipping...\nWarning: NaN or Inf loss at batch 188. Skipping...\nWarning: NaN or Inf loss at batch 189. Skipping...\nWarning: NaN or Inf loss at batch 190. Skipping...\nWarning: NaN or Inf loss at batch 191. Skipping...\nWarning: NaN or Inf loss at batch 192. Skipping...\nWarning: NaN or Inf loss at batch 193. Skipping...\nWarning: NaN or Inf loss at batch 194. Skipping...\nWarning: NaN or Inf loss at batch 195. Skipping...\nWarning: NaN or Inf loss at batch 196. Skipping...\nWarning: NaN or Inf loss at batch 197. Skipping...\nWarning: NaN or Inf loss at batch 198. Skipping...\nWarning: NaN or Inf loss at batch 199. Skipping...\nWarning: NaN or Inf loss at batch 200. Skipping...\nWarning: NaN or Inf loss at batch 201. Skipping...\nWarning: NaN or Inf loss at batch 202. Skipping...\nWarning: NaN or Inf loss at batch 203. Skipping...\nWarning: NaN or Inf loss at batch 204. Skipping...\nWarning: NaN or Inf loss at batch 205. Skipping...\nWarning: NaN or Inf loss at batch 206. Skipping...\nWarning: NaN or Inf loss at batch 207. Skipping...\nWarning: NaN or Inf loss at batch 208. Skipping...\nWarning: NaN or Inf loss at batch 209. Skipping...\nWarning: NaN or Inf loss at batch 210. Skipping...\nWarning: NaN or Inf loss at batch 211. Skipping...\nWarning: NaN or Inf loss at batch 212. Skipping...\nWarning: NaN or Inf loss at batch 213. Skipping...\nWarning: NaN or Inf loss at batch 214. Skipping...\nWarning: NaN or Inf loss at batch 215. Skipping...\nWarning: NaN or Inf loss at batch 216. Skipping...\nWarning: NaN or Inf loss at batch 217. Skipping...\nWarning: NaN or Inf loss at batch 218. Skipping...\nWarning: NaN or Inf loss at batch 219. Skipping...\nWarning: NaN or Inf loss at batch 220. Skipping...\nWarning: NaN or Inf loss at batch 221. Skipping...\nWarning: NaN or Inf loss at batch 222. Skipping...\nWarning: NaN or Inf loss at batch 223. Skipping...\nWarning: NaN or Inf loss at batch 224. Skipping...\nWarning: NaN or Inf loss at batch 225. Skipping...\nWarning: NaN or Inf loss at batch 226. Skipping...\nWarning: NaN or Inf loss at batch 227. Skipping...\nWarning: NaN or Inf loss at batch 228. Skipping...\nWarning: NaN or Inf loss at batch 229. Skipping...\nWarning: NaN or Inf loss at batch 230. Skipping...\nWarning: NaN or Inf loss at batch 231. Skipping...\nWarning: NaN or Inf loss at batch 232. Skipping...\nWarning: NaN or Inf loss at batch 233. Skipping...\nWarning: NaN or Inf loss at batch 234. Skipping...\nWarning: NaN or Inf loss at batch 235. Skipping...\nWarning: NaN or Inf loss at batch 236. Skipping...\nWarning: NaN or Inf loss at batch 237. Skipping...\nWarning: NaN or Inf loss at batch 238. Skipping...\nWarning: NaN or Inf loss at batch 239. Skipping...\nWarning: NaN or Inf loss at batch 240. Skipping...\nWarning: NaN or Inf loss at batch 241. Skipping...\nWarning: NaN or Inf loss at batch 242. Skipping...\nWarning: NaN or Inf loss at batch 243. Skipping...\nWarning: NaN or Inf loss at batch 244. Skipping...\nWarning: NaN or Inf loss at batch 245. Skipping...\nWarning: NaN or Inf loss at batch 246. Skipping...\nWarning: NaN or Inf loss at batch 247. Skipping...\nWarning: NaN or Inf loss at batch 248. Skipping...\nWarning: NaN or Inf loss at batch 249. Skipping...\nWarning: NaN or Inf loss at batch 250. Skipping...\nWarning: NaN or Inf loss at batch 251. Skipping...\nWarning: NaN or Inf loss at batch 252. Skipping...\nWarning: NaN or Inf loss at batch 253. Skipping...\nWarning: NaN or Inf loss at batch 254. Skipping...\nWarning: NaN or Inf loss at batch 255. Skipping...\nEpoch 12/20, Loss: 0.0000\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nWarning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\nToken distribution (Batch 0): {41: 18}, Pred length: 18\nToken distribution (Batch 1): {41: 14}, Pred length: 14\nToken distribution (Batch 2): {41: 14}, Pred length: 14\nToken distribution (Batch 3): {41: 16}, Pred length: 16\nToken distribution (Batch 4): {41: 12}, Pred length: 12\nToken distribution (Batch 5): {41: 24}, Pred length: 24\nToken distribution (Batch 6): {41: 16}, Pred length: 16\nToken distribution (Batch 7): {41: 8}, Pred length: 8\nToken distribution (Batch 8): {41: 10}, Pred length: 10\nToken distribution (Batch 9): {41: 14}, Pred length: 14\nToken distribution (Batch 10): {41: 12}, Pred length: 12\nToken distribution (Batch 11): {41: 16}, Pred length: 16\nToken distribution (Batch 12): {41: 20}, Pred length: 20\nToken distribution (Batch 13): {41: 12}, Pred length: 12\nToken distribution (Batch 14): {41: 24}, Pred length: 24\nToken distribution (Batch 15): {41: 16}, Pred length: 16\nToken distribution (Batch 16): {41: 22}, Pred length: 22\nToken distribution (Batch 17): {41: 22}, Pred length: 22\nToken distribution (Batch 18): {41: 22}, Pred length: 22\nToken distribution (Batch 19): {41: 12}, Pred length: 12\nToken distribution (Batch 20): {41: 20}, Pred length: 20\nToken distribution (Batch 21): {41: 12}, Pred length: 12\nToken distribution (Batch 22): {41: 24}, Pred length: 24\nToken distribution (Batch 23): {41: 20}, Pred length: 20\nToken distribution (Batch 24): {41: 18}, Pred length: 18\nToken distribution (Batch 25): {41: 18}, Pred length: 18\nToken distribution (Batch 26): {41: 22}, Pred length: 22\nToken distribution (Batch 27): {41: 22}, Pred length: 22\nToken distribution (Batch 28): {41: 22}, Pred length: 22\nToken distribution (Batch 29): {41: 12}, Pred length: 12\nToken distribution (Batch 30): {41: 12}, Pred length: 12\nToken distribution (Batch 31): {41: 18}, Pred length: 18\nValidation Loss: nan\nValidation Predictions: ['O', 'O', 'O', 'O', 'O']\nGround Truth: ['ve7uMQO-s', 'eL-9rvB', 'uBtzogk', 'ODcDyF2N', 'BInWpy']\nCurrent Learning Rate: 5.500000000000001e-06\nEpoch 13, Filtered data size: 10240, Sample labels: ['qV8ib8H', 'MdI8', '1XS0DRdJ']\nTrain size: 8192, Val size: 2048\nWarning: NaN or Inf loss at batch 0. Skipping...\nWarning: NaN or Inf loss at batch 1. Skipping...\nWarning: NaN or Inf loss at batch 2. Skipping...\nWarning: NaN or Inf loss at batch 3. Skipping...\nWarning: NaN or Inf loss at batch 4. Skipping...\nWarning: NaN or Inf loss at batch 5. Skipping...\nWarning: NaN or Inf loss at batch 6. Skipping...\nWarning: NaN or Inf loss at batch 7. Skipping...\nWarning: NaN or Inf loss at batch 8. Skipping...\nWarning: NaN or Inf loss at batch 9. Skipping...\nWarning: NaN or Inf loss at batch 10. Skipping...\nWarning: NaN or Inf loss at batch 11. Skipping...\nWarning: NaN or Inf loss at batch 12. Skipping...\nWarning: NaN or Inf loss at batch 13. Skipping...\nWarning: NaN or Inf loss at batch 14. Skipping...\nWarning: NaN or Inf loss at batch 15. Skipping...\nWarning: NaN or Inf loss at batch 16. Skipping...\nWarning: NaN or Inf loss at batch 17. Skipping...\nWarning: NaN or Inf loss at batch 18. Skipping...\nWarning: NaN or Inf loss at batch 19. Skipping...\nWarning: NaN or Inf loss at batch 20. Skipping...\nWarning: NaN or Inf loss at batch 21. Skipping...\nWarning: NaN or Inf loss at batch 22. Skipping...\nWarning: NaN or Inf loss at batch 23. Skipping...\nWarning: NaN or Inf loss at batch 24. Skipping...\nWarning: NaN or Inf loss at batch 25. Skipping...\nWarning: NaN or Inf loss at batch 26. Skipping...\nWarning: NaN or Inf loss at batch 27. Skipping...\nWarning: NaN or Inf loss at batch 28. Skipping...\nWarning: NaN or Inf loss at batch 29. Skipping...\nWarning: NaN or Inf loss at batch 30. Skipping...\nWarning: NaN or Inf loss at batch 31. Skipping...\nWarning: NaN or Inf loss at batch 32. Skipping...\nWarning: NaN or Inf loss at batch 33. Skipping...\nWarning: NaN or Inf loss at batch 34. Skipping...\nWarning: NaN or Inf loss at batch 35. Skipping...\nWarning: NaN or Inf loss at batch 36. Skipping...\nWarning: NaN or Inf loss at batch 37. Skipping...\nWarning: NaN or Inf loss at batch 38. Skipping...\nWarning: NaN or Inf loss at batch 39. Skipping...\nWarning: NaN or Inf loss at batch 40. Skipping...\nWarning: NaN or Inf loss at batch 41. Skipping...\nWarning: NaN or Inf loss at batch 42. Skipping...\nWarning: NaN or Inf loss at batch 43. Skipping...\nWarning: NaN or Inf loss at batch 44. Skipping...\nWarning: NaN or Inf loss at batch 45. Skipping...\nWarning: NaN or Inf loss at batch 46. Skipping...\nWarning: NaN or Inf loss at batch 47. Skipping...\nWarning: NaN or Inf loss at batch 48. Skipping...\nWarning: NaN or Inf loss at batch 49. Skipping...\nWarning: NaN or Inf loss at batch 50. Skipping...\nWarning: NaN or Inf loss at batch 51. Skipping...\nWarning: NaN or Inf loss at batch 52. Skipping...\nWarning: NaN or Inf loss at batch 53. Skipping...\nWarning: NaN or Inf loss at batch 54. Skipping...\nWarning: NaN or Inf loss at batch 55. Skipping...\nWarning: NaN or Inf loss at batch 56. Skipping...\nWarning: NaN or Inf loss at batch 57. Skipping...\nWarning: NaN or Inf loss at batch 58. Skipping...\nWarning: NaN or Inf loss at batch 59. Skipping...\nWarning: NaN or Inf loss at batch 60. Skipping...\nWarning: NaN or Inf loss at batch 61. Skipping...\nWarning: NaN or Inf loss at batch 62. Skipping...\nWarning: NaN or Inf loss at batch 63. Skipping...\nWarning: NaN or Inf loss at batch 64. Skipping...\nWarning: NaN or Inf loss at batch 65. Skipping...\nWarning: NaN or Inf loss at batch 66. Skipping...\nWarning: NaN or Inf loss at batch 67. Skipping...\nWarning: NaN or Inf loss at batch 68. Skipping...\nWarning: NaN or Inf loss at batch 69. Skipping...\nWarning: NaN or Inf loss at batch 70. Skipping...\nWarning: NaN or Inf loss at batch 71. Skipping...\nWarning: NaN or Inf loss at batch 72. Skipping...\nWarning: NaN or Inf loss at batch 73. Skipping...\nWarning: NaN or Inf loss at batch 74. Skipping...\nWarning: NaN or Inf loss at batch 75. Skipping...\nWarning: NaN or Inf loss at batch 76. Skipping...\nWarning: NaN or Inf loss at batch 77. Skipping...\nWarning: NaN or Inf loss at batch 78. Skipping...\nWarning: NaN or Inf loss at batch 79. Skipping...\nWarning: NaN or Inf loss at batch 80. Skipping...\nWarning: NaN or Inf loss at batch 81. Skipping...\nWarning: NaN or Inf loss at batch 82. Skipping...\nWarning: NaN or Inf loss at batch 83. Skipping...\nWarning: NaN or Inf loss at batch 84. Skipping...\nWarning: NaN or Inf loss at batch 85. Skipping...\nWarning: NaN or Inf loss at batch 86. Skipping...\nWarning: NaN or Inf loss at batch 87. Skipping...\nWarning: NaN or Inf loss at batch 88. Skipping...\nWarning: NaN or Inf loss at batch 89. Skipping...\nWarning: NaN or Inf loss at batch 90. Skipping...\nWarning: NaN or Inf loss at batch 91. Skipping...\nWarning: NaN or Inf loss at batch 92. Skipping...\nWarning: NaN or Inf loss at batch 93. Skipping...\nWarning: NaN or Inf loss at batch 94. Skipping...\nWarning: NaN or Inf loss at batch 95. Skipping...\nWarning: NaN or Inf loss at batch 96. Skipping...\nWarning: NaN or Inf loss at batch 97. Skipping...\nWarning: NaN or Inf loss at batch 98. Skipping...\nWarning: NaN or Inf loss at batch 99. Skipping...\nWarning: NaN or Inf loss at batch 100. Skipping...\nWarning: NaN or Inf loss at batch 101. Skipping...\nWarning: NaN or Inf loss at batch 102. Skipping...\nWarning: NaN or Inf loss at batch 103. Skipping...\nWarning: NaN or Inf loss at batch 104. Skipping...\nWarning: NaN or Inf loss at batch 105. Skipping...\nWarning: NaN or Inf loss at batch 106. Skipping...\nWarning: NaN or Inf loss at batch 107. Skipping...\nWarning: NaN or Inf loss at batch 108. Skipping...\nWarning: NaN or Inf loss at batch 109. Skipping...\nWarning: NaN or Inf loss at batch 110. Skipping...\nWarning: NaN or Inf loss at batch 111. Skipping...\nWarning: NaN or Inf loss at batch 112. Skipping...\nWarning: NaN or Inf loss at batch 113. Skipping...\nWarning: NaN or Inf loss at batch 114. Skipping...\nWarning: NaN or Inf loss at batch 115. Skipping...\nWarning: NaN or Inf loss at batch 116. Skipping...\nWarning: NaN or Inf loss at batch 117. Skipping...\nWarning: NaN or Inf loss at batch 118. Skipping...\nWarning: NaN or Inf loss at batch 119. Skipping...\nWarning: NaN or Inf loss at batch 120. Skipping...\nWarning: NaN or Inf loss at batch 121. Skipping...\nWarning: NaN or Inf loss at batch 122. Skipping...\nWarning: NaN or Inf loss at batch 123. Skipping...\nWarning: NaN or Inf loss at batch 124. Skipping...\nWarning: NaN or Inf loss at batch 125. Skipping...\nWarning: NaN or Inf loss at batch 126. Skipping...\nWarning: NaN or Inf loss at batch 127. Skipping...\nWarning: NaN or Inf loss at batch 128. Skipping...\nWarning: NaN or Inf loss at batch 129. Skipping...\nWarning: NaN or Inf loss at batch 130. Skipping...\nWarning: NaN or Inf loss at batch 131. Skipping...\nWarning: NaN or Inf loss at batch 132. Skipping...\nWarning: NaN or Inf loss at batch 133. Skipping...\nWarning: NaN or Inf loss at batch 134. Skipping...\nWarning: NaN or Inf loss at batch 135. Skipping...\nWarning: NaN or Inf loss at batch 136. Skipping...\nWarning: NaN or Inf loss at batch 137. Skipping...\nWarning: NaN or Inf loss at batch 138. Skipping...\nWarning: NaN or Inf loss at batch 139. Skipping...\nWarning: NaN or Inf loss at batch 140. Skipping...\nWarning: NaN or Inf loss at batch 141. Skipping...\nWarning: NaN or Inf loss at batch 142. Skipping...\nWarning: NaN or Inf loss at batch 143. Skipping...\nWarning: NaN or Inf loss at batch 144. Skipping...\nWarning: NaN or Inf loss at batch 145. Skipping...\nWarning: NaN or Inf loss at batch 146. Skipping...\nWarning: NaN or Inf loss at batch 147. Skipping...\nWarning: NaN or Inf loss at batch 148. Skipping...\nWarning: NaN or Inf loss at batch 149. Skipping...\nWarning: NaN or Inf loss at batch 150. Skipping...\nWarning: NaN or Inf loss at batch 151. Skipping...\nWarning: NaN or Inf loss at batch 152. Skipping...\nWarning: NaN or Inf loss at batch 153. Skipping...\nWarning: NaN or Inf loss at batch 154. Skipping...\nWarning: NaN or Inf loss at batch 155. Skipping...\nWarning: NaN or Inf loss at batch 156. Skipping...\nWarning: NaN or Inf loss at batch 157. Skipping...\nWarning: NaN or Inf loss at batch 158. Skipping...\nWarning: NaN or Inf loss at batch 159. Skipping...\nWarning: NaN or Inf loss at batch 160. Skipping...\nWarning: NaN or Inf loss at batch 161. Skipping...\nWarning: NaN or Inf loss at batch 162. Skipping...\nWarning: NaN or Inf loss at batch 163. Skipping...\nWarning: NaN or Inf loss at batch 164. Skipping...\nWarning: NaN or Inf loss at batch 165. Skipping...\nWarning: NaN or Inf loss at batch 166. Skipping...\nWarning: NaN or Inf loss at batch 167. Skipping...\nWarning: NaN or Inf loss at batch 168. Skipping...\nWarning: NaN or Inf loss at batch 169. Skipping...\nWarning: NaN or Inf loss at batch 170. Skipping...\nWarning: NaN or Inf loss at batch 171. Skipping...\nWarning: NaN or Inf loss at batch 172. Skipping...\nWarning: NaN or Inf loss at batch 173. Skipping...\nWarning: NaN or Inf loss at batch 174. Skipping...\nWarning: NaN or Inf loss at batch 175. Skipping...\nWarning: NaN or Inf loss at batch 176. Skipping...\nWarning: NaN or Inf loss at batch 177. Skipping...\nWarning: NaN or Inf loss at batch 178. Skipping...\nWarning: NaN or Inf loss at batch 179. Skipping...\nWarning: NaN or Inf loss at batch 180. Skipping...\nWarning: NaN or Inf loss at batch 181. Skipping...\nWarning: NaN or Inf loss at batch 182. Skipping...\nWarning: NaN or Inf loss at batch 183. Skipping...\nWarning: NaN or Inf loss at batch 184. Skipping...\nWarning: NaN or Inf loss at batch 185. Skipping...\nWarning: NaN or Inf loss at batch 186. Skipping...\nWarning: NaN or Inf loss at batch 187. Skipping...\nWarning: NaN or Inf loss at batch 188. Skipping...\nWarning: NaN or Inf loss at batch 189. Skipping...\nWarning: NaN or Inf loss at batch 190. Skipping...\nWarning: NaN or Inf loss at batch 191. Skipping...\nWarning: NaN or Inf loss at batch 192. Skipping...\nWarning: NaN or Inf loss at batch 193. Skipping...\nWarning: NaN or Inf loss at batch 194. Skipping...\nWarning: NaN or Inf loss at batch 195. Skipping...\nWarning: NaN or Inf loss at batch 196. Skipping...\nWarning: NaN or Inf loss at batch 197. Skipping...\nWarning: NaN or Inf loss at batch 198. Skipping...\nWarning: NaN or Inf loss at batch 199. Skipping...\nWarning: NaN or Inf loss at batch 200. Skipping...\nWarning: NaN or Inf loss at batch 201. Skipping...\nWarning: NaN or Inf loss at batch 202. Skipping...\nWarning: NaN or Inf loss at batch 203. Skipping...\nWarning: NaN or Inf loss at batch 204. Skipping...\nWarning: NaN or Inf loss at batch 205. Skipping...\nWarning: NaN or Inf loss at batch 206. Skipping...\nWarning: NaN or Inf loss at batch 207. Skipping...\nWarning: NaN or Inf loss at batch 208. Skipping...\nWarning: NaN or Inf loss at batch 209. Skipping...\nWarning: NaN or Inf loss at batch 210. Skipping...\nWarning: NaN or Inf loss at batch 211. Skipping...\nWarning: NaN or Inf loss at batch 212. Skipping...\nWarning: NaN or Inf loss at batch 213. Skipping...\nWarning: NaN or Inf loss at batch 214. Skipping...\nWarning: NaN or Inf loss at batch 215. Skipping...\nWarning: NaN or Inf loss at batch 216. Skipping...\nWarning: NaN or Inf loss at batch 217. Skipping...\nWarning: NaN or Inf loss at batch 218. Skipping...\nWarning: NaN or Inf loss at batch 219. Skipping...\nWarning: NaN or Inf loss at batch 220. Skipping...\nWarning: NaN or Inf loss at batch 221. Skipping...\nWarning: NaN or Inf loss at batch 222. Skipping...\nWarning: NaN or Inf loss at batch 223. Skipping...\nWarning: NaN or Inf loss at batch 224. Skipping...\nWarning: NaN or Inf loss at batch 225. Skipping...\nWarning: NaN or Inf loss at batch 226. Skipping...\nWarning: NaN or Inf loss at batch 227. Skipping...\nWarning: NaN or Inf loss at batch 228. Skipping...\nWarning: NaN or Inf loss at batch 229. Skipping...\nWarning: NaN or Inf loss at batch 230. Skipping...\nWarning: NaN or Inf loss at batch 231. Skipping...\nWarning: NaN or Inf loss at batch 232. Skipping...\nWarning: NaN or Inf loss at batch 233. Skipping...\nWarning: NaN or Inf loss at batch 234. Skipping...\nWarning: NaN or Inf loss at batch 235. Skipping...\nWarning: NaN or Inf loss at batch 236. Skipping...\nWarning: NaN or Inf loss at batch 237. Skipping...\nWarning: NaN or Inf loss at batch 238. Skipping...\nWarning: NaN or Inf loss at batch 239. Skipping...\nWarning: NaN or Inf loss at batch 240. Skipping...\nWarning: NaN or Inf loss at batch 241. Skipping...\nWarning: NaN or Inf loss at batch 242. Skipping...\nWarning: NaN or Inf loss at batch 243. Skipping...\nWarning: NaN or Inf loss at batch 244. Skipping...\nWarning: NaN or Inf loss at batch 245. Skipping...\nWarning: NaN or Inf loss at batch 246. Skipping...\nWarning: NaN or Inf loss at batch 247. Skipping...\nWarning: NaN or Inf loss at batch 248. Skipping...\nWarning: NaN or Inf loss at batch 249. Skipping...\nWarning: NaN or Inf loss at batch 250. Skipping...\nWarning: NaN or Inf loss at batch 251. Skipping...\nWarning: NaN or Inf loss at batch 252. Skipping...\nWarning: NaN or Inf loss at batch 253. Skipping...\nWarning: NaN or Inf loss at batch 254. Skipping...\nWarning: NaN or Inf loss at batch 255. Skipping...\nEpoch 13/20, Loss: 0.0000\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nWarning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\nToken distribution (Batch 0): {41: 14}, Pred length: 14\nToken distribution (Batch 1): {41: 20}, Pred length: 20\nToken distribution (Batch 2): {41: 14}, Pred length: 14\nToken distribution (Batch 3): {41: 22}, Pred length: 22\nToken distribution (Batch 4): {41: 16}, Pred length: 16\nToken distribution (Batch 5): {41: 22}, Pred length: 22\nToken distribution (Batch 6): {41: 16}, Pred length: 16\nToken distribution (Batch 7): {41: 14}, Pred length: 14\nToken distribution (Batch 8): {41: 10}, Pred length: 10\nToken distribution (Batch 9): {41: 22}, Pred length: 22\nToken distribution (Batch 10): {41: 16}, Pred length: 16\nToken distribution (Batch 11): {41: 20}, Pred length: 20\nToken distribution (Batch 12): {41: 10}, Pred length: 10\nToken distribution (Batch 13): {41: 10}, Pred length: 10\nToken distribution (Batch 14): {41: 20}, Pred length: 20\nToken distribution (Batch 15): {41: 18}, Pred length: 18\nToken distribution (Batch 16): {41: 22}, Pred length: 22\nToken distribution (Batch 17): {41: 20}, Pred length: 20\nToken distribution (Batch 18): {41: 20}, Pred length: 20\nToken distribution (Batch 19): {41: 8}, Pred length: 8\nToken distribution (Batch 20): {41: 20}, Pred length: 20\nToken distribution (Batch 21): {41: 18}, Pred length: 18\nToken distribution (Batch 22): {41: 22}, Pred length: 22\nToken distribution (Batch 23): {41: 16}, Pred length: 16\nToken distribution (Batch 24): {41: 22}, Pred length: 22\nToken distribution (Batch 25): {41: 14}, Pred length: 14\nToken distribution (Batch 26): {41: 18}, Pred length: 18\nToken distribution (Batch 27): {41: 18}, Pred length: 18\nToken distribution (Batch 28): {41: 18}, Pred length: 18\nToken distribution (Batch 29): {41: 12}, Pred length: 12\nToken distribution (Batch 30): {41: 10}, Pred length: 10\nToken distribution (Batch 31): {41: 8}, Pred length: 8\nValidation Loss: nan\nValidation Predictions: ['O', 'O', 'O', 'O', 'O']\nGround Truth: ['CREMjBq', 'Vn*PagPAi4', 'X4EfJtW', 'u*z1C*2qKyg', 'LQADvt8*']\nCurrent Learning Rate: 2.7500000000000004e-06\nEpoch 14, Filtered data size: 10240, Sample labels: ['qV8ib8H', 'MdI8', '1XS0DRdJ']\nTrain size: 8192, Val size: 2048\nWarning: NaN or Inf loss at batch 0. Skipping...\nWarning: NaN or Inf loss at batch 1. Skipping...\nWarning: NaN or Inf loss at batch 2. Skipping...\nWarning: NaN or Inf loss at batch 3. Skipping...\nWarning: NaN or Inf loss at batch 4. Skipping...\nWarning: NaN or Inf loss at batch 5. Skipping...\nWarning: NaN or Inf loss at batch 6. Skipping...\nWarning: NaN or Inf loss at batch 7. Skipping...\nWarning: NaN or Inf loss at batch 8. Skipping...\nWarning: NaN or Inf loss at batch 9. Skipping...\nWarning: NaN or Inf loss at batch 10. Skipping...\nWarning: NaN or Inf loss at batch 11. Skipping...\nWarning: NaN or Inf loss at batch 12. Skipping...\nWarning: NaN or Inf loss at batch 13. Skipping...\nWarning: NaN or Inf loss at batch 14. Skipping...\nWarning: NaN or Inf loss at batch 15. Skipping...\nWarning: NaN or Inf loss at batch 16. Skipping...\nWarning: NaN or Inf loss at batch 17. Skipping...\nWarning: NaN or Inf loss at batch 18. Skipping...\nWarning: NaN or Inf loss at batch 19. Skipping...\nWarning: NaN or Inf loss at batch 20. Skipping...\nWarning: NaN or Inf loss at batch 21. Skipping...\nWarning: NaN or Inf loss at batch 22. Skipping...\nWarning: NaN or Inf loss at batch 23. Skipping...\nWarning: NaN or Inf loss at batch 24. Skipping...\nWarning: NaN or Inf loss at batch 25. Skipping...\nWarning: NaN or Inf loss at batch 26. Skipping...\nWarning: NaN or Inf loss at batch 27. Skipping...\nWarning: NaN or Inf loss at batch 28. Skipping...\nWarning: NaN or Inf loss at batch 29. Skipping...\nWarning: NaN or Inf loss at batch 30. Skipping...\nWarning: NaN or Inf loss at batch 31. Skipping...\nWarning: NaN or Inf loss at batch 32. Skipping...\nWarning: NaN or Inf loss at batch 33. Skipping...\nWarning: NaN or Inf loss at batch 34. Skipping...\nWarning: NaN or Inf loss at batch 35. Skipping...\nWarning: NaN or Inf loss at batch 36. Skipping...\nWarning: NaN or Inf loss at batch 37. Skipping...\nWarning: NaN or Inf loss at batch 38. Skipping...\nWarning: NaN or Inf loss at batch 39. Skipping...\nWarning: NaN or Inf loss at batch 40. Skipping...\nWarning: NaN or Inf loss at batch 41. Skipping...\nWarning: NaN or Inf loss at batch 42. Skipping...\nWarning: NaN or Inf loss at batch 43. Skipping...\nWarning: NaN or Inf loss at batch 44. Skipping...\nWarning: NaN or Inf loss at batch 45. Skipping...\nWarning: NaN or Inf loss at batch 46. Skipping...\nWarning: NaN or Inf loss at batch 47. Skipping...\nWarning: NaN or Inf loss at batch 48. Skipping...\nWarning: NaN or Inf loss at batch 49. Skipping...\nWarning: NaN or Inf loss at batch 50. Skipping...\nWarning: NaN or Inf loss at batch 51. Skipping...\nWarning: NaN or Inf loss at batch 52. Skipping...\nWarning: NaN or Inf loss at batch 53. Skipping...\nWarning: NaN or Inf loss at batch 54. Skipping...\nWarning: NaN or Inf loss at batch 55. Skipping...\nWarning: NaN or Inf loss at batch 56. Skipping...\nWarning: NaN or Inf loss at batch 57. Skipping...\nWarning: NaN or Inf loss at batch 58. Skipping...\nWarning: NaN or Inf loss at batch 59. Skipping...\nWarning: NaN or Inf loss at batch 60. Skipping...\nWarning: NaN or Inf loss at batch 61. Skipping...\nWarning: NaN or Inf loss at batch 62. Skipping...\nWarning: NaN or Inf loss at batch 63. Skipping...\nWarning: NaN or Inf loss at batch 64. Skipping...\nWarning: NaN or Inf loss at batch 65. Skipping...\nWarning: NaN or Inf loss at batch 66. Skipping...\nWarning: NaN or Inf loss at batch 67. Skipping...\nWarning: NaN or Inf loss at batch 68. Skipping...\nWarning: NaN or Inf loss at batch 69. Skipping...\nWarning: NaN or Inf loss at batch 70. Skipping...\nWarning: NaN or Inf loss at batch 71. Skipping...\nWarning: NaN or Inf loss at batch 72. Skipping...\nWarning: NaN or Inf loss at batch 73. Skipping...\nWarning: NaN or Inf loss at batch 74. Skipping...\nWarning: NaN or Inf loss at batch 75. Skipping...\nWarning: NaN or Inf loss at batch 76. Skipping...\nWarning: NaN or Inf loss at batch 77. Skipping...\nWarning: NaN or Inf loss at batch 78. Skipping...\nWarning: NaN or Inf loss at batch 79. Skipping...\nWarning: NaN or Inf loss at batch 80. Skipping...\nWarning: NaN or Inf loss at batch 81. Skipping...\nWarning: NaN or Inf loss at batch 82. Skipping...\nWarning: NaN or Inf loss at batch 83. Skipping...\nWarning: NaN or Inf loss at batch 84. Skipping...\nWarning: NaN or Inf loss at batch 85. Skipping...\nWarning: NaN or Inf loss at batch 86. Skipping...\nWarning: NaN or Inf loss at batch 87. Skipping...\nWarning: NaN or Inf loss at batch 88. Skipping...\nWarning: NaN or Inf loss at batch 89. Skipping...\nWarning: NaN or Inf loss at batch 90. Skipping...\nWarning: NaN or Inf loss at batch 91. Skipping...\nWarning: NaN or Inf loss at batch 92. Skipping...\nWarning: NaN or Inf loss at batch 93. Skipping...\nWarning: NaN or Inf loss at batch 94. Skipping...\nWarning: NaN or Inf loss at batch 95. Skipping...\nWarning: NaN or Inf loss at batch 96. Skipping...\nWarning: NaN or Inf loss at batch 97. Skipping...\nWarning: NaN or Inf loss at batch 98. Skipping...\nWarning: NaN or Inf loss at batch 99. Skipping...\nWarning: NaN or Inf loss at batch 100. Skipping...\nWarning: NaN or Inf loss at batch 101. Skipping...\nWarning: NaN or Inf loss at batch 102. Skipping...\nWarning: NaN or Inf loss at batch 103. Skipping...\nWarning: NaN or Inf loss at batch 104. Skipping...\nWarning: NaN or Inf loss at batch 105. Skipping...\nWarning: NaN or Inf loss at batch 106. Skipping...\nWarning: NaN or Inf loss at batch 107. Skipping...\nWarning: NaN or Inf loss at batch 108. Skipping...\nWarning: NaN or Inf loss at batch 109. Skipping...\nWarning: NaN or Inf loss at batch 110. Skipping...\nWarning: NaN or Inf loss at batch 111. Skipping...\nWarning: NaN or Inf loss at batch 112. Skipping...\nWarning: NaN or Inf loss at batch 113. Skipping...\nWarning: NaN or Inf loss at batch 114. Skipping...\nWarning: NaN or Inf loss at batch 115. Skipping...\nWarning: NaN or Inf loss at batch 116. Skipping...\nWarning: NaN or Inf loss at batch 117. Skipping...\nWarning: NaN or Inf loss at batch 118. Skipping...\nWarning: NaN or Inf loss at batch 119. Skipping...\nWarning: NaN or Inf loss at batch 120. Skipping...\nWarning: NaN or Inf loss at batch 121. Skipping...\nWarning: NaN or Inf loss at batch 122. Skipping...\nWarning: NaN or Inf loss at batch 123. Skipping...\nWarning: NaN or Inf loss at batch 124. Skipping...\nWarning: NaN or Inf loss at batch 125. Skipping...\nWarning: NaN or Inf loss at batch 126. Skipping...\nWarning: NaN or Inf loss at batch 127. Skipping...\nWarning: NaN or Inf loss at batch 128. Skipping...\nWarning: NaN or Inf loss at batch 129. Skipping...\nWarning: NaN or Inf loss at batch 130. Skipping...\nWarning: NaN or Inf loss at batch 131. Skipping...\nWarning: NaN or Inf loss at batch 132. Skipping...\nWarning: NaN or Inf loss at batch 133. Skipping...\nWarning: NaN or Inf loss at batch 134. Skipping...\nWarning: NaN or Inf loss at batch 135. Skipping...\nWarning: NaN or Inf loss at batch 136. Skipping...\nWarning: NaN or Inf loss at batch 137. Skipping...\nWarning: NaN or Inf loss at batch 138. Skipping...\nWarning: NaN or Inf loss at batch 139. Skipping...\nWarning: NaN or Inf loss at batch 140. Skipping...\nWarning: NaN or Inf loss at batch 141. Skipping...\nWarning: NaN or Inf loss at batch 142. Skipping...\nWarning: NaN or Inf loss at batch 143. Skipping...\nWarning: NaN or Inf loss at batch 144. Skipping...\nWarning: NaN or Inf loss at batch 145. Skipping...\nWarning: NaN or Inf loss at batch 146. Skipping...\nWarning: NaN or Inf loss at batch 147. Skipping...\nWarning: NaN or Inf loss at batch 148. Skipping...\nWarning: NaN or Inf loss at batch 149. Skipping...\nWarning: NaN or Inf loss at batch 150. Skipping...\nWarning: NaN or Inf loss at batch 151. Skipping...\nWarning: NaN or Inf loss at batch 152. Skipping...\nWarning: NaN or Inf loss at batch 153. Skipping...\nWarning: NaN or Inf loss at batch 154. Skipping...\nWarning: NaN or Inf loss at batch 155. Skipping...\nWarning: NaN or Inf loss at batch 156. Skipping...\nWarning: NaN or Inf loss at batch 157. Skipping...\nWarning: NaN or Inf loss at batch 158. Skipping...\nWarning: NaN or Inf loss at batch 159. Skipping...\nWarning: NaN or Inf loss at batch 160. Skipping...\nWarning: NaN or Inf loss at batch 161. Skipping...\nWarning: NaN or Inf loss at batch 162. Skipping...\nWarning: NaN or Inf loss at batch 163. Skipping...\nWarning: NaN or Inf loss at batch 164. Skipping...\nWarning: NaN or Inf loss at batch 165. Skipping...\nWarning: NaN or Inf loss at batch 166. Skipping...\nWarning: NaN or Inf loss at batch 167. Skipping...\nWarning: NaN or Inf loss at batch 168. Skipping...\nWarning: NaN or Inf loss at batch 169. Skipping...\nWarning: NaN or Inf loss at batch 170. Skipping...\nWarning: NaN or Inf loss at batch 171. Skipping...\nWarning: NaN or Inf loss at batch 172. Skipping...\nWarning: NaN or Inf loss at batch 173. Skipping...\nWarning: NaN or Inf loss at batch 174. Skipping...\nWarning: NaN or Inf loss at batch 175. Skipping...\nWarning: NaN or Inf loss at batch 176. Skipping...\nWarning: NaN or Inf loss at batch 177. Skipping...\nWarning: NaN or Inf loss at batch 178. Skipping...\nWarning: NaN or Inf loss at batch 179. Skipping...\nWarning: NaN or Inf loss at batch 180. Skipping...\nWarning: NaN or Inf loss at batch 181. Skipping...\nWarning: NaN or Inf loss at batch 182. Skipping...\nWarning: NaN or Inf loss at batch 183. Skipping...\nWarning: NaN or Inf loss at batch 184. Skipping...\nWarning: NaN or Inf loss at batch 185. Skipping...\nWarning: NaN or Inf loss at batch 186. Skipping...\nWarning: NaN or Inf loss at batch 187. Skipping...\nWarning: NaN or Inf loss at batch 188. Skipping...\nWarning: NaN or Inf loss at batch 189. Skipping...\nWarning: NaN or Inf loss at batch 190. Skipping...\nWarning: NaN or Inf loss at batch 191. Skipping...\nWarning: NaN or Inf loss at batch 192. Skipping...\nWarning: NaN or Inf loss at batch 193. Skipping...\nWarning: NaN or Inf loss at batch 194. Skipping...\nWarning: NaN or Inf loss at batch 195. Skipping...\nWarning: NaN or Inf loss at batch 196. Skipping...\nWarning: NaN or Inf loss at batch 197. Skipping...\nWarning: NaN or Inf loss at batch 198. Skipping...\nWarning: NaN or Inf loss at batch 199. Skipping...\nWarning: NaN or Inf loss at batch 200. Skipping...\nWarning: NaN or Inf loss at batch 201. Skipping...\nWarning: NaN or Inf loss at batch 202. Skipping...\nWarning: NaN or Inf loss at batch 203. Skipping...\nWarning: NaN or Inf loss at batch 204. Skipping...\nWarning: NaN or Inf loss at batch 205. Skipping...\nWarning: NaN or Inf loss at batch 206. Skipping...\nWarning: NaN or Inf loss at batch 207. Skipping...\nWarning: NaN or Inf loss at batch 208. Skipping...\nWarning: NaN or Inf loss at batch 209. Skipping...\nWarning: NaN or Inf loss at batch 210. Skipping...\nWarning: NaN or Inf loss at batch 211. Skipping...\nWarning: NaN or Inf loss at batch 212. Skipping...\nWarning: NaN or Inf loss at batch 213. Skipping...\nWarning: NaN or Inf loss at batch 214. Skipping...\nWarning: NaN or Inf loss at batch 215. Skipping...\nWarning: NaN or Inf loss at batch 216. Skipping...\nWarning: NaN or Inf loss at batch 217. Skipping...\nWarning: NaN or Inf loss at batch 218. Skipping...\nWarning: NaN or Inf loss at batch 219. Skipping...\nWarning: NaN or Inf loss at batch 220. Skipping...\nWarning: NaN or Inf loss at batch 221. Skipping...\nWarning: NaN or Inf loss at batch 222. Skipping...\nWarning: NaN or Inf loss at batch 223. Skipping...\nWarning: NaN or Inf loss at batch 224. Skipping...\nWarning: NaN or Inf loss at batch 225. Skipping...\nWarning: NaN or Inf loss at batch 226. Skipping...\nWarning: NaN or Inf loss at batch 227. Skipping...\nWarning: NaN or Inf loss at batch 228. Skipping...\nWarning: NaN or Inf loss at batch 229. Skipping...\nWarning: NaN or Inf loss at batch 230. Skipping...\nWarning: NaN or Inf loss at batch 231. Skipping...\nWarning: NaN or Inf loss at batch 232. Skipping...\nWarning: NaN or Inf loss at batch 233. Skipping...\nWarning: NaN or Inf loss at batch 234. Skipping...\nWarning: NaN or Inf loss at batch 235. Skipping...\nWarning: NaN or Inf loss at batch 236. Skipping...\nWarning: NaN or Inf loss at batch 237. Skipping...\nWarning: NaN or Inf loss at batch 238. Skipping...\nWarning: NaN or Inf loss at batch 239. Skipping...\nWarning: NaN or Inf loss at batch 240. Skipping...\nWarning: NaN or Inf loss at batch 241. Skipping...\nWarning: NaN or Inf loss at batch 242. Skipping...\nWarning: NaN or Inf loss at batch 243. Skipping...\nWarning: NaN or Inf loss at batch 244. Skipping...\nWarning: NaN or Inf loss at batch 245. Skipping...\nWarning: NaN or Inf loss at batch 246. Skipping...\nWarning: NaN or Inf loss at batch 247. Skipping...\nWarning: NaN or Inf loss at batch 248. Skipping...\nWarning: NaN or Inf loss at batch 249. Skipping...\nWarning: NaN or Inf loss at batch 250. Skipping...\nWarning: NaN or Inf loss at batch 251. Skipping...\nWarning: NaN or Inf loss at batch 252. Skipping...\nWarning: NaN or Inf loss at batch 253. Skipping...\nWarning: NaN or Inf loss at batch 254. Skipping...\nWarning: NaN or Inf loss at batch 255. Skipping...\nEpoch 14/20, Loss: 0.0000\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nWarning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\nToken distribution (Batch 0): {41: 22}, Pred length: 22\nToken distribution (Batch 1): {41: 22}, Pred length: 22\nToken distribution (Batch 2): {41: 18}, Pred length: 18\nToken distribution (Batch 3): {41: 18}, Pred length: 18\nToken distribution (Batch 4): {41: 8}, Pred length: 8\nToken distribution (Batch 5): {41: 8}, Pred length: 8\nToken distribution (Batch 6): {41: 20}, Pred length: 20\nToken distribution (Batch 7): {41: 14}, Pred length: 14\nToken distribution (Batch 8): {41: 14}, Pred length: 14\nToken distribution (Batch 9): {41: 20}, Pred length: 20\nToken distribution (Batch 10): {41: 12}, Pred length: 12\nToken distribution (Batch 11): {41: 10}, Pred length: 10\nToken distribution (Batch 12): {41: 20}, Pred length: 20\nToken distribution (Batch 13): {41: 14}, Pred length: 14\nToken distribution (Batch 14): {41: 12}, Pred length: 12\nToken distribution (Batch 15): {41: 22}, Pred length: 22\nToken distribution (Batch 16): {41: 24}, Pred length: 24\nToken distribution (Batch 17): {41: 20}, Pred length: 20\nToken distribution (Batch 18): {41: 24}, Pred length: 24\nToken distribution (Batch 19): {41: 20}, Pred length: 20\nToken distribution (Batch 20): {41: 24}, Pred length: 24\nToken distribution (Batch 21): {41: 22}, Pred length: 22\nToken distribution (Batch 22): {41: 20}, Pred length: 20\nToken distribution (Batch 23): {41: 8}, Pred length: 8\nToken distribution (Batch 24): {41: 10}, Pred length: 10\nToken distribution (Batch 25): {41: 12}, Pred length: 12\nToken distribution (Batch 26): {41: 14}, Pred length: 14\nToken distribution (Batch 27): {41: 22}, Pred length: 22\nToken distribution (Batch 28): {41: 24}, Pred length: 24\nToken distribution (Batch 29): {41: 12}, Pred length: 12\nToken distribution (Batch 30): {41: 24}, Pred length: 24\nToken distribution (Batch 31): {41: 8}, Pred length: 8\nValidation Loss: nan\nValidation Predictions: ['O', 'O', 'O', 'O', 'O']\nGround Truth: ['kKA*fD6SrkP', 'ZVDvNM-taq6', '4lUjx*ngZ', 'SkrOlyf5B', 'sQIR']\nCurrent Learning Rate: 2.7500000000000004e-06\nEpoch 15, Filtered data size: 10240, Sample labels: ['qV8ib8H', 'MdI8', '1XS0DRdJ']\nTrain size: 8192, Val size: 2048\nWarning: NaN or Inf loss at batch 0. Skipping...\nWarning: NaN or Inf loss at batch 1. Skipping...\nWarning: NaN or Inf loss at batch 2. Skipping...\nWarning: NaN or Inf loss at batch 3. Skipping...\nWarning: NaN or Inf loss at batch 4. Skipping...\nWarning: NaN or Inf loss at batch 5. Skipping...\nWarning: NaN or Inf loss at batch 6. Skipping...\nWarning: NaN or Inf loss at batch 7. Skipping...\nWarning: NaN or Inf loss at batch 8. Skipping...\nWarning: NaN or Inf loss at batch 9. Skipping...\nWarning: NaN or Inf loss at batch 10. Skipping...\nWarning: NaN or Inf loss at batch 11. Skipping...\nWarning: NaN or Inf loss at batch 12. Skipping...\nWarning: NaN or Inf loss at batch 13. Skipping...\nWarning: NaN or Inf loss at batch 14. Skipping...\nWarning: NaN or Inf loss at batch 15. Skipping...\nWarning: NaN or Inf loss at batch 16. Skipping...\nWarning: NaN or Inf loss at batch 17. Skipping...\nWarning: NaN or Inf loss at batch 18. Skipping...\nWarning: NaN or Inf loss at batch 19. Skipping...\nWarning: NaN or Inf loss at batch 20. Skipping...\nWarning: NaN or Inf loss at batch 21. Skipping...\nWarning: NaN or Inf loss at batch 22. Skipping...\nWarning: NaN or Inf loss at batch 23. Skipping...\nWarning: NaN or Inf loss at batch 24. Skipping...\nWarning: NaN or Inf loss at batch 25. Skipping...\nWarning: NaN or Inf loss at batch 26. Skipping...\nWarning: NaN or Inf loss at batch 27. Skipping...\nWarning: NaN or Inf loss at batch 28. Skipping...\nWarning: NaN or Inf loss at batch 29. Skipping...\nWarning: NaN or Inf loss at batch 30. Skipping...\nWarning: NaN or Inf loss at batch 31. Skipping...\nWarning: NaN or Inf loss at batch 32. Skipping...\nWarning: NaN or Inf loss at batch 33. Skipping...\nWarning: NaN or Inf loss at batch 34. Skipping...\nWarning: NaN or Inf loss at batch 35. Skipping...\nWarning: NaN or Inf loss at batch 36. Skipping...\nWarning: NaN or Inf loss at batch 37. Skipping...\nWarning: NaN or Inf loss at batch 38. Skipping...\nWarning: NaN or Inf loss at batch 39. Skipping...\nWarning: NaN or Inf loss at batch 40. Skipping...\nWarning: NaN or Inf loss at batch 41. Skipping...\nWarning: NaN or Inf loss at batch 42. Skipping...\nWarning: NaN or Inf loss at batch 43. Skipping...\nWarning: NaN or Inf loss at batch 44. Skipping...\nWarning: NaN or Inf loss at batch 45. Skipping...\nWarning: NaN or Inf loss at batch 46. Skipping...\nWarning: NaN or Inf loss at batch 47. Skipping...\nWarning: NaN or Inf loss at batch 48. Skipping...\nWarning: NaN or Inf loss at batch 49. Skipping...\nWarning: NaN or Inf loss at batch 50. Skipping...\nWarning: NaN or Inf loss at batch 51. Skipping...\nWarning: NaN or Inf loss at batch 52. Skipping...\nWarning: NaN or Inf loss at batch 53. Skipping...\nWarning: NaN or Inf loss at batch 54. Skipping...\nWarning: NaN or Inf loss at batch 55. Skipping...\nWarning: NaN or Inf loss at batch 56. Skipping...\nWarning: NaN or Inf loss at batch 57. Skipping...\nWarning: NaN or Inf loss at batch 58. Skipping...\nWarning: NaN or Inf loss at batch 59. Skipping...\nWarning: NaN or Inf loss at batch 60. Skipping...\nWarning: NaN or Inf loss at batch 61. Skipping...\nWarning: NaN or Inf loss at batch 62. Skipping...\nWarning: NaN or Inf loss at batch 63. Skipping...\nWarning: NaN or Inf loss at batch 64. Skipping...\nWarning: NaN or Inf loss at batch 65. Skipping...\nWarning: NaN or Inf loss at batch 66. Skipping...\nWarning: NaN or Inf loss at batch 67. Skipping...\nWarning: NaN or Inf loss at batch 68. Skipping...\nWarning: NaN or Inf loss at batch 69. Skipping...\nWarning: NaN or Inf loss at batch 70. Skipping...\nWarning: NaN or Inf loss at batch 71. Skipping...\nWarning: NaN or Inf loss at batch 72. Skipping...\nWarning: NaN or Inf loss at batch 73. Skipping...\nWarning: NaN or Inf loss at batch 74. Skipping...\nWarning: NaN or Inf loss at batch 75. Skipping...\nWarning: NaN or Inf loss at batch 76. Skipping...\nWarning: NaN or Inf loss at batch 77. Skipping...\nWarning: NaN or Inf loss at batch 78. Skipping...\nWarning: NaN or Inf loss at batch 79. Skipping...\nWarning: NaN or Inf loss at batch 80. Skipping...\nWarning: NaN or Inf loss at batch 81. Skipping...\nWarning: NaN or Inf loss at batch 82. Skipping...\nWarning: NaN or Inf loss at batch 83. Skipping...\nWarning: NaN or Inf loss at batch 84. Skipping...\nWarning: NaN or Inf loss at batch 85. Skipping...\nWarning: NaN or Inf loss at batch 86. Skipping...\nWarning: NaN or Inf loss at batch 87. Skipping...\nWarning: NaN or Inf loss at batch 88. Skipping...\nWarning: NaN or Inf loss at batch 89. Skipping...\nWarning: NaN or Inf loss at batch 90. Skipping...\nWarning: NaN or Inf loss at batch 91. Skipping...\nWarning: NaN or Inf loss at batch 92. Skipping...\nWarning: NaN or Inf loss at batch 93. Skipping...\nWarning: NaN or Inf loss at batch 94. Skipping...\nWarning: NaN or Inf loss at batch 95. Skipping...\nWarning: NaN or Inf loss at batch 96. Skipping...\nWarning: NaN or Inf loss at batch 97. Skipping...\nWarning: NaN or Inf loss at batch 98. Skipping...\nWarning: NaN or Inf loss at batch 99. Skipping...\nWarning: NaN or Inf loss at batch 100. Skipping...\nWarning: NaN or Inf loss at batch 101. Skipping...\nWarning: NaN or Inf loss at batch 102. Skipping...\nWarning: NaN or Inf loss at batch 103. Skipping...\nWarning: NaN or Inf loss at batch 104. Skipping...\nWarning: NaN or Inf loss at batch 105. Skipping...\nWarning: NaN or Inf loss at batch 106. Skipping...\nWarning: NaN or Inf loss at batch 107. Skipping...\nWarning: NaN or Inf loss at batch 108. Skipping...\nWarning: NaN or Inf loss at batch 109. Skipping...\nWarning: NaN or Inf loss at batch 110. Skipping...\nWarning: NaN or Inf loss at batch 111. Skipping...\nWarning: NaN or Inf loss at batch 112. Skipping...\nWarning: NaN or Inf loss at batch 113. Skipping...\nWarning: NaN or Inf loss at batch 114. Skipping...\nWarning: NaN or Inf loss at batch 115. Skipping...\nWarning: NaN or Inf loss at batch 116. Skipping...\nWarning: NaN or Inf loss at batch 117. Skipping...\nWarning: NaN or Inf loss at batch 118. Skipping...\nWarning: NaN or Inf loss at batch 119. Skipping...\nWarning: NaN or Inf loss at batch 120. Skipping...\nWarning: NaN or Inf loss at batch 121. Skipping...\nWarning: NaN or Inf loss at batch 122. Skipping...\nWarning: NaN or Inf loss at batch 123. Skipping...\nWarning: NaN or Inf loss at batch 124. Skipping...\nWarning: NaN or Inf loss at batch 125. Skipping...\nWarning: NaN or Inf loss at batch 126. Skipping...\nWarning: NaN or Inf loss at batch 127. Skipping...\nWarning: NaN or Inf loss at batch 128. Skipping...\nWarning: NaN or Inf loss at batch 129. Skipping...\nWarning: NaN or Inf loss at batch 130. Skipping...\nWarning: NaN or Inf loss at batch 131. Skipping...\nWarning: NaN or Inf loss at batch 132. Skipping...\nWarning: NaN or Inf loss at batch 133. Skipping...\nWarning: NaN or Inf loss at batch 134. Skipping...\nWarning: NaN or Inf loss at batch 135. Skipping...\nWarning: NaN or Inf loss at batch 136. Skipping...\nWarning: NaN or Inf loss at batch 137. Skipping...\nWarning: NaN or Inf loss at batch 138. Skipping...\nWarning: NaN or Inf loss at batch 139. Skipping...\nWarning: NaN or Inf loss at batch 140. Skipping...\nWarning: NaN or Inf loss at batch 141. Skipping...\nWarning: NaN or Inf loss at batch 142. Skipping...\nWarning: NaN or Inf loss at batch 143. Skipping...\nWarning: NaN or Inf loss at batch 144. Skipping...\nWarning: NaN or Inf loss at batch 145. Skipping...\nWarning: NaN or Inf loss at batch 146. Skipping...\nWarning: NaN or Inf loss at batch 147. Skipping...\nWarning: NaN or Inf loss at batch 148. Skipping...\nWarning: NaN or Inf loss at batch 149. Skipping...\nWarning: NaN or Inf loss at batch 150. Skipping...\nWarning: NaN or Inf loss at batch 151. Skipping...\nWarning: NaN or Inf loss at batch 152. Skipping...\nWarning: NaN or Inf loss at batch 153. Skipping...\nWarning: NaN or Inf loss at batch 154. Skipping...\nWarning: NaN or Inf loss at batch 155. Skipping...\nWarning: NaN or Inf loss at batch 156. Skipping...\nWarning: NaN or Inf loss at batch 157. Skipping...\nWarning: NaN or Inf loss at batch 158. Skipping...\nWarning: NaN or Inf loss at batch 159. Skipping...\nWarning: NaN or Inf loss at batch 160. Skipping...\nWarning: NaN or Inf loss at batch 161. Skipping...\nWarning: NaN or Inf loss at batch 162. Skipping...\nWarning: NaN or Inf loss at batch 163. Skipping...\nWarning: NaN or Inf loss at batch 164. Skipping...\nWarning: NaN or Inf loss at batch 165. Skipping...\nWarning: NaN or Inf loss at batch 166. Skipping...\nWarning: NaN or Inf loss at batch 167. Skipping...\nWarning: NaN or Inf loss at batch 168. Skipping...\nWarning: NaN or Inf loss at batch 169. Skipping...\nWarning: NaN or Inf loss at batch 170. Skipping...\nWarning: NaN or Inf loss at batch 171. Skipping...\nWarning: NaN or Inf loss at batch 172. Skipping...\nWarning: NaN or Inf loss at batch 173. Skipping...\nWarning: NaN or Inf loss at batch 174. Skipping...\nWarning: NaN or Inf loss at batch 175. Skipping...\nWarning: NaN or Inf loss at batch 176. Skipping...\nWarning: NaN or Inf loss at batch 177. Skipping...\nWarning: NaN or Inf loss at batch 178. Skipping...\nWarning: NaN or Inf loss at batch 179. Skipping...\nWarning: NaN or Inf loss at batch 180. Skipping...\nWarning: NaN or Inf loss at batch 181. Skipping...\nWarning: NaN or Inf loss at batch 182. Skipping...\nWarning: NaN or Inf loss at batch 183. Skipping...\nWarning: NaN or Inf loss at batch 184. Skipping...\nWarning: NaN or Inf loss at batch 185. Skipping...\nWarning: NaN or Inf loss at batch 186. Skipping...\nWarning: NaN or Inf loss at batch 187. Skipping...\nWarning: NaN or Inf loss at batch 188. Skipping...\nWarning: NaN or Inf loss at batch 189. Skipping...\nWarning: NaN or Inf loss at batch 190. Skipping...\nWarning: NaN or Inf loss at batch 191. Skipping...\nWarning: NaN or Inf loss at batch 192. Skipping...\nWarning: NaN or Inf loss at batch 193. Skipping...\nWarning: NaN or Inf loss at batch 194. Skipping...\nWarning: NaN or Inf loss at batch 195. Skipping...\nWarning: NaN or Inf loss at batch 196. Skipping...\nWarning: NaN or Inf loss at batch 197. Skipping...\nWarning: NaN or Inf loss at batch 198. Skipping...\nWarning: NaN or Inf loss at batch 199. Skipping...\nWarning: NaN or Inf loss at batch 200. Skipping...\nWarning: NaN or Inf loss at batch 201. Skipping...\nWarning: NaN or Inf loss at batch 202. Skipping...\nWarning: NaN or Inf loss at batch 203. Skipping...\nWarning: NaN or Inf loss at batch 204. Skipping...\nWarning: NaN or Inf loss at batch 205. Skipping...\nWarning: NaN or Inf loss at batch 206. Skipping...\nWarning: NaN or Inf loss at batch 207. Skipping...\nWarning: NaN or Inf loss at batch 208. Skipping...\nWarning: NaN or Inf loss at batch 209. Skipping...\nWarning: NaN or Inf loss at batch 210. Skipping...\nWarning: NaN or Inf loss at batch 211. Skipping...\nWarning: NaN or Inf loss at batch 212. Skipping...\nWarning: NaN or Inf loss at batch 213. Skipping...\nWarning: NaN or Inf loss at batch 214. Skipping...\nWarning: NaN or Inf loss at batch 215. Skipping...\nWarning: NaN or Inf loss at batch 216. Skipping...\nWarning: NaN or Inf loss at batch 217. Skipping...\nWarning: NaN or Inf loss at batch 218. Skipping...\nWarning: NaN or Inf loss at batch 219. Skipping...\nWarning: NaN or Inf loss at batch 220. Skipping...\nWarning: NaN or Inf loss at batch 221. Skipping...\nWarning: NaN or Inf loss at batch 222. Skipping...\nWarning: NaN or Inf loss at batch 223. Skipping...\nWarning: NaN or Inf loss at batch 224. Skipping...\nWarning: NaN or Inf loss at batch 225. Skipping...\nWarning: NaN or Inf loss at batch 226. Skipping...\nWarning: NaN or Inf loss at batch 227. Skipping...\nWarning: NaN or Inf loss at batch 228. Skipping...\nWarning: NaN or Inf loss at batch 229. Skipping...\nWarning: NaN or Inf loss at batch 230. Skipping...\nWarning: NaN or Inf loss at batch 231. Skipping...\nWarning: NaN or Inf loss at batch 232. Skipping...\nWarning: NaN or Inf loss at batch 233. Skipping...\nWarning: NaN or Inf loss at batch 234. Skipping...\nWarning: NaN or Inf loss at batch 235. Skipping...\nWarning: NaN or Inf loss at batch 236. Skipping...\nWarning: NaN or Inf loss at batch 237. Skipping...\nWarning: NaN or Inf loss at batch 238. Skipping...\nWarning: NaN or Inf loss at batch 239. Skipping...\nWarning: NaN or Inf loss at batch 240. Skipping...\nWarning: NaN or Inf loss at batch 241. Skipping...\nWarning: NaN or Inf loss at batch 242. Skipping...\nWarning: NaN or Inf loss at batch 243. Skipping...\nWarning: NaN or Inf loss at batch 244. Skipping...\nWarning: NaN or Inf loss at batch 245. Skipping...\nWarning: NaN or Inf loss at batch 246. Skipping...\nWarning: NaN or Inf loss at batch 247. Skipping...\nWarning: NaN or Inf loss at batch 248. Skipping...\nWarning: NaN or Inf loss at batch 249. Skipping...\nWarning: NaN or Inf loss at batch 250. Skipping...\nWarning: NaN or Inf loss at batch 251. Skipping...\nWarning: NaN or Inf loss at batch 252. Skipping...\nWarning: NaN or Inf loss at batch 253. Skipping...\nWarning: NaN or Inf loss at batch 254. Skipping...\nWarning: NaN or Inf loss at batch 255. Skipping...\nEpoch 15/20, Loss: 0.0000\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nWarning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\nToken distribution (Batch 0): {41: 10}, Pred length: 10\nToken distribution (Batch 1): {41: 8}, Pred length: 8\nToken distribution (Batch 2): {41: 18}, Pred length: 18\nToken distribution (Batch 3): {41: 14}, Pred length: 14\nToken distribution (Batch 4): {41: 20}, Pred length: 20\nToken distribution (Batch 5): {41: 20}, Pred length: 20\nToken distribution (Batch 6): {41: 8}, Pred length: 8\nToken distribution (Batch 7): {41: 22}, Pred length: 22\nToken distribution (Batch 8): {41: 20}, Pred length: 20\nToken distribution (Batch 9): {41: 14}, Pred length: 14\nToken distribution (Batch 10): {41: 12}, Pred length: 12\nToken distribution (Batch 11): {41: 18}, Pred length: 18\nToken distribution (Batch 12): {41: 8}, Pred length: 8\nToken distribution (Batch 13): {41: 24}, Pred length: 24\nToken distribution (Batch 14): {41: 8}, Pred length: 8\nToken distribution (Batch 15): {41: 14}, Pred length: 14\nToken distribution (Batch 16): {41: 14}, Pred length: 14\nToken distribution (Batch 17): {41: 10}, Pred length: 10\nToken distribution (Batch 18): {41: 22}, Pred length: 22\nToken distribution (Batch 19): {41: 8}, Pred length: 8\nToken distribution (Batch 20): {41: 14}, Pred length: 14\nToken distribution (Batch 21): {41: 18}, Pred length: 18\nToken distribution (Batch 22): {41: 22}, Pred length: 22\nToken distribution (Batch 23): {41: 10}, Pred length: 10\nToken distribution (Batch 24): {41: 20}, Pred length: 20\nToken distribution (Batch 25): {41: 12}, Pred length: 12\nToken distribution (Batch 26): {41: 22}, Pred length: 22\nToken distribution (Batch 27): {41: 18}, Pred length: 18\nToken distribution (Batch 28): {41: 10}, Pred length: 10\nToken distribution (Batch 29): {41: 8}, Pred length: 8\nToken distribution (Batch 30): {41: 18}, Pred length: 18\nToken distribution (Batch 31): {41: 22}, Pred length: 22\nValidation Loss: nan\nValidation Predictions: ['O', 'O', 'O', 'O', 'O']\nGround Truth: ['2y922', 'TZJ7', 'EksfsRnlL', 'lYq*6lA', '09LHiDjhYh']\nCurrent Learning Rate: 2.7500000000000004e-06\nEpoch 16, Filtered data size: 10240, Sample labels: ['qV8ib8H', 'MdI8', '1XS0DRdJ']\nTrain size: 8192, Val size: 2048\nWarning: NaN or Inf loss at batch 0. Skipping...\nWarning: NaN or Inf loss at batch 1. Skipping...\nWarning: NaN or Inf loss at batch 2. Skipping...\nWarning: NaN or Inf loss at batch 3. Skipping...\nWarning: NaN or Inf loss at batch 4. Skipping...\nWarning: NaN or Inf loss at batch 5. Skipping...\nWarning: NaN or Inf loss at batch 6. Skipping...\nWarning: NaN or Inf loss at batch 7. Skipping...\nWarning: NaN or Inf loss at batch 8. Skipping...\nWarning: NaN or Inf loss at batch 9. Skipping...\nWarning: NaN or Inf loss at batch 10. Skipping...\nWarning: NaN or Inf loss at batch 11. Skipping...\nWarning: NaN or Inf loss at batch 12. Skipping...\nWarning: NaN or Inf loss at batch 13. Skipping...\nWarning: NaN or Inf loss at batch 14. Skipping...\nWarning: NaN or Inf loss at batch 15. Skipping...\nWarning: NaN or Inf loss at batch 16. Skipping...\nWarning: NaN or Inf loss at batch 17. Skipping...\nWarning: NaN or Inf loss at batch 18. Skipping...\nWarning: NaN or Inf loss at batch 19. Skipping...\nWarning: NaN or Inf loss at batch 20. Skipping...\nWarning: NaN or Inf loss at batch 21. Skipping...\nWarning: NaN or Inf loss at batch 22. Skipping...\nWarning: NaN or Inf loss at batch 23. Skipping...\nWarning: NaN or Inf loss at batch 24. Skipping...\nWarning: NaN or Inf loss at batch 25. Skipping...\nWarning: NaN or Inf loss at batch 26. Skipping...\nWarning: NaN or Inf loss at batch 27. Skipping...\nWarning: NaN or Inf loss at batch 28. Skipping...\nWarning: NaN or Inf loss at batch 29. Skipping...\nWarning: NaN or Inf loss at batch 30. Skipping...\nWarning: NaN or Inf loss at batch 31. Skipping...\nWarning: NaN or Inf loss at batch 32. Skipping...\nWarning: NaN or Inf loss at batch 33. Skipping...\nWarning: NaN or Inf loss at batch 34. Skipping...\nWarning: NaN or Inf loss at batch 35. Skipping...\nWarning: NaN or Inf loss at batch 36. Skipping...\nWarning: NaN or Inf loss at batch 37. Skipping...\nWarning: NaN or Inf loss at batch 38. Skipping...\nWarning: NaN or Inf loss at batch 39. Skipping...\nWarning: NaN or Inf loss at batch 40. Skipping...\nWarning: NaN or Inf loss at batch 41. Skipping...\nWarning: NaN or Inf loss at batch 42. Skipping...\nWarning: NaN or Inf loss at batch 43. Skipping...\nWarning: NaN or Inf loss at batch 44. Skipping...\nWarning: NaN or Inf loss at batch 45. Skipping...\nWarning: NaN or Inf loss at batch 46. Skipping...\nWarning: NaN or Inf loss at batch 47. Skipping...\nWarning: NaN or Inf loss at batch 48. Skipping...\nWarning: NaN or Inf loss at batch 49. Skipping...\nWarning: NaN or Inf loss at batch 50. Skipping...\nWarning: NaN or Inf loss at batch 51. Skipping...\nWarning: NaN or Inf loss at batch 52. Skipping...\nWarning: NaN or Inf loss at batch 53. Skipping...\nWarning: NaN or Inf loss at batch 54. Skipping...\nWarning: NaN or Inf loss at batch 55. Skipping...\nWarning: NaN or Inf loss at batch 56. Skipping...\nWarning: NaN or Inf loss at batch 57. Skipping...\nWarning: NaN or Inf loss at batch 58. Skipping...\nWarning: NaN or Inf loss at batch 59. Skipping...\nWarning: NaN or Inf loss at batch 60. Skipping...\nWarning: NaN or Inf loss at batch 61. Skipping...\nWarning: NaN or Inf loss at batch 62. Skipping...\nWarning: NaN or Inf loss at batch 63. Skipping...\nWarning: NaN or Inf loss at batch 64. Skipping...\nWarning: NaN or Inf loss at batch 65. Skipping...\nWarning: NaN or Inf loss at batch 66. Skipping...\nWarning: NaN or Inf loss at batch 67. Skipping...\nWarning: NaN or Inf loss at batch 68. Skipping...\nWarning: NaN or Inf loss at batch 69. Skipping...\nWarning: NaN or Inf loss at batch 70. Skipping...\nWarning: NaN or Inf loss at batch 71. Skipping...\nWarning: NaN or Inf loss at batch 72. Skipping...\nWarning: NaN or Inf loss at batch 73. Skipping...\nWarning: NaN or Inf loss at batch 74. Skipping...\nWarning: NaN or Inf loss at batch 75. Skipping...\nWarning: NaN or Inf loss at batch 76. Skipping...\nWarning: NaN or Inf loss at batch 77. Skipping...\nWarning: NaN or Inf loss at batch 78. Skipping...\nWarning: NaN or Inf loss at batch 79. Skipping...\nWarning: NaN or Inf loss at batch 80. Skipping...\nWarning: NaN or Inf loss at batch 81. Skipping...\nWarning: NaN or Inf loss at batch 82. Skipping...\nWarning: NaN or Inf loss at batch 83. Skipping...\nWarning: NaN or Inf loss at batch 84. Skipping...\nWarning: NaN or Inf loss at batch 85. Skipping...\nWarning: NaN or Inf loss at batch 86. Skipping...\nWarning: NaN or Inf loss at batch 87. Skipping...\nWarning: NaN or Inf loss at batch 88. Skipping...\nWarning: NaN or Inf loss at batch 89. Skipping...\nWarning: NaN or Inf loss at batch 90. Skipping...\nWarning: NaN or Inf loss at batch 91. Skipping...\nWarning: NaN or Inf loss at batch 92. Skipping...\nWarning: NaN or Inf loss at batch 93. Skipping...\nWarning: NaN or Inf loss at batch 94. Skipping...\nWarning: NaN or Inf loss at batch 95. Skipping...\nWarning: NaN or Inf loss at batch 96. Skipping...\nWarning: NaN or Inf loss at batch 97. Skipping...\nWarning: NaN or Inf loss at batch 98. Skipping...\nWarning: NaN or Inf loss at batch 99. Skipping...\nWarning: NaN or Inf loss at batch 100. Skipping...\nWarning: NaN or Inf loss at batch 101. Skipping...\nWarning: NaN or Inf loss at batch 102. Skipping...\nWarning: NaN or Inf loss at batch 103. Skipping...\nWarning: NaN or Inf loss at batch 104. Skipping...\nWarning: NaN or Inf loss at batch 105. Skipping...\nWarning: NaN or Inf loss at batch 106. Skipping...\nWarning: NaN or Inf loss at batch 107. Skipping...\nWarning: NaN or Inf loss at batch 108. Skipping...\nWarning: NaN or Inf loss at batch 109. Skipping...\nWarning: NaN or Inf loss at batch 110. Skipping...\nWarning: NaN or Inf loss at batch 111. Skipping...\nWarning: NaN or Inf loss at batch 112. Skipping...\nWarning: NaN or Inf loss at batch 113. Skipping...\nWarning: NaN or Inf loss at batch 114. Skipping...\nWarning: NaN or Inf loss at batch 115. Skipping...\nWarning: NaN or Inf loss at batch 116. Skipping...\nWarning: NaN or Inf loss at batch 117. Skipping...\nWarning: NaN or Inf loss at batch 118. Skipping...\nWarning: NaN or Inf loss at batch 119. Skipping...\nWarning: NaN or Inf loss at batch 120. Skipping...\nWarning: NaN or Inf loss at batch 121. Skipping...\nWarning: NaN or Inf loss at batch 122. Skipping...\nWarning: NaN or Inf loss at batch 123. Skipping...\nWarning: NaN or Inf loss at batch 124. Skipping...\nWarning: NaN or Inf loss at batch 125. Skipping...\nWarning: NaN or Inf loss at batch 126. Skipping...\nWarning: NaN or Inf loss at batch 127. Skipping...\nWarning: NaN or Inf loss at batch 128. Skipping...\nWarning: NaN or Inf loss at batch 129. Skipping...\nWarning: NaN or Inf loss at batch 130. Skipping...\nWarning: NaN or Inf loss at batch 131. Skipping...\nWarning: NaN or Inf loss at batch 132. Skipping...\nWarning: NaN or Inf loss at batch 133. Skipping...\nWarning: NaN or Inf loss at batch 134. Skipping...\nWarning: NaN or Inf loss at batch 135. Skipping...\nWarning: NaN or Inf loss at batch 136. Skipping...\nWarning: NaN or Inf loss at batch 137. Skipping...\nWarning: NaN or Inf loss at batch 138. Skipping...\nWarning: NaN or Inf loss at batch 139. Skipping...\nWarning: NaN or Inf loss at batch 140. Skipping...\nWarning: NaN or Inf loss at batch 141. Skipping...\nWarning: NaN or Inf loss at batch 142. Skipping...\nWarning: NaN or Inf loss at batch 143. Skipping...\nWarning: NaN or Inf loss at batch 144. Skipping...\nWarning: NaN or Inf loss at batch 145. Skipping...\nWarning: NaN or Inf loss at batch 146. Skipping...\nWarning: NaN or Inf loss at batch 147. Skipping...\nWarning: NaN or Inf loss at batch 148. Skipping...\nWarning: NaN or Inf loss at batch 149. Skipping...\nWarning: NaN or Inf loss at batch 150. Skipping...\nWarning: NaN or Inf loss at batch 151. Skipping...\nWarning: NaN or Inf loss at batch 152. Skipping...\nWarning: NaN or Inf loss at batch 153. Skipping...\nWarning: NaN or Inf loss at batch 154. Skipping...\nWarning: NaN or Inf loss at batch 155. Skipping...\nWarning: NaN or Inf loss at batch 156. Skipping...\nWarning: NaN or Inf loss at batch 157. Skipping...\nWarning: NaN or Inf loss at batch 158. Skipping...\nWarning: NaN or Inf loss at batch 159. Skipping...\nWarning: NaN or Inf loss at batch 160. Skipping...\nWarning: NaN or Inf loss at batch 161. Skipping...\nWarning: NaN or Inf loss at batch 162. Skipping...\nWarning: NaN or Inf loss at batch 163. Skipping...\nWarning: NaN or Inf loss at batch 164. Skipping...\nWarning: NaN or Inf loss at batch 165. Skipping...\nWarning: NaN or Inf loss at batch 166. Skipping...\nWarning: NaN or Inf loss at batch 167. Skipping...\nWarning: NaN or Inf loss at batch 168. Skipping...\nWarning: NaN or Inf loss at batch 169. Skipping...\nWarning: NaN or Inf loss at batch 170. Skipping...\nWarning: NaN or Inf loss at batch 171. Skipping...\nWarning: NaN or Inf loss at batch 172. Skipping...\nWarning: NaN or Inf loss at batch 173. Skipping...\nWarning: NaN or Inf loss at batch 174. Skipping...\nWarning: NaN or Inf loss at batch 175. Skipping...\nWarning: NaN or Inf loss at batch 176. Skipping...\nWarning: NaN or Inf loss at batch 177. Skipping...\nWarning: NaN or Inf loss at batch 178. Skipping...\nWarning: NaN or Inf loss at batch 179. Skipping...\nWarning: NaN or Inf loss at batch 180. Skipping...\nWarning: NaN or Inf loss at batch 181. Skipping...\nWarning: NaN or Inf loss at batch 182. Skipping...\nWarning: NaN or Inf loss at batch 183. Skipping...\nWarning: NaN or Inf loss at batch 184. Skipping...\nWarning: NaN or Inf loss at batch 185. Skipping...\nWarning: NaN or Inf loss at batch 186. Skipping...\nWarning: NaN or Inf loss at batch 187. Skipping...\nWarning: NaN or Inf loss at batch 188. Skipping...\nWarning: NaN or Inf loss at batch 189. Skipping...\nWarning: NaN or Inf loss at batch 190. Skipping...\nWarning: NaN or Inf loss at batch 191. Skipping...\nWarning: NaN or Inf loss at batch 192. Skipping...\nWarning: NaN or Inf loss at batch 193. Skipping...\nWarning: NaN or Inf loss at batch 194. Skipping...\nWarning: NaN or Inf loss at batch 195. Skipping...\nWarning: NaN or Inf loss at batch 196. Skipping...\nWarning: NaN or Inf loss at batch 197. Skipping...\nWarning: NaN or Inf loss at batch 198. Skipping...\nWarning: NaN or Inf loss at batch 199. Skipping...\nWarning: NaN or Inf loss at batch 200. Skipping...\nWarning: NaN or Inf loss at batch 201. Skipping...\nWarning: NaN or Inf loss at batch 202. Skipping...\nWarning: NaN or Inf loss at batch 203. Skipping...\nWarning: NaN or Inf loss at batch 204. Skipping...\nWarning: NaN or Inf loss at batch 205. Skipping...\nWarning: NaN or Inf loss at batch 206. Skipping...\nWarning: NaN or Inf loss at batch 207. Skipping...\nWarning: NaN or Inf loss at batch 208. Skipping...\nWarning: NaN or Inf loss at batch 209. Skipping...\nWarning: NaN or Inf loss at batch 210. Skipping...\nWarning: NaN or Inf loss at batch 211. Skipping...\nWarning: NaN or Inf loss at batch 212. Skipping...\nWarning: NaN or Inf loss at batch 213. Skipping...\nWarning: NaN or Inf loss at batch 214. Skipping...\nWarning: NaN or Inf loss at batch 215. Skipping...\nWarning: NaN or Inf loss at batch 216. Skipping...\nWarning: NaN or Inf loss at batch 217. Skipping...\nWarning: NaN or Inf loss at batch 218. Skipping...\nWarning: NaN or Inf loss at batch 219. Skipping...\nWarning: NaN or Inf loss at batch 220. Skipping...\nWarning: NaN or Inf loss at batch 221. Skipping...\nWarning: NaN or Inf loss at batch 222. Skipping...\nWarning: NaN or Inf loss at batch 223. Skipping...\nWarning: NaN or Inf loss at batch 224. Skipping...\nWarning: NaN or Inf loss at batch 225. Skipping...\nWarning: NaN or Inf loss at batch 226. Skipping...\nWarning: NaN or Inf loss at batch 227. Skipping...\nWarning: NaN or Inf loss at batch 228. Skipping...\nWarning: NaN or Inf loss at batch 229. Skipping...\nWarning: NaN or Inf loss at batch 230. Skipping...\nWarning: NaN or Inf loss at batch 231. Skipping...\nWarning: NaN or Inf loss at batch 232. Skipping...\nWarning: NaN or Inf loss at batch 233. Skipping...\nWarning: NaN or Inf loss at batch 234. Skipping...\nWarning: NaN or Inf loss at batch 235. Skipping...\nWarning: NaN or Inf loss at batch 236. Skipping...\nWarning: NaN or Inf loss at batch 237. Skipping...\nWarning: NaN or Inf loss at batch 238. Skipping...\nWarning: NaN or Inf loss at batch 239. Skipping...\nWarning: NaN or Inf loss at batch 240. Skipping...\nWarning: NaN or Inf loss at batch 241. Skipping...\nWarning: NaN or Inf loss at batch 242. Skipping...\nWarning: NaN or Inf loss at batch 243. Skipping...\nWarning: NaN or Inf loss at batch 244. Skipping...\nWarning: NaN or Inf loss at batch 245. Skipping...\nWarning: NaN or Inf loss at batch 246. Skipping...\nWarning: NaN or Inf loss at batch 247. Skipping...\nWarning: NaN or Inf loss at batch 248. Skipping...\nWarning: NaN or Inf loss at batch 249. Skipping...\nWarning: NaN or Inf loss at batch 250. Skipping...\nWarning: NaN or Inf loss at batch 251. Skipping...\nWarning: NaN or Inf loss at batch 252. Skipping...\nWarning: NaN or Inf loss at batch 253. Skipping...\nWarning: NaN or Inf loss at batch 254. Skipping...\nWarning: NaN or Inf loss at batch 255. Skipping...\nEpoch 16/20, Loss: 0.0000\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nWarning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\nToken distribution (Batch 0): {41: 20}, Pred length: 20\nToken distribution (Batch 1): {41: 10}, Pred length: 10\nToken distribution (Batch 2): {41: 20}, Pred length: 20\nToken distribution (Batch 3): {41: 24}, Pred length: 24\nToken distribution (Batch 4): {41: 18}, Pred length: 18\nToken distribution (Batch 5): {41: 22}, Pred length: 22\nToken distribution (Batch 6): {41: 12}, Pred length: 12\nToken distribution (Batch 7): {41: 8}, Pred length: 8\nToken distribution (Batch 8): {41: 22}, Pred length: 22\nToken distribution (Batch 9): {41: 20}, Pred length: 20\nToken distribution (Batch 10): {41: 20}, Pred length: 20\nToken distribution (Batch 11): {41: 8}, Pred length: 8\nToken distribution (Batch 12): {41: 24}, Pred length: 24\nToken distribution (Batch 13): {41: 18}, Pred length: 18\nToken distribution (Batch 14): {41: 12}, Pred length: 12\nToken distribution (Batch 15): {41: 20}, Pred length: 20\nToken distribution (Batch 16): {41: 24}, Pred length: 24\nToken distribution (Batch 17): {41: 12}, Pred length: 12\nToken distribution (Batch 18): {41: 22}, Pred length: 22\nToken distribution (Batch 19): {41: 8}, Pred length: 8\nToken distribution (Batch 20): {41: 24}, Pred length: 24\nToken distribution (Batch 21): {41: 18}, Pred length: 18\nToken distribution (Batch 22): {41: 24}, Pred length: 24\nToken distribution (Batch 23): {41: 18}, Pred length: 18\nToken distribution (Batch 24): {41: 12}, Pred length: 12\nToken distribution (Batch 25): {41: 14}, Pred length: 14\nToken distribution (Batch 26): {41: 12}, Pred length: 12\nToken distribution (Batch 27): {41: 22}, Pred length: 22\nToken distribution (Batch 28): {41: 8}, Pred length: 8\nToken distribution (Batch 29): {41: 12}, Pred length: 12\nToken distribution (Batch 30): {41: 22}, Pred length: 22\nToken distribution (Batch 31): {41: 24}, Pred length: 24\nValidation Loss: nan\nValidation Predictions: ['O', 'O', 'O', 'O', 'O']\nGround Truth: ['6Mir2yt1Sp', 'A6y76', 'x5liy0yX7n', '2tAiuRuj*OY1', 'QM6MmvXqd']\nCurrent Learning Rate: 1.3750000000000002e-06\nEpoch 17, Filtered data size: 10240, Sample labels: ['qV8ib8H', 'MdI8', '1XS0DRdJ']\nTrain size: 8192, Val size: 2048\nWarning: NaN or Inf loss at batch 0. Skipping...\nWarning: NaN or Inf loss at batch 1. Skipping...\nWarning: NaN or Inf loss at batch 2. Skipping...\nWarning: NaN or Inf loss at batch 3. Skipping...\nWarning: NaN or Inf loss at batch 4. Skipping...\nWarning: NaN or Inf loss at batch 5. Skipping...\nWarning: NaN or Inf loss at batch 6. Skipping...\nWarning: NaN or Inf loss at batch 7. Skipping...\nWarning: NaN or Inf loss at batch 8. Skipping...\nWarning: NaN or Inf loss at batch 9. Skipping...\nWarning: NaN or Inf loss at batch 10. Skipping...\nWarning: NaN or Inf loss at batch 11. Skipping...\nWarning: NaN or Inf loss at batch 12. Skipping...\nWarning: NaN or Inf loss at batch 13. Skipping...\nWarning: NaN or Inf loss at batch 14. Skipping...\nWarning: NaN or Inf loss at batch 15. Skipping...\nWarning: NaN or Inf loss at batch 16. Skipping...\nWarning: NaN or Inf loss at batch 17. Skipping...\nWarning: NaN or Inf loss at batch 18. Skipping...\nWarning: NaN or Inf loss at batch 19. Skipping...\nWarning: NaN or Inf loss at batch 20. Skipping...\nWarning: NaN or Inf loss at batch 21. Skipping...\nWarning: NaN or Inf loss at batch 22. Skipping...\nWarning: NaN or Inf loss at batch 23. Skipping...\nWarning: NaN or Inf loss at batch 24. Skipping...\nWarning: NaN or Inf loss at batch 25. Skipping...\nWarning: NaN or Inf loss at batch 26. Skipping...\nWarning: NaN or Inf loss at batch 27. Skipping...\nWarning: NaN or Inf loss at batch 28. Skipping...\nWarning: NaN or Inf loss at batch 29. Skipping...\nWarning: NaN or Inf loss at batch 30. Skipping...\nWarning: NaN or Inf loss at batch 31. Skipping...\nWarning: NaN or Inf loss at batch 32. Skipping...\nWarning: NaN or Inf loss at batch 33. Skipping...\nWarning: NaN or Inf loss at batch 34. Skipping...\nWarning: NaN or Inf loss at batch 35. Skipping...\nWarning: NaN or Inf loss at batch 36. Skipping...\nWarning: NaN or Inf loss at batch 37. Skipping...\nWarning: NaN or Inf loss at batch 38. Skipping...\nWarning: NaN or Inf loss at batch 39. Skipping...\nWarning: NaN or Inf loss at batch 40. Skipping...\nWarning: NaN or Inf loss at batch 41. Skipping...\nWarning: NaN or Inf loss at batch 42. Skipping...\nWarning: NaN or Inf loss at batch 43. Skipping...\nWarning: NaN or Inf loss at batch 44. Skipping...\nWarning: NaN or Inf loss at batch 45. Skipping...\nWarning: NaN or Inf loss at batch 46. Skipping...\nWarning: NaN or Inf loss at batch 47. Skipping...\nWarning: NaN or Inf loss at batch 48. Skipping...\nWarning: NaN or Inf loss at batch 49. Skipping...\nWarning: NaN or Inf loss at batch 50. Skipping...\nWarning: NaN or Inf loss at batch 51. Skipping...\nWarning: NaN or Inf loss at batch 52. Skipping...\nWarning: NaN or Inf loss at batch 53. Skipping...\nWarning: NaN or Inf loss at batch 54. Skipping...\nWarning: NaN or Inf loss at batch 55. Skipping...\nWarning: NaN or Inf loss at batch 56. Skipping...\nWarning: NaN or Inf loss at batch 57. Skipping...\nWarning: NaN or Inf loss at batch 58. Skipping...\nWarning: NaN or Inf loss at batch 59. Skipping...\nWarning: NaN or Inf loss at batch 60. Skipping...\nWarning: NaN or Inf loss at batch 61. Skipping...\nWarning: NaN or Inf loss at batch 62. Skipping...\nWarning: NaN or Inf loss at batch 63. Skipping...\nWarning: NaN or Inf loss at batch 64. Skipping...\nWarning: NaN or Inf loss at batch 65. Skipping...\nWarning: NaN or Inf loss at batch 66. Skipping...\nWarning: NaN or Inf loss at batch 67. Skipping...\nWarning: NaN or Inf loss at batch 68. Skipping...\nWarning: NaN or Inf loss at batch 69. Skipping...\nWarning: NaN or Inf loss at batch 70. Skipping...\nWarning: NaN or Inf loss at batch 71. Skipping...\nWarning: NaN or Inf loss at batch 72. Skipping...\nWarning: NaN or Inf loss at batch 73. Skipping...\nWarning: NaN or Inf loss at batch 74. Skipping...\nWarning: NaN or Inf loss at batch 75. Skipping...\nWarning: NaN or Inf loss at batch 76. Skipping...\nWarning: NaN or Inf loss at batch 77. Skipping...\nWarning: NaN or Inf loss at batch 78. Skipping...\nWarning: NaN or Inf loss at batch 79. Skipping...\nWarning: NaN or Inf loss at batch 80. Skipping...\nWarning: NaN or Inf loss at batch 81. Skipping...\nWarning: NaN or Inf loss at batch 82. Skipping...\nWarning: NaN or Inf loss at batch 83. Skipping...\nWarning: NaN or Inf loss at batch 84. Skipping...\nWarning: NaN or Inf loss at batch 85. Skipping...\nWarning: NaN or Inf loss at batch 86. Skipping...\nWarning: NaN or Inf loss at batch 87. Skipping...\nWarning: NaN or Inf loss at batch 88. Skipping...\nWarning: NaN or Inf loss at batch 89. Skipping...\nWarning: NaN or Inf loss at batch 90. Skipping...\nWarning: NaN or Inf loss at batch 91. Skipping...\nWarning: NaN or Inf loss at batch 92. Skipping...\nWarning: NaN or Inf loss at batch 93. Skipping...\nWarning: NaN or Inf loss at batch 94. Skipping...\nWarning: NaN or Inf loss at batch 95. Skipping...\nWarning: NaN or Inf loss at batch 96. Skipping...\nWarning: NaN or Inf loss at batch 97. Skipping...\nWarning: NaN or Inf loss at batch 98. Skipping...\nWarning: NaN or Inf loss at batch 99. Skipping...\nWarning: NaN or Inf loss at batch 100. Skipping...\nWarning: NaN or Inf loss at batch 101. Skipping...\nWarning: NaN or Inf loss at batch 102. Skipping...\nWarning: NaN or Inf loss at batch 103. Skipping...\nWarning: NaN or Inf loss at batch 104. Skipping...\nWarning: NaN or Inf loss at batch 105. Skipping...\nWarning: NaN or Inf loss at batch 106. Skipping...\nWarning: NaN or Inf loss at batch 107. Skipping...\nWarning: NaN or Inf loss at batch 108. Skipping...\nWarning: NaN or Inf loss at batch 109. Skipping...\nWarning: NaN or Inf loss at batch 110. Skipping...\nWarning: NaN or Inf loss at batch 111. Skipping...\nWarning: NaN or Inf loss at batch 112. Skipping...\nWarning: NaN or Inf loss at batch 113. Skipping...\nWarning: NaN or Inf loss at batch 114. Skipping...\nWarning: NaN or Inf loss at batch 115. Skipping...\nWarning: NaN or Inf loss at batch 116. Skipping...\nWarning: NaN or Inf loss at batch 117. Skipping...\nWarning: NaN or Inf loss at batch 118. Skipping...\nWarning: NaN or Inf loss at batch 119. Skipping...\nWarning: NaN or Inf loss at batch 120. Skipping...\nWarning: NaN or Inf loss at batch 121. Skipping...\nWarning: NaN or Inf loss at batch 122. Skipping...\nWarning: NaN or Inf loss at batch 123. Skipping...\nWarning: NaN or Inf loss at batch 124. Skipping...\nWarning: NaN or Inf loss at batch 125. Skipping...\nWarning: NaN or Inf loss at batch 126. Skipping...\nWarning: NaN or Inf loss at batch 127. Skipping...\nWarning: NaN or Inf loss at batch 128. Skipping...\nWarning: NaN or Inf loss at batch 129. Skipping...\nWarning: NaN or Inf loss at batch 130. Skipping...\nWarning: NaN or Inf loss at batch 131. Skipping...\nWarning: NaN or Inf loss at batch 132. Skipping...\nWarning: NaN or Inf loss at batch 133. Skipping...\nWarning: NaN or Inf loss at batch 134. Skipping...\nWarning: NaN or Inf loss at batch 135. Skipping...\nWarning: NaN or Inf loss at batch 136. Skipping...\nWarning: NaN or Inf loss at batch 137. Skipping...\nWarning: NaN or Inf loss at batch 138. Skipping...\nWarning: NaN or Inf loss at batch 139. Skipping...\nWarning: NaN or Inf loss at batch 140. Skipping...\nWarning: NaN or Inf loss at batch 141. Skipping...\nWarning: NaN or Inf loss at batch 142. Skipping...\nWarning: NaN or Inf loss at batch 143. Skipping...\nWarning: NaN or Inf loss at batch 144. Skipping...\nWarning: NaN or Inf loss at batch 145. Skipping...\nWarning: NaN or Inf loss at batch 146. Skipping...\nWarning: NaN or Inf loss at batch 147. Skipping...\nWarning: NaN or Inf loss at batch 148. Skipping...\nWarning: NaN or Inf loss at batch 149. Skipping...\nWarning: NaN or Inf loss at batch 150. Skipping...\nWarning: NaN or Inf loss at batch 151. Skipping...\nWarning: NaN or Inf loss at batch 152. Skipping...\nWarning: NaN or Inf loss at batch 153. Skipping...\nWarning: NaN or Inf loss at batch 154. Skipping...\nWarning: NaN or Inf loss at batch 155. Skipping...\nWarning: NaN or Inf loss at batch 156. Skipping...\nWarning: NaN or Inf loss at batch 157. Skipping...\nWarning: NaN or Inf loss at batch 158. Skipping...\nWarning: NaN or Inf loss at batch 159. Skipping...\nWarning: NaN or Inf loss at batch 160. Skipping...\nWarning: NaN or Inf loss at batch 161. Skipping...\nWarning: NaN or Inf loss at batch 162. Skipping...\nWarning: NaN or Inf loss at batch 163. Skipping...\nWarning: NaN or Inf loss at batch 164. Skipping...\nWarning: NaN or Inf loss at batch 165. Skipping...\nWarning: NaN or Inf loss at batch 166. Skipping...\nWarning: NaN or Inf loss at batch 167. Skipping...\nWarning: NaN or Inf loss at batch 168. Skipping...\nWarning: NaN or Inf loss at batch 169. Skipping...\nWarning: NaN or Inf loss at batch 170. Skipping...\nWarning: NaN or Inf loss at batch 171. Skipping...\nWarning: NaN or Inf loss at batch 172. Skipping...\nWarning: NaN or Inf loss at batch 173. Skipping...\nWarning: NaN or Inf loss at batch 174. Skipping...\nWarning: NaN or Inf loss at batch 175. Skipping...\nWarning: NaN or Inf loss at batch 176. Skipping...\nWarning: NaN or Inf loss at batch 177. Skipping...\nWarning: NaN or Inf loss at batch 178. Skipping...\nWarning: NaN or Inf loss at batch 179. Skipping...\nWarning: NaN or Inf loss at batch 180. Skipping...\nWarning: NaN or Inf loss at batch 181. Skipping...\nWarning: NaN or Inf loss at batch 182. Skipping...\nWarning: NaN or Inf loss at batch 183. Skipping...\nWarning: NaN or Inf loss at batch 184. Skipping...\nWarning: NaN or Inf loss at batch 185. Skipping...\nWarning: NaN or Inf loss at batch 186. Skipping...\nWarning: NaN or Inf loss at batch 187. Skipping...\nWarning: NaN or Inf loss at batch 188. Skipping...\nWarning: NaN or Inf loss at batch 189. Skipping...\nWarning: NaN or Inf loss at batch 190. Skipping...\nWarning: NaN or Inf loss at batch 191. Skipping...\nWarning: NaN or Inf loss at batch 192. Skipping...\nWarning: NaN or Inf loss at batch 193. Skipping...\nWarning: NaN or Inf loss at batch 194. Skipping...\nWarning: NaN or Inf loss at batch 195. Skipping...\nWarning: NaN or Inf loss at batch 196. Skipping...\nWarning: NaN or Inf loss at batch 197. Skipping...\nWarning: NaN or Inf loss at batch 198. Skipping...\nWarning: NaN or Inf loss at batch 199. Skipping...\nWarning: NaN or Inf loss at batch 200. Skipping...\nWarning: NaN or Inf loss at batch 201. Skipping...\nWarning: NaN or Inf loss at batch 202. Skipping...\nWarning: NaN or Inf loss at batch 203. Skipping...\nWarning: NaN or Inf loss at batch 204. Skipping...\nWarning: NaN or Inf loss at batch 205. Skipping...\nWarning: NaN or Inf loss at batch 206. Skipping...\nWarning: NaN or Inf loss at batch 207. Skipping...\nWarning: NaN or Inf loss at batch 208. Skipping...\nWarning: NaN or Inf loss at batch 209. Skipping...\nWarning: NaN or Inf loss at batch 210. Skipping...\nWarning: NaN or Inf loss at batch 211. Skipping...\nWarning: NaN or Inf loss at batch 212. Skipping...\nWarning: NaN or Inf loss at batch 213. Skipping...\nWarning: NaN or Inf loss at batch 214. Skipping...\nWarning: NaN or Inf loss at batch 215. Skipping...\nWarning: NaN or Inf loss at batch 216. Skipping...\nWarning: NaN or Inf loss at batch 217. Skipping...\nWarning: NaN or Inf loss at batch 218. Skipping...\nWarning: NaN or Inf loss at batch 219. Skipping...\nWarning: NaN or Inf loss at batch 220. Skipping...\nWarning: NaN or Inf loss at batch 221. Skipping...\nWarning: NaN or Inf loss at batch 222. Skipping...\nWarning: NaN or Inf loss at batch 223. Skipping...\nWarning: NaN or Inf loss at batch 224. Skipping...\nWarning: NaN or Inf loss at batch 225. Skipping...\nWarning: NaN or Inf loss at batch 226. Skipping...\nWarning: NaN or Inf loss at batch 227. Skipping...\nWarning: NaN or Inf loss at batch 228. Skipping...\nWarning: NaN or Inf loss at batch 229. Skipping...\nWarning: NaN or Inf loss at batch 230. Skipping...\nWarning: NaN or Inf loss at batch 231. Skipping...\nWarning: NaN or Inf loss at batch 232. Skipping...\nWarning: NaN or Inf loss at batch 233. Skipping...\nWarning: NaN or Inf loss at batch 234. Skipping...\nWarning: NaN or Inf loss at batch 235. Skipping...\nWarning: NaN or Inf loss at batch 236. Skipping...\nWarning: NaN or Inf loss at batch 237. Skipping...\nWarning: NaN or Inf loss at batch 238. Skipping...\nWarning: NaN or Inf loss at batch 239. Skipping...\nWarning: NaN or Inf loss at batch 240. Skipping...\nWarning: NaN or Inf loss at batch 241. Skipping...\nWarning: NaN or Inf loss at batch 242. Skipping...\nWarning: NaN or Inf loss at batch 243. Skipping...\nWarning: NaN or Inf loss at batch 244. Skipping...\nWarning: NaN or Inf loss at batch 245. Skipping...\nWarning: NaN or Inf loss at batch 246. Skipping...\nWarning: NaN or Inf loss at batch 247. Skipping...\nWarning: NaN or Inf loss at batch 248. Skipping...\nWarning: NaN or Inf loss at batch 249. Skipping...\nWarning: NaN or Inf loss at batch 250. Skipping...\nWarning: NaN or Inf loss at batch 251. Skipping...\nWarning: NaN or Inf loss at batch 252. Skipping...\nWarning: NaN or Inf loss at batch 253. Skipping...\nWarning: NaN or Inf loss at batch 254. Skipping...\nWarning: NaN or Inf loss at batch 255. Skipping...\nEpoch 17/20, Loss: 0.0000\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nWarning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\nToken distribution (Batch 0): {41: 20}, Pred length: 20\nToken distribution (Batch 1): {41: 16}, Pred length: 16\nToken distribution (Batch 2): {41: 8}, Pred length: 8\nToken distribution (Batch 3): {41: 10}, Pred length: 10\nToken distribution (Batch 4): {41: 24}, Pred length: 24\nToken distribution (Batch 5): {41: 24}, Pred length: 24\nToken distribution (Batch 6): {41: 12}, Pred length: 12\nToken distribution (Batch 7): {41: 20}, Pred length: 20\nToken distribution (Batch 8): {41: 12}, Pred length: 12\nToken distribution (Batch 9): {41: 24}, Pred length: 24\nToken distribution (Batch 10): {41: 8}, Pred length: 8\nToken distribution (Batch 11): {41: 18}, Pred length: 18\nToken distribution (Batch 12): {41: 24}, Pred length: 24\nToken distribution (Batch 13): {41: 14}, Pred length: 14\nToken distribution (Batch 14): {41: 24}, Pred length: 24\nToken distribution (Batch 15): {41: 18}, Pred length: 18\nToken distribution (Batch 16): {41: 24}, Pred length: 24\nToken distribution (Batch 17): {41: 12}, Pred length: 12\nToken distribution (Batch 18): {41: 12}, Pred length: 12\nToken distribution (Batch 19): {41: 16}, Pred length: 16\nToken distribution (Batch 20): {41: 16}, Pred length: 16\nToken distribution (Batch 21): {41: 14}, Pred length: 14\nToken distribution (Batch 22): {41: 16}, Pred length: 16\nToken distribution (Batch 23): {41: 8}, Pred length: 8\nToken distribution (Batch 24): {41: 16}, Pred length: 16\nToken distribution (Batch 25): {41: 24}, Pred length: 24\nToken distribution (Batch 26): {41: 10}, Pred length: 10\nToken distribution (Batch 27): {41: 18}, Pred length: 18\nToken distribution (Batch 28): {41: 14}, Pred length: 14\nToken distribution (Batch 29): {41: 18}, Pred length: 18\nToken distribution (Batch 30): {41: 24}, Pred length: 24\nToken distribution (Batch 31): {41: 16}, Pred length: 16\nValidation Loss: nan\nValidation Predictions: ['O', 'O', 'O', 'O', 'O']\nGround Truth: ['S6PulPMAv2', 'Y2T1BI4C', 'aKko', 'G6Eub', 'dbs2-wEylCuq']\nCurrent Learning Rate: 1.3750000000000002e-06\nEpoch 18, Filtered data size: 10240, Sample labels: ['qV8ib8H', 'MdI8', '1XS0DRdJ']\nTrain size: 8192, Val size: 2048\nWarning: NaN or Inf loss at batch 0. Skipping...\nWarning: NaN or Inf loss at batch 1. Skipping...\nWarning: NaN or Inf loss at batch 2. Skipping...\nWarning: NaN or Inf loss at batch 3. Skipping...\nWarning: NaN or Inf loss at batch 4. Skipping...\nWarning: NaN or Inf loss at batch 5. Skipping...\nWarning: NaN or Inf loss at batch 6. Skipping...\nWarning: NaN or Inf loss at batch 7. Skipping...\nWarning: NaN or Inf loss at batch 8. Skipping...\nWarning: NaN or Inf loss at batch 9. Skipping...\nWarning: NaN or Inf loss at batch 10. Skipping...\nWarning: NaN or Inf loss at batch 11. Skipping...\nWarning: NaN or Inf loss at batch 12. Skipping...\nWarning: NaN or Inf loss at batch 13. Skipping...\nWarning: NaN or Inf loss at batch 14. Skipping...\nWarning: NaN or Inf loss at batch 15. Skipping...\nWarning: NaN or Inf loss at batch 16. Skipping...\nWarning: NaN or Inf loss at batch 17. Skipping...\nWarning: NaN or Inf loss at batch 18. Skipping...\nWarning: NaN or Inf loss at batch 19. Skipping...\nWarning: NaN or Inf loss at batch 20. Skipping...\nWarning: NaN or Inf loss at batch 21. Skipping...\nWarning: NaN or Inf loss at batch 22. Skipping...\nWarning: NaN or Inf loss at batch 23. Skipping...\nWarning: NaN or Inf loss at batch 24. Skipping...\nWarning: NaN or Inf loss at batch 25. Skipping...\nWarning: NaN or Inf loss at batch 26. Skipping...\nWarning: NaN or Inf loss at batch 27. Skipping...\nWarning: NaN or Inf loss at batch 28. Skipping...\nWarning: NaN or Inf loss at batch 29. Skipping...\nWarning: NaN or Inf loss at batch 30. Skipping...\nWarning: NaN or Inf loss at batch 31. Skipping...\nWarning: NaN or Inf loss at batch 32. Skipping...\nWarning: NaN or Inf loss at batch 33. Skipping...\nWarning: NaN or Inf loss at batch 34. Skipping...\nWarning: NaN or Inf loss at batch 35. Skipping...\nWarning: NaN or Inf loss at batch 36. Skipping...\nWarning: NaN or Inf loss at batch 37. Skipping...\nWarning: NaN or Inf loss at batch 38. Skipping...\nWarning: NaN or Inf loss at batch 39. Skipping...\nWarning: NaN or Inf loss at batch 40. Skipping...\nWarning: NaN or Inf loss at batch 41. Skipping...\nWarning: NaN or Inf loss at batch 42. Skipping...\nWarning: NaN or Inf loss at batch 43. Skipping...\nWarning: NaN or Inf loss at batch 44. Skipping...\nWarning: NaN or Inf loss at batch 45. Skipping...\nWarning: NaN or Inf loss at batch 46. Skipping...\nWarning: NaN or Inf loss at batch 47. Skipping...\nWarning: NaN or Inf loss at batch 48. Skipping...\nWarning: NaN or Inf loss at batch 49. Skipping...\nWarning: NaN or Inf loss at batch 50. Skipping...\nWarning: NaN or Inf loss at batch 51. Skipping...\nWarning: NaN or Inf loss at batch 52. Skipping...\nWarning: NaN or Inf loss at batch 53. Skipping...\nWarning: NaN or Inf loss at batch 54. Skipping...\nWarning: NaN or Inf loss at batch 55. Skipping...\nWarning: NaN or Inf loss at batch 56. Skipping...\nWarning: NaN or Inf loss at batch 57. Skipping...\nWarning: NaN or Inf loss at batch 58. Skipping...\nWarning: NaN or Inf loss at batch 59. Skipping...\nWarning: NaN or Inf loss at batch 60. Skipping...\nWarning: NaN or Inf loss at batch 61. Skipping...\nWarning: NaN or Inf loss at batch 62. Skipping...\nWarning: NaN or Inf loss at batch 63. Skipping...\nWarning: NaN or Inf loss at batch 64. Skipping...\nWarning: NaN or Inf loss at batch 65. Skipping...\nWarning: NaN or Inf loss at batch 66. Skipping...\nWarning: NaN or Inf loss at batch 67. Skipping...\nWarning: NaN or Inf loss at batch 68. Skipping...\nWarning: NaN or Inf loss at batch 69. Skipping...\nWarning: NaN or Inf loss at batch 70. Skipping...\nWarning: NaN or Inf loss at batch 71. Skipping...\nWarning: NaN or Inf loss at batch 72. Skipping...\nWarning: NaN or Inf loss at batch 73. Skipping...\nWarning: NaN or Inf loss at batch 74. Skipping...\nWarning: NaN or Inf loss at batch 75. Skipping...\nWarning: NaN or Inf loss at batch 76. Skipping...\nWarning: NaN or Inf loss at batch 77. Skipping...\nWarning: NaN or Inf loss at batch 78. Skipping...\nWarning: NaN or Inf loss at batch 79. Skipping...\nWarning: NaN or Inf loss at batch 80. Skipping...\nWarning: NaN or Inf loss at batch 81. Skipping...\nWarning: NaN or Inf loss at batch 82. Skipping...\nWarning: NaN or Inf loss at batch 83. Skipping...\nWarning: NaN or Inf loss at batch 84. Skipping...\nWarning: NaN or Inf loss at batch 85. Skipping...\nWarning: NaN or Inf loss at batch 86. Skipping...\nWarning: NaN or Inf loss at batch 87. Skipping...\nWarning: NaN or Inf loss at batch 88. Skipping...\nWarning: NaN or Inf loss at batch 89. Skipping...\nWarning: NaN or Inf loss at batch 90. Skipping...\nWarning: NaN or Inf loss at batch 91. Skipping...\nWarning: NaN or Inf loss at batch 92. Skipping...\nWarning: NaN or Inf loss at batch 93. Skipping...\nWarning: NaN or Inf loss at batch 94. Skipping...\nWarning: NaN or Inf loss at batch 95. Skipping...\nWarning: NaN or Inf loss at batch 96. Skipping...\nWarning: NaN or Inf loss at batch 97. Skipping...\nWarning: NaN or Inf loss at batch 98. Skipping...\nWarning: NaN or Inf loss at batch 99. Skipping...\nWarning: NaN or Inf loss at batch 100. Skipping...\nWarning: NaN or Inf loss at batch 101. Skipping...\nWarning: NaN or Inf loss at batch 102. Skipping...\nWarning: NaN or Inf loss at batch 103. Skipping...\nWarning: NaN or Inf loss at batch 104. Skipping...\nWarning: NaN or Inf loss at batch 105. Skipping...\nWarning: NaN or Inf loss at batch 106. Skipping...\nWarning: NaN or Inf loss at batch 107. Skipping...\nWarning: NaN or Inf loss at batch 108. Skipping...\nWarning: NaN or Inf loss at batch 109. Skipping...\nWarning: NaN or Inf loss at batch 110. Skipping...\nWarning: NaN or Inf loss at batch 111. Skipping...\nWarning: NaN or Inf loss at batch 112. Skipping...\nWarning: NaN or Inf loss at batch 113. Skipping...\nWarning: NaN or Inf loss at batch 114. Skipping...\nWarning: NaN or Inf loss at batch 115. Skipping...\nWarning: NaN or Inf loss at batch 116. Skipping...\nWarning: NaN or Inf loss at batch 117. Skipping...\nWarning: NaN or Inf loss at batch 118. Skipping...\nWarning: NaN or Inf loss at batch 119. Skipping...\nWarning: NaN or Inf loss at batch 120. Skipping...\nWarning: NaN or Inf loss at batch 121. Skipping...\nWarning: NaN or Inf loss at batch 122. Skipping...\nWarning: NaN or Inf loss at batch 123. Skipping...\nWarning: NaN or Inf loss at batch 124. Skipping...\nWarning: NaN or Inf loss at batch 125. Skipping...\nWarning: NaN or Inf loss at batch 126. Skipping...\nWarning: NaN or Inf loss at batch 127. Skipping...\nWarning: NaN or Inf loss at batch 128. Skipping...\nWarning: NaN or Inf loss at batch 129. Skipping...\nWarning: NaN or Inf loss at batch 130. Skipping...\nWarning: NaN or Inf loss at batch 131. Skipping...\nWarning: NaN or Inf loss at batch 132. Skipping...\nWarning: NaN or Inf loss at batch 133. Skipping...\nWarning: NaN or Inf loss at batch 134. Skipping...\nWarning: NaN or Inf loss at batch 135. Skipping...\nWarning: NaN or Inf loss at batch 136. Skipping...\nWarning: NaN or Inf loss at batch 137. Skipping...\nWarning: NaN or Inf loss at batch 138. Skipping...\nWarning: NaN or Inf loss at batch 139. Skipping...\nWarning: NaN or Inf loss at batch 140. Skipping...\nWarning: NaN or Inf loss at batch 141. Skipping...\nWarning: NaN or Inf loss at batch 142. Skipping...\nWarning: NaN or Inf loss at batch 143. Skipping...\nWarning: NaN or Inf loss at batch 144. Skipping...\nWarning: NaN or Inf loss at batch 145. Skipping...\nWarning: NaN or Inf loss at batch 146. Skipping...\nWarning: NaN or Inf loss at batch 147. Skipping...\nWarning: NaN or Inf loss at batch 148. Skipping...\nWarning: NaN or Inf loss at batch 149. Skipping...\nWarning: NaN or Inf loss at batch 150. Skipping...\nWarning: NaN or Inf loss at batch 151. Skipping...\nWarning: NaN or Inf loss at batch 152. Skipping...\nWarning: NaN or Inf loss at batch 153. Skipping...\nWarning: NaN or Inf loss at batch 154. Skipping...\nWarning: NaN or Inf loss at batch 155. Skipping...\nWarning: NaN or Inf loss at batch 156. Skipping...\nWarning: NaN or Inf loss at batch 157. Skipping...\nWarning: NaN or Inf loss at batch 158. Skipping...\nWarning: NaN or Inf loss at batch 159. Skipping...\nWarning: NaN or Inf loss at batch 160. Skipping...\nWarning: NaN or Inf loss at batch 161. Skipping...\nWarning: NaN or Inf loss at batch 162. Skipping...\nWarning: NaN or Inf loss at batch 163. Skipping...\nWarning: NaN or Inf loss at batch 164. Skipping...\nWarning: NaN or Inf loss at batch 165. Skipping...\nWarning: NaN or Inf loss at batch 166. Skipping...\nWarning: NaN or Inf loss at batch 167. Skipping...\nWarning: NaN or Inf loss at batch 168. Skipping...\nWarning: NaN or Inf loss at batch 169. Skipping...\nWarning: NaN or Inf loss at batch 170. Skipping...\nWarning: NaN or Inf loss at batch 171. Skipping...\nWarning: NaN or Inf loss at batch 172. Skipping...\nWarning: NaN or Inf loss at batch 173. Skipping...\nWarning: NaN or Inf loss at batch 174. Skipping...\nWarning: NaN or Inf loss at batch 175. Skipping...\nWarning: NaN or Inf loss at batch 176. Skipping...\nWarning: NaN or Inf loss at batch 177. Skipping...\nWarning: NaN or Inf loss at batch 178. Skipping...\nWarning: NaN or Inf loss at batch 179. Skipping...\nWarning: NaN or Inf loss at batch 180. Skipping...\nWarning: NaN or Inf loss at batch 181. Skipping...\nWarning: NaN or Inf loss at batch 182. Skipping...\nWarning: NaN or Inf loss at batch 183. Skipping...\nWarning: NaN or Inf loss at batch 184. Skipping...\nWarning: NaN or Inf loss at batch 185. Skipping...\nWarning: NaN or Inf loss at batch 186. Skipping...\nWarning: NaN or Inf loss at batch 187. Skipping...\nWarning: NaN or Inf loss at batch 188. Skipping...\nWarning: NaN or Inf loss at batch 189. Skipping...\nWarning: NaN or Inf loss at batch 190. Skipping...\nWarning: NaN or Inf loss at batch 191. Skipping...\nWarning: NaN or Inf loss at batch 192. Skipping...\nWarning: NaN or Inf loss at batch 193. Skipping...\nWarning: NaN or Inf loss at batch 194. Skipping...\nWarning: NaN or Inf loss at batch 195. Skipping...\nWarning: NaN or Inf loss at batch 196. Skipping...\nWarning: NaN or Inf loss at batch 197. Skipping...\nWarning: NaN or Inf loss at batch 198. Skipping...\nWarning: NaN or Inf loss at batch 199. Skipping...\nWarning: NaN or Inf loss at batch 200. Skipping...\nWarning: NaN or Inf loss at batch 201. Skipping...\nWarning: NaN or Inf loss at batch 202. Skipping...\nWarning: NaN or Inf loss at batch 203. Skipping...\nWarning: NaN or Inf loss at batch 204. Skipping...\nWarning: NaN or Inf loss at batch 205. Skipping...\nWarning: NaN or Inf loss at batch 206. Skipping...\nWarning: NaN or Inf loss at batch 207. Skipping...\nWarning: NaN or Inf loss at batch 208. Skipping...\nWarning: NaN or Inf loss at batch 209. Skipping...\nWarning: NaN or Inf loss at batch 210. Skipping...\nWarning: NaN or Inf loss at batch 211. Skipping...\nWarning: NaN or Inf loss at batch 212. Skipping...\nWarning: NaN or Inf loss at batch 213. Skipping...\nWarning: NaN or Inf loss at batch 214. Skipping...\nWarning: NaN or Inf loss at batch 215. Skipping...\nWarning: NaN or Inf loss at batch 216. Skipping...\nWarning: NaN or Inf loss at batch 217. Skipping...\nWarning: NaN or Inf loss at batch 218. Skipping...\nWarning: NaN or Inf loss at batch 219. Skipping...\nWarning: NaN or Inf loss at batch 220. Skipping...\nWarning: NaN or Inf loss at batch 221. Skipping...\nWarning: NaN or Inf loss at batch 222. Skipping...\nWarning: NaN or Inf loss at batch 223. Skipping...\nWarning: NaN or Inf loss at batch 224. Skipping...\nWarning: NaN or Inf loss at batch 225. Skipping...\nWarning: NaN or Inf loss at batch 226. Skipping...\nWarning: NaN or Inf loss at batch 227. Skipping...\nWarning: NaN or Inf loss at batch 228. Skipping...\nWarning: NaN or Inf loss at batch 229. Skipping...\nWarning: NaN or Inf loss at batch 230. Skipping...\nWarning: NaN or Inf loss at batch 231. Skipping...\nWarning: NaN or Inf loss at batch 232. Skipping...\nWarning: NaN or Inf loss at batch 233. Skipping...\nWarning: NaN or Inf loss at batch 234. Skipping...\nWarning: NaN or Inf loss at batch 235. Skipping...\nWarning: NaN or Inf loss at batch 236. Skipping...\nWarning: NaN or Inf loss at batch 237. Skipping...\nWarning: NaN or Inf loss at batch 238. Skipping...\nWarning: NaN or Inf loss at batch 239. Skipping...\nWarning: NaN or Inf loss at batch 240. Skipping...\nWarning: NaN or Inf loss at batch 241. Skipping...\nWarning: NaN or Inf loss at batch 242. Skipping...\nWarning: NaN or Inf loss at batch 243. Skipping...\nWarning: NaN or Inf loss at batch 244. Skipping...\nWarning: NaN or Inf loss at batch 245. Skipping...\nWarning: NaN or Inf loss at batch 246. Skipping...\nWarning: NaN or Inf loss at batch 247. Skipping...\nWarning: NaN or Inf loss at batch 248. Skipping...\nWarning: NaN or Inf loss at batch 249. Skipping...\nWarning: NaN or Inf loss at batch 250. Skipping...\nWarning: NaN or Inf loss at batch 251. Skipping...\nWarning: NaN or Inf loss at batch 252. Skipping...\nWarning: NaN or Inf loss at batch 253. Skipping...\nWarning: NaN or Inf loss at batch 254. Skipping...\nWarning: NaN or Inf loss at batch 255. Skipping...\nEpoch 18/20, Loss: 0.0000\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nWarning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\nToken distribution (Batch 0): {41: 18}, Pred length: 18\nToken distribution (Batch 1): {41: 20}, Pred length: 20\nToken distribution (Batch 2): {41: 14}, Pred length: 14\nToken distribution (Batch 3): {41: 16}, Pred length: 16\nToken distribution (Batch 4): {41: 24}, Pred length: 24\nToken distribution (Batch 5): {41: 14}, Pred length: 14\nToken distribution (Batch 6): {41: 20}, Pred length: 20\nToken distribution (Batch 7): {41: 24}, Pred length: 24\nToken distribution (Batch 8): {41: 16}, Pred length: 16\nToken distribution (Batch 9): {41: 22}, Pred length: 22\nToken distribution (Batch 10): {41: 20}, Pred length: 20\nToken distribution (Batch 11): {41: 8}, Pred length: 8\nToken distribution (Batch 12): {41: 18}, Pred length: 18\nToken distribution (Batch 13): {41: 10}, Pred length: 10\nToken distribution (Batch 14): {41: 20}, Pred length: 20\nToken distribution (Batch 15): {41: 20}, Pred length: 20\nToken distribution (Batch 16): {41: 24}, Pred length: 24\nToken distribution (Batch 17): {41: 24}, Pred length: 24\nToken distribution (Batch 18): {41: 14}, Pred length: 14\nToken distribution (Batch 19): {41: 16}, Pred length: 16\nToken distribution (Batch 20): {41: 10}, Pred length: 10\nToken distribution (Batch 21): {41: 10}, Pred length: 10\nToken distribution (Batch 22): {41: 24}, Pred length: 24\nToken distribution (Batch 23): {41: 16}, Pred length: 16\nToken distribution (Batch 24): {41: 12}, Pred length: 12\nToken distribution (Batch 25): {41: 12}, Pred length: 12\nToken distribution (Batch 26): {41: 8}, Pred length: 8\nToken distribution (Batch 27): {41: 12}, Pred length: 12\nToken distribution (Batch 28): {41: 22}, Pred length: 22\nToken distribution (Batch 29): {41: 14}, Pred length: 14\nToken distribution (Batch 30): {41: 14}, Pred length: 14\nToken distribution (Batch 31): {41: 12}, Pred length: 12\nValidation Loss: nan\nValidation Predictions: ['O', 'O', 'O', 'O', 'O']\nGround Truth: ['cQXuk-qSe', 'Pp914vFqda', '8e1JdCZ', 'Tfr8yrX*', 'P1WEI6*e5oTI']\nCurrent Learning Rate: 1.3750000000000002e-06\nEpoch 19, Filtered data size: 10240, Sample labels: ['qV8ib8H', 'MdI8', '1XS0DRdJ']\nTrain size: 8192, Val size: 2048\nWarning: NaN or Inf loss at batch 0. Skipping...\nWarning: NaN or Inf loss at batch 1. Skipping...\nWarning: NaN or Inf loss at batch 2. Skipping...\nWarning: NaN or Inf loss at batch 3. Skipping...\nWarning: NaN or Inf loss at batch 4. Skipping...\nWarning: NaN or Inf loss at batch 5. Skipping...\nWarning: NaN or Inf loss at batch 6. Skipping...\nWarning: NaN or Inf loss at batch 7. Skipping...\nWarning: NaN or Inf loss at batch 8. Skipping...\nWarning: NaN or Inf loss at batch 9. Skipping...\nWarning: NaN or Inf loss at batch 10. Skipping...\nWarning: NaN or Inf loss at batch 11. Skipping...\nWarning: NaN or Inf loss at batch 12. Skipping...\nWarning: NaN or Inf loss at batch 13. Skipping...\nWarning: NaN or Inf loss at batch 14. Skipping...\nWarning: NaN or Inf loss at batch 15. Skipping...\nWarning: NaN or Inf loss at batch 16. Skipping...\nWarning: NaN or Inf loss at batch 17. Skipping...\nWarning: NaN or Inf loss at batch 18. Skipping...\nWarning: NaN or Inf loss at batch 19. Skipping...\nWarning: NaN or Inf loss at batch 20. Skipping...\nWarning: NaN or Inf loss at batch 21. Skipping...\nWarning: NaN or Inf loss at batch 22. Skipping...\nWarning: NaN or Inf loss at batch 23. Skipping...\nWarning: NaN or Inf loss at batch 24. Skipping...\nWarning: NaN or Inf loss at batch 25. Skipping...\nWarning: NaN or Inf loss at batch 26. Skipping...\nWarning: NaN or Inf loss at batch 27. Skipping...\nWarning: NaN or Inf loss at batch 28. Skipping...\nWarning: NaN or Inf loss at batch 29. Skipping...\nWarning: NaN or Inf loss at batch 30. Skipping...\nWarning: NaN or Inf loss at batch 31. Skipping...\nWarning: NaN or Inf loss at batch 32. Skipping...\nWarning: NaN or Inf loss at batch 33. Skipping...\nWarning: NaN or Inf loss at batch 34. Skipping...\nWarning: NaN or Inf loss at batch 35. Skipping...\nWarning: NaN or Inf loss at batch 36. Skipping...\nWarning: NaN or Inf loss at batch 37. Skipping...\nWarning: NaN or Inf loss at batch 38. Skipping...\nWarning: NaN or Inf loss at batch 39. Skipping...\nWarning: NaN or Inf loss at batch 40. Skipping...\nWarning: NaN or Inf loss at batch 41. Skipping...\nWarning: NaN or Inf loss at batch 42. Skipping...\nWarning: NaN or Inf loss at batch 43. Skipping...\nWarning: NaN or Inf loss at batch 44. Skipping...\nWarning: NaN or Inf loss at batch 45. Skipping...\nWarning: NaN or Inf loss at batch 46. Skipping...\nWarning: NaN or Inf loss at batch 47. Skipping...\nWarning: NaN or Inf loss at batch 48. Skipping...\nWarning: NaN or Inf loss at batch 49. Skipping...\nWarning: NaN or Inf loss at batch 50. Skipping...\nWarning: NaN or Inf loss at batch 51. Skipping...\nWarning: NaN or Inf loss at batch 52. Skipping...\nWarning: NaN or Inf loss at batch 53. Skipping...\nWarning: NaN or Inf loss at batch 54. Skipping...\nWarning: NaN or Inf loss at batch 55. Skipping...\nWarning: NaN or Inf loss at batch 56. Skipping...\nWarning: NaN or Inf loss at batch 57. Skipping...\nWarning: NaN or Inf loss at batch 58. Skipping...\nWarning: NaN or Inf loss at batch 59. Skipping...\nWarning: NaN or Inf loss at batch 60. Skipping...\nWarning: NaN or Inf loss at batch 61. Skipping...\nWarning: NaN or Inf loss at batch 62. Skipping...\nWarning: NaN or Inf loss at batch 63. Skipping...\nWarning: NaN or Inf loss at batch 64. Skipping...\nWarning: NaN or Inf loss at batch 65. Skipping...\nWarning: NaN or Inf loss at batch 66. Skipping...\nWarning: NaN or Inf loss at batch 67. Skipping...\nWarning: NaN or Inf loss at batch 68. Skipping...\nWarning: NaN or Inf loss at batch 69. Skipping...\nWarning: NaN or Inf loss at batch 70. Skipping...\nWarning: NaN or Inf loss at batch 71. Skipping...\nWarning: NaN or Inf loss at batch 72. Skipping...\nWarning: NaN or Inf loss at batch 73. Skipping...\nWarning: NaN or Inf loss at batch 74. Skipping...\nWarning: NaN or Inf loss at batch 75. Skipping...\nWarning: NaN or Inf loss at batch 76. Skipping...\nWarning: NaN or Inf loss at batch 77. Skipping...\nWarning: NaN or Inf loss at batch 78. Skipping...\nWarning: NaN or Inf loss at batch 79. Skipping...\nWarning: NaN or Inf loss at batch 80. Skipping...\nWarning: NaN or Inf loss at batch 81. Skipping...\nWarning: NaN or Inf loss at batch 82. Skipping...\nWarning: NaN or Inf loss at batch 83. Skipping...\nWarning: NaN or Inf loss at batch 84. Skipping...\nWarning: NaN or Inf loss at batch 85. Skipping...\nWarning: NaN or Inf loss at batch 86. Skipping...\nWarning: NaN or Inf loss at batch 87. Skipping...\nWarning: NaN or Inf loss at batch 88. Skipping...\nWarning: NaN or Inf loss at batch 89. Skipping...\nWarning: NaN or Inf loss at batch 90. Skipping...\nWarning: NaN or Inf loss at batch 91. Skipping...\nWarning: NaN or Inf loss at batch 92. Skipping...\nWarning: NaN or Inf loss at batch 93. Skipping...\nWarning: NaN or Inf loss at batch 94. Skipping...\nWarning: NaN or Inf loss at batch 95. Skipping...\nWarning: NaN or Inf loss at batch 96. Skipping...\nWarning: NaN or Inf loss at batch 97. Skipping...\nWarning: NaN or Inf loss at batch 98. Skipping...\nWarning: NaN or Inf loss at batch 99. Skipping...\nWarning: NaN or Inf loss at batch 100. Skipping...\nWarning: NaN or Inf loss at batch 101. Skipping...\nWarning: NaN or Inf loss at batch 102. Skipping...\nWarning: NaN or Inf loss at batch 103. Skipping...\nWarning: NaN or Inf loss at batch 104. Skipping...\nWarning: NaN or Inf loss at batch 105. Skipping...\nWarning: NaN or Inf loss at batch 106. Skipping...\nWarning: NaN or Inf loss at batch 107. Skipping...\nWarning: NaN or Inf loss at batch 108. Skipping...\nWarning: NaN or Inf loss at batch 109. Skipping...\nWarning: NaN or Inf loss at batch 110. Skipping...\nWarning: NaN or Inf loss at batch 111. Skipping...\nWarning: NaN or Inf loss at batch 112. Skipping...\nWarning: NaN or Inf loss at batch 113. Skipping...\nWarning: NaN or Inf loss at batch 114. Skipping...\nWarning: NaN or Inf loss at batch 115. Skipping...\nWarning: NaN or Inf loss at batch 116. Skipping...\nWarning: NaN or Inf loss at batch 117. Skipping...\nWarning: NaN or Inf loss at batch 118. Skipping...\nWarning: NaN or Inf loss at batch 119. Skipping...\nWarning: NaN or Inf loss at batch 120. Skipping...\nWarning: NaN or Inf loss at batch 121. Skipping...\nWarning: NaN or Inf loss at batch 122. Skipping...\nWarning: NaN or Inf loss at batch 123. Skipping...\nWarning: NaN or Inf loss at batch 124. Skipping...\nWarning: NaN or Inf loss at batch 125. Skipping...\nWarning: NaN or Inf loss at batch 126. Skipping...\nWarning: NaN or Inf loss at batch 127. Skipping...\nWarning: NaN or Inf loss at batch 128. Skipping...\nWarning: NaN or Inf loss at batch 129. Skipping...\nWarning: NaN or Inf loss at batch 130. Skipping...\nWarning: NaN or Inf loss at batch 131. Skipping...\nWarning: NaN or Inf loss at batch 132. Skipping...\nWarning: NaN or Inf loss at batch 133. Skipping...\nWarning: NaN or Inf loss at batch 134. Skipping...\nWarning: NaN or Inf loss at batch 135. Skipping...\nWarning: NaN or Inf loss at batch 136. Skipping...\nWarning: NaN or Inf loss at batch 137. Skipping...\nWarning: NaN or Inf loss at batch 138. Skipping...\nWarning: NaN or Inf loss at batch 139. Skipping...\nWarning: NaN or Inf loss at batch 140. Skipping...\nWarning: NaN or Inf loss at batch 141. Skipping...\nWarning: NaN or Inf loss at batch 142. Skipping...\nWarning: NaN or Inf loss at batch 143. Skipping...\nWarning: NaN or Inf loss at batch 144. Skipping...\nWarning: NaN or Inf loss at batch 145. Skipping...\nWarning: NaN or Inf loss at batch 146. Skipping...\nWarning: NaN or Inf loss at batch 147. Skipping...\nWarning: NaN or Inf loss at batch 148. Skipping...\nWarning: NaN or Inf loss at batch 149. Skipping...\nWarning: NaN or Inf loss at batch 150. Skipping...\nWarning: NaN or Inf loss at batch 151. Skipping...\nWarning: NaN or Inf loss at batch 152. Skipping...\nWarning: NaN or Inf loss at batch 153. Skipping...\nWarning: NaN or Inf loss at batch 154. Skipping...\nWarning: NaN or Inf loss at batch 155. Skipping...\nWarning: NaN or Inf loss at batch 156. Skipping...\nWarning: NaN or Inf loss at batch 157. Skipping...\nWarning: NaN or Inf loss at batch 158. Skipping...\nWarning: NaN or Inf loss at batch 159. Skipping...\nWarning: NaN or Inf loss at batch 160. Skipping...\nWarning: NaN or Inf loss at batch 161. Skipping...\nWarning: NaN or Inf loss at batch 162. Skipping...\nWarning: NaN or Inf loss at batch 163. Skipping...\nWarning: NaN or Inf loss at batch 164. Skipping...\nWarning: NaN or Inf loss at batch 165. Skipping...\nWarning: NaN or Inf loss at batch 166. Skipping...\nWarning: NaN or Inf loss at batch 167. Skipping...\nWarning: NaN or Inf loss at batch 168. Skipping...\nWarning: NaN or Inf loss at batch 169. Skipping...\nWarning: NaN or Inf loss at batch 170. Skipping...\nWarning: NaN or Inf loss at batch 171. Skipping...\nWarning: NaN or Inf loss at batch 172. Skipping...\nWarning: NaN or Inf loss at batch 173. Skipping...\nWarning: NaN or Inf loss at batch 174. Skipping...\nWarning: NaN or Inf loss at batch 175. Skipping...\nWarning: NaN or Inf loss at batch 176. Skipping...\nWarning: NaN or Inf loss at batch 177. Skipping...\nWarning: NaN or Inf loss at batch 178. Skipping...\nWarning: NaN or Inf loss at batch 179. Skipping...\nWarning: NaN or Inf loss at batch 180. Skipping...\nWarning: NaN or Inf loss at batch 181. Skipping...\nWarning: NaN or Inf loss at batch 182. Skipping...\nWarning: NaN or Inf loss at batch 183. Skipping...\nWarning: NaN or Inf loss at batch 184. Skipping...\nWarning: NaN or Inf loss at batch 185. Skipping...\nWarning: NaN or Inf loss at batch 186. Skipping...\nWarning: NaN or Inf loss at batch 187. Skipping...\nWarning: NaN or Inf loss at batch 188. Skipping...\nWarning: NaN or Inf loss at batch 189. Skipping...\nWarning: NaN or Inf loss at batch 190. Skipping...\nWarning: NaN or Inf loss at batch 191. Skipping...\nWarning: NaN or Inf loss at batch 192. Skipping...\nWarning: NaN or Inf loss at batch 193. Skipping...\nWarning: NaN or Inf loss at batch 194. Skipping...\nWarning: NaN or Inf loss at batch 195. Skipping...\nWarning: NaN or Inf loss at batch 196. Skipping...\nWarning: NaN or Inf loss at batch 197. Skipping...\nWarning: NaN or Inf loss at batch 198. Skipping...\nWarning: NaN or Inf loss at batch 199. Skipping...\nWarning: NaN or Inf loss at batch 200. Skipping...\nWarning: NaN or Inf loss at batch 201. Skipping...\nWarning: NaN or Inf loss at batch 202. Skipping...\nWarning: NaN or Inf loss at batch 203. Skipping...\nWarning: NaN or Inf loss at batch 204. Skipping...\nWarning: NaN or Inf loss at batch 205. Skipping...\nWarning: NaN or Inf loss at batch 206. Skipping...\nWarning: NaN or Inf loss at batch 207. Skipping...\nWarning: NaN or Inf loss at batch 208. Skipping...\nWarning: NaN or Inf loss at batch 209. Skipping...\nWarning: NaN or Inf loss at batch 210. Skipping...\nWarning: NaN or Inf loss at batch 211. Skipping...\nWarning: NaN or Inf loss at batch 212. Skipping...\nWarning: NaN or Inf loss at batch 213. Skipping...\nWarning: NaN or Inf loss at batch 214. Skipping...\nWarning: NaN or Inf loss at batch 215. Skipping...\nWarning: NaN or Inf loss at batch 216. Skipping...\nWarning: NaN or Inf loss at batch 217. Skipping...\nWarning: NaN or Inf loss at batch 218. Skipping...\nWarning: NaN or Inf loss at batch 219. Skipping...\nWarning: NaN or Inf loss at batch 220. Skipping...\nWarning: NaN or Inf loss at batch 221. Skipping...\nWarning: NaN or Inf loss at batch 222. Skipping...\nWarning: NaN or Inf loss at batch 223. Skipping...\nWarning: NaN or Inf loss at batch 224. Skipping...\nWarning: NaN or Inf loss at batch 225. Skipping...\nWarning: NaN or Inf loss at batch 226. Skipping...\nWarning: NaN or Inf loss at batch 227. Skipping...\nWarning: NaN or Inf loss at batch 228. Skipping...\nWarning: NaN or Inf loss at batch 229. Skipping...\nWarning: NaN or Inf loss at batch 230. Skipping...\nWarning: NaN or Inf loss at batch 231. Skipping...\nWarning: NaN or Inf loss at batch 232. Skipping...\nWarning: NaN or Inf loss at batch 233. Skipping...\nWarning: NaN or Inf loss at batch 234. Skipping...\nWarning: NaN or Inf loss at batch 235. Skipping...\nWarning: NaN or Inf loss at batch 236. Skipping...\nWarning: NaN or Inf loss at batch 237. Skipping...\nWarning: NaN or Inf loss at batch 238. Skipping...\nWarning: NaN or Inf loss at batch 239. Skipping...\nWarning: NaN or Inf loss at batch 240. Skipping...\nWarning: NaN or Inf loss at batch 241. Skipping...\nWarning: NaN or Inf loss at batch 242. Skipping...\nWarning: NaN or Inf loss at batch 243. Skipping...\nWarning: NaN or Inf loss at batch 244. Skipping...\nWarning: NaN or Inf loss at batch 245. Skipping...\nWarning: NaN or Inf loss at batch 246. Skipping...\nWarning: NaN or Inf loss at batch 247. Skipping...\nWarning: NaN or Inf loss at batch 248. Skipping...\nWarning: NaN or Inf loss at batch 249. Skipping...\nWarning: NaN or Inf loss at batch 250. Skipping...\nWarning: NaN or Inf loss at batch 251. Skipping...\nWarning: NaN or Inf loss at batch 252. Skipping...\nWarning: NaN or Inf loss at batch 253. Skipping...\nWarning: NaN or Inf loss at batch 254. Skipping...\nWarning: NaN or Inf loss at batch 255. Skipping...\nEpoch 19/20, Loss: 0.0000\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nWarning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\nToken distribution (Batch 0): {41: 22}, Pred length: 22\nToken distribution (Batch 1): {41: 22}, Pred length: 22\nToken distribution (Batch 2): {41: 16}, Pred length: 16\nToken distribution (Batch 3): {41: 14}, Pred length: 14\nToken distribution (Batch 4): {41: 10}, Pred length: 10\nToken distribution (Batch 5): {41: 10}, Pred length: 10\nToken distribution (Batch 6): {41: 16}, Pred length: 16\nToken distribution (Batch 7): {41: 14}, Pred length: 14\nToken distribution (Batch 8): {41: 22}, Pred length: 22\nToken distribution (Batch 9): {41: 24}, Pred length: 24\nToken distribution (Batch 10): {41: 12}, Pred length: 12\nToken distribution (Batch 11): {41: 10}, Pred length: 10\nToken distribution (Batch 12): {41: 14}, Pred length: 14\nToken distribution (Batch 13): {41: 16}, Pred length: 16\nToken distribution (Batch 14): {41: 14}, Pred length: 14\nToken distribution (Batch 15): {41: 24}, Pred length: 24\nToken distribution (Batch 16): {41: 20}, Pred length: 20\nToken distribution (Batch 17): {41: 10}, Pred length: 10\nToken distribution (Batch 18): {41: 10}, Pred length: 10\nToken distribution (Batch 19): {41: 10}, Pred length: 10\nToken distribution (Batch 20): {41: 10}, Pred length: 10\nToken distribution (Batch 21): {41: 14}, Pred length: 14\nToken distribution (Batch 22): {41: 8}, Pred length: 8\nToken distribution (Batch 23): {41: 22}, Pred length: 22\nToken distribution (Batch 24): {41: 16}, Pred length: 16\nToken distribution (Batch 25): {41: 14}, Pred length: 14\nToken distribution (Batch 26): {41: 10}, Pred length: 10\nToken distribution (Batch 27): {41: 8}, Pred length: 8\nToken distribution (Batch 28): {41: 16}, Pred length: 16\nToken distribution (Batch 29): {41: 12}, Pred length: 12\nToken distribution (Batch 30): {41: 20}, Pred length: 20\nToken distribution (Batch 31): {41: 24}, Pred length: 24\nValidation Loss: nan\nValidation Predictions: ['O', 'O', 'O', 'O', 'O']\nGround Truth: ['0hcYSXUhVS6', 'ifg5qTIUaWd', 'bVjyrG-z', 'jxG9J9G', '27ZVf']\nCurrent Learning Rate: 6.875000000000001e-07\nEpoch 20, Filtered data size: 10240, Sample labels: ['qV8ib8H', 'MdI8', '1XS0DRdJ']\nTrain size: 8192, Val size: 2048\nWarning: NaN or Inf loss at batch 0. Skipping...\nWarning: NaN or Inf loss at batch 1. Skipping...\nWarning: NaN or Inf loss at batch 2. Skipping...\nWarning: NaN or Inf loss at batch 3. Skipping...\nWarning: NaN or Inf loss at batch 4. Skipping...\nWarning: NaN or Inf loss at batch 5. Skipping...\nWarning: NaN or Inf loss at batch 6. Skipping...\nWarning: NaN or Inf loss at batch 7. Skipping...\nWarning: NaN or Inf loss at batch 8. Skipping...\nWarning: NaN or Inf loss at batch 9. Skipping...\nWarning: NaN or Inf loss at batch 10. Skipping...\nWarning: NaN or Inf loss at batch 11. Skipping...\nWarning: NaN or Inf loss at batch 12. Skipping...\nWarning: NaN or Inf loss at batch 13. Skipping...\nWarning: NaN or Inf loss at batch 14. Skipping...\nWarning: NaN or Inf loss at batch 15. Skipping...\nWarning: NaN or Inf loss at batch 16. Skipping...\nWarning: NaN or Inf loss at batch 17. Skipping...\nWarning: NaN or Inf loss at batch 18. Skipping...\nWarning: NaN or Inf loss at batch 19. Skipping...\nWarning: NaN or Inf loss at batch 20. Skipping...\nWarning: NaN or Inf loss at batch 21. Skipping...\nWarning: NaN or Inf loss at batch 22. Skipping...\nWarning: NaN or Inf loss at batch 23. Skipping...\nWarning: NaN or Inf loss at batch 24. Skipping...\nWarning: NaN or Inf loss at batch 25. Skipping...\nWarning: NaN or Inf loss at batch 26. Skipping...\nWarning: NaN or Inf loss at batch 27. Skipping...\nWarning: NaN or Inf loss at batch 28. Skipping...\nWarning: NaN or Inf loss at batch 29. Skipping...\nWarning: NaN or Inf loss at batch 30. Skipping...\nWarning: NaN or Inf loss at batch 31. Skipping...\nWarning: NaN or Inf loss at batch 32. Skipping...\nWarning: NaN or Inf loss at batch 33. Skipping...\nWarning: NaN or Inf loss at batch 34. Skipping...\nWarning: NaN or Inf loss at batch 35. Skipping...\nWarning: NaN or Inf loss at batch 36. Skipping...\nWarning: NaN or Inf loss at batch 37. Skipping...\nWarning: NaN or Inf loss at batch 38. Skipping...\nWarning: NaN or Inf loss at batch 39. Skipping...\nWarning: NaN or Inf loss at batch 40. Skipping...\nWarning: NaN or Inf loss at batch 41. Skipping...\nWarning: NaN or Inf loss at batch 42. Skipping...\nWarning: NaN or Inf loss at batch 43. Skipping...\nWarning: NaN or Inf loss at batch 44. Skipping...\nWarning: NaN or Inf loss at batch 45. Skipping...\nWarning: NaN or Inf loss at batch 46. Skipping...\nWarning: NaN or Inf loss at batch 47. Skipping...\nWarning: NaN or Inf loss at batch 48. Skipping...\nWarning: NaN or Inf loss at batch 49. Skipping...\nWarning: NaN or Inf loss at batch 50. Skipping...\nWarning: NaN or Inf loss at batch 51. Skipping...\nWarning: NaN or Inf loss at batch 52. Skipping...\nWarning: NaN or Inf loss at batch 53. Skipping...\nWarning: NaN or Inf loss at batch 54. Skipping...\nWarning: NaN or Inf loss at batch 55. Skipping...\nWarning: NaN or Inf loss at batch 56. Skipping...\nWarning: NaN or Inf loss at batch 57. Skipping...\nWarning: NaN or Inf loss at batch 58. Skipping...\nWarning: NaN or Inf loss at batch 59. Skipping...\nWarning: NaN or Inf loss at batch 60. Skipping...\nWarning: NaN or Inf loss at batch 61. Skipping...\nWarning: NaN or Inf loss at batch 62. Skipping...\nWarning: NaN or Inf loss at batch 63. Skipping...\nWarning: NaN or Inf loss at batch 64. Skipping...\nWarning: NaN or Inf loss at batch 65. Skipping...\nWarning: NaN or Inf loss at batch 66. Skipping...\nWarning: NaN or Inf loss at batch 67. Skipping...\nWarning: NaN or Inf loss at batch 68. Skipping...\nWarning: NaN or Inf loss at batch 69. Skipping...\nWarning: NaN or Inf loss at batch 70. Skipping...\nWarning: NaN or Inf loss at batch 71. Skipping...\nWarning: NaN or Inf loss at batch 72. Skipping...\nWarning: NaN or Inf loss at batch 73. Skipping...\nWarning: NaN or Inf loss at batch 74. Skipping...\nWarning: NaN or Inf loss at batch 75. Skipping...\nWarning: NaN or Inf loss at batch 76. Skipping...\nWarning: NaN or Inf loss at batch 77. Skipping...\nWarning: NaN or Inf loss at batch 78. Skipping...\nWarning: NaN or Inf loss at batch 79. Skipping...\nWarning: NaN or Inf loss at batch 80. Skipping...\nWarning: NaN or Inf loss at batch 81. Skipping...\nWarning: NaN or Inf loss at batch 82. Skipping...\nWarning: NaN or Inf loss at batch 83. Skipping...\nWarning: NaN or Inf loss at batch 84. Skipping...\nWarning: NaN or Inf loss at batch 85. Skipping...\nWarning: NaN or Inf loss at batch 86. Skipping...\nWarning: NaN or Inf loss at batch 87. Skipping...\nWarning: NaN or Inf loss at batch 88. Skipping...\nWarning: NaN or Inf loss at batch 89. Skipping...\nWarning: NaN or Inf loss at batch 90. Skipping...\nWarning: NaN or Inf loss at batch 91. Skipping...\nWarning: NaN or Inf loss at batch 92. Skipping...\nWarning: NaN or Inf loss at batch 93. Skipping...\nWarning: NaN or Inf loss at batch 94. Skipping...\nWarning: NaN or Inf loss at batch 95. Skipping...\nWarning: NaN or Inf loss at batch 96. Skipping...\nWarning: NaN or Inf loss at batch 97. Skipping...\nWarning: NaN or Inf loss at batch 98. Skipping...\nWarning: NaN or Inf loss at batch 99. Skipping...\nWarning: NaN or Inf loss at batch 100. Skipping...\nWarning: NaN or Inf loss at batch 101. Skipping...\nWarning: NaN or Inf loss at batch 102. Skipping...\nWarning: NaN or Inf loss at batch 103. Skipping...\nWarning: NaN or Inf loss at batch 104. Skipping...\nWarning: NaN or Inf loss at batch 105. Skipping...\nWarning: NaN or Inf loss at batch 106. Skipping...\nWarning: NaN or Inf loss at batch 107. Skipping...\nWarning: NaN or Inf loss at batch 108. Skipping...\nWarning: NaN or Inf loss at batch 109. Skipping...\nWarning: NaN or Inf loss at batch 110. Skipping...\nWarning: NaN or Inf loss at batch 111. Skipping...\nWarning: NaN or Inf loss at batch 112. Skipping...\nWarning: NaN or Inf loss at batch 113. Skipping...\nWarning: NaN or Inf loss at batch 114. Skipping...\nWarning: NaN or Inf loss at batch 115. Skipping...\nWarning: NaN or Inf loss at batch 116. Skipping...\nWarning: NaN or Inf loss at batch 117. Skipping...\nWarning: NaN or Inf loss at batch 118. Skipping...\nWarning: NaN or Inf loss at batch 119. Skipping...\nWarning: NaN or Inf loss at batch 120. Skipping...\nWarning: NaN or Inf loss at batch 121. Skipping...\nWarning: NaN or Inf loss at batch 122. Skipping...\nWarning: NaN or Inf loss at batch 123. Skipping...\nWarning: NaN or Inf loss at batch 124. Skipping...\nWarning: NaN or Inf loss at batch 125. Skipping...\nWarning: NaN or Inf loss at batch 126. Skipping...\nWarning: NaN or Inf loss at batch 127. Skipping...\nWarning: NaN or Inf loss at batch 128. Skipping...\nWarning: NaN or Inf loss at batch 129. Skipping...\nWarning: NaN or Inf loss at batch 130. Skipping...\nWarning: NaN or Inf loss at batch 131. Skipping...\nWarning: NaN or Inf loss at batch 132. Skipping...\nWarning: NaN or Inf loss at batch 133. Skipping...\nWarning: NaN or Inf loss at batch 134. Skipping...\nWarning: NaN or Inf loss at batch 135. Skipping...\nWarning: NaN or Inf loss at batch 136. Skipping...\nWarning: NaN or Inf loss at batch 137. Skipping...\nWarning: NaN or Inf loss at batch 138. Skipping...\nWarning: NaN or Inf loss at batch 139. Skipping...\nWarning: NaN or Inf loss at batch 140. Skipping...\nWarning: NaN or Inf loss at batch 141. Skipping...\nWarning: NaN or Inf loss at batch 142. Skipping...\nWarning: NaN or Inf loss at batch 143. Skipping...\nWarning: NaN or Inf loss at batch 144. Skipping...\nWarning: NaN or Inf loss at batch 145. Skipping...\nWarning: NaN or Inf loss at batch 146. Skipping...\nWarning: NaN or Inf loss at batch 147. Skipping...\nWarning: NaN or Inf loss at batch 148. Skipping...\nWarning: NaN or Inf loss at batch 149. Skipping...\nWarning: NaN or Inf loss at batch 150. Skipping...\nWarning: NaN or Inf loss at batch 151. Skipping...\nWarning: NaN or Inf loss at batch 152. Skipping...\nWarning: NaN or Inf loss at batch 153. Skipping...\nWarning: NaN or Inf loss at batch 154. Skipping...\nWarning: NaN or Inf loss at batch 155. Skipping...\nWarning: NaN or Inf loss at batch 156. Skipping...\nWarning: NaN or Inf loss at batch 157. Skipping...\nWarning: NaN or Inf loss at batch 158. Skipping...\nWarning: NaN or Inf loss at batch 159. Skipping...\nWarning: NaN or Inf loss at batch 160. Skipping...\nWarning: NaN or Inf loss at batch 161. Skipping...\nWarning: NaN or Inf loss at batch 162. Skipping...\nWarning: NaN or Inf loss at batch 163. Skipping...\nWarning: NaN or Inf loss at batch 164. Skipping...\nWarning: NaN or Inf loss at batch 165. Skipping...\nWarning: NaN or Inf loss at batch 166. Skipping...\nWarning: NaN or Inf loss at batch 167. Skipping...\nWarning: NaN or Inf loss at batch 168. Skipping...\nWarning: NaN or Inf loss at batch 169. Skipping...\nWarning: NaN or Inf loss at batch 170. Skipping...\nWarning: NaN or Inf loss at batch 171. Skipping...\nWarning: NaN or Inf loss at batch 172. Skipping...\nWarning: NaN or Inf loss at batch 173. Skipping...\nWarning: NaN or Inf loss at batch 174. Skipping...\nWarning: NaN or Inf loss at batch 175. Skipping...\nWarning: NaN or Inf loss at batch 176. Skipping...\nWarning: NaN or Inf loss at batch 177. Skipping...\nWarning: NaN or Inf loss at batch 178. Skipping...\nWarning: NaN or Inf loss at batch 179. Skipping...\nWarning: NaN or Inf loss at batch 180. Skipping...\nWarning: NaN or Inf loss at batch 181. Skipping...\nWarning: NaN or Inf loss at batch 182. Skipping...\nWarning: NaN or Inf loss at batch 183. Skipping...\nWarning: NaN or Inf loss at batch 184. Skipping...\nWarning: NaN or Inf loss at batch 185. Skipping...\nWarning: NaN or Inf loss at batch 186. Skipping...\nWarning: NaN or Inf loss at batch 187. Skipping...\nWarning: NaN or Inf loss at batch 188. Skipping...\nWarning: NaN or Inf loss at batch 189. Skipping...\nWarning: NaN or Inf loss at batch 190. Skipping...\nWarning: NaN or Inf loss at batch 191. Skipping...\nWarning: NaN or Inf loss at batch 192. Skipping...\nWarning: NaN or Inf loss at batch 193. Skipping...\nWarning: NaN or Inf loss at batch 194. Skipping...\nWarning: NaN or Inf loss at batch 195. Skipping...\nWarning: NaN or Inf loss at batch 196. Skipping...\nWarning: NaN or Inf loss at batch 197. Skipping...\nWarning: NaN or Inf loss at batch 198. Skipping...\nWarning: NaN or Inf loss at batch 199. Skipping...\nWarning: NaN or Inf loss at batch 200. Skipping...\nWarning: NaN or Inf loss at batch 201. Skipping...\nWarning: NaN or Inf loss at batch 202. Skipping...\nWarning: NaN or Inf loss at batch 203. Skipping...\nWarning: NaN or Inf loss at batch 204. Skipping...\nWarning: NaN or Inf loss at batch 205. Skipping...\nWarning: NaN or Inf loss at batch 206. Skipping...\nWarning: NaN or Inf loss at batch 207. Skipping...\nWarning: NaN or Inf loss at batch 208. Skipping...\nWarning: NaN or Inf loss at batch 209. Skipping...\nWarning: NaN or Inf loss at batch 210. Skipping...\nWarning: NaN or Inf loss at batch 211. Skipping...\nWarning: NaN or Inf loss at batch 212. Skipping...\nWarning: NaN or Inf loss at batch 213. Skipping...\nWarning: NaN or Inf loss at batch 214. Skipping...\nWarning: NaN or Inf loss at batch 215. Skipping...\nWarning: NaN or Inf loss at batch 216. Skipping...\nWarning: NaN or Inf loss at batch 217. Skipping...\nWarning: NaN or Inf loss at batch 218. Skipping...\nWarning: NaN or Inf loss at batch 219. Skipping...\nWarning: NaN or Inf loss at batch 220. Skipping...\nWarning: NaN or Inf loss at batch 221. Skipping...\nWarning: NaN or Inf loss at batch 222. Skipping...\nWarning: NaN or Inf loss at batch 223. Skipping...\nWarning: NaN or Inf loss at batch 224. Skipping...\nWarning: NaN or Inf loss at batch 225. Skipping...\nWarning: NaN or Inf loss at batch 226. Skipping...\nWarning: NaN or Inf loss at batch 227. Skipping...\nWarning: NaN or Inf loss at batch 228. Skipping...\nWarning: NaN or Inf loss at batch 229. Skipping...\nWarning: NaN or Inf loss at batch 230. Skipping...\nWarning: NaN or Inf loss at batch 231. Skipping...\nWarning: NaN or Inf loss at batch 232. Skipping...\nWarning: NaN or Inf loss at batch 233. Skipping...\nWarning: NaN or Inf loss at batch 234. Skipping...\nWarning: NaN or Inf loss at batch 235. Skipping...\nWarning: NaN or Inf loss at batch 236. Skipping...\nWarning: NaN or Inf loss at batch 237. Skipping...\nWarning: NaN or Inf loss at batch 238. Skipping...\nWarning: NaN or Inf loss at batch 239. Skipping...\nWarning: NaN or Inf loss at batch 240. Skipping...\nWarning: NaN or Inf loss at batch 241. Skipping...\nWarning: NaN or Inf loss at batch 242. Skipping...\nWarning: NaN or Inf loss at batch 243. Skipping...\nWarning: NaN or Inf loss at batch 244. Skipping...\nWarning: NaN or Inf loss at batch 245. Skipping...\nWarning: NaN or Inf loss at batch 246. Skipping...\nWarning: NaN or Inf loss at batch 247. Skipping...\nWarning: NaN or Inf loss at batch 248. Skipping...\nWarning: NaN or Inf loss at batch 249. Skipping...\nWarning: NaN or Inf loss at batch 250. Skipping...\nWarning: NaN or Inf loss at batch 251. Skipping...\nWarning: NaN or Inf loss at batch 252. Skipping...\nWarning: NaN or Inf loss at batch 253. Skipping...\nWarning: NaN or Inf loss at batch 254. Skipping...\nWarning: NaN or Inf loss at batch 255. Skipping...\nEpoch 20/20, Loss: 0.0000\nOutputs shape: torch.Size([32, 32, 64]), Outputs[0] shape: torch.Size([32, 64])\nWarning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\nToken distribution (Batch 0): {41: 24}, Pred length: 24\nToken distribution (Batch 1): {41: 22}, Pred length: 22\nToken distribution (Batch 2): {41: 8}, Pred length: 8\nToken distribution (Batch 3): {41: 20}, Pred length: 20\nToken distribution (Batch 4): {41: 24}, Pred length: 24\nToken distribution (Batch 5): {41: 22}, Pred length: 22\nToken distribution (Batch 6): {41: 20}, Pred length: 20\nToken distribution (Batch 7): {41: 22}, Pred length: 22\nToken distribution (Batch 8): {41: 16}, Pred length: 16\nToken distribution (Batch 9): {41: 24}, Pred length: 24\nToken distribution (Batch 10): {41: 20}, Pred length: 20\nToken distribution (Batch 11): {41: 22}, Pred length: 22\nToken distribution (Batch 12): {41: 22}, Pred length: 22\nToken distribution (Batch 13): {41: 10}, Pred length: 10\nToken distribution (Batch 14): {41: 24}, Pred length: 24\nToken distribution (Batch 15): {41: 22}, Pred length: 22\nToken distribution (Batch 16): {41: 14}, Pred length: 14\nToken distribution (Batch 17): {41: 20}, Pred length: 20\nToken distribution (Batch 18): {41: 12}, Pred length: 12\nToken distribution (Batch 19): {41: 12}, Pred length: 12\nToken distribution (Batch 20): {41: 18}, Pred length: 18\nToken distribution (Batch 21): {41: 12}, Pred length: 12\nToken distribution (Batch 22): {41: 12}, Pred length: 12\nToken distribution (Batch 23): {41: 8}, Pred length: 8\nToken distribution (Batch 24): {41: 20}, Pred length: 20\nToken distribution (Batch 25): {41: 14}, Pred length: 14\nToken distribution (Batch 26): {41: 16}, Pred length: 16\nToken distribution (Batch 27): {41: 22}, Pred length: 22\nToken distribution (Batch 28): {41: 12}, Pred length: 12\nToken distribution (Batch 29): {41: 16}, Pred length: 16\nToken distribution (Batch 30): {41: 22}, Pred length: 22\nToken distribution (Batch 31): {41: 24}, Pred length: 24\nValidation Loss: nan\nValidation Predictions: ['O', 'O', 'O', 'O', 'O']\nGround Truth: ['bgM0Ld5I6JeJ', 'vexDGfpBsCa', 'ksx5', 'c6BEwG9bn9', '1XKz9vdWcuve']\nCurrent Learning Rate: 6.875000000000001e-07\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def zip_folder_with_shutil(source_folder, output_path):\n    '''Function for zip dir data'''\n    shutil.make_archive(output_path, 'zip', source_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T21:21:25.778052Z","iopub.execute_input":"2025-03-11T21:21:25.778382Z","iopub.status.idle":"2025-03-11T21:21:25.783794Z","shell.execute_reply.started":"2025-03-11T21:21:25.778340Z","shell.execute_reply":"2025-03-11T21:21:25.782633Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"zip_folder_with_shutil('/kaggle/working/backgrounds', '/kaggle/working/backgrounds')\nzip_folder_with_shutil('/kaggle/working/synthetic_data', '/kaggle/working/synthetic_data')\nzip_folder_with_shutil('/kaggle/working/model_dir', '/kaggle/working/model_dir')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T21:21:25.785043Z","iopub.execute_input":"2025-03-11T21:21:25.785421Z","iopub.status.idle":"2025-03-11T21:21:29.429899Z","shell.execute_reply.started":"2025-03-11T21:21:25.785376Z","shell.execute_reply":"2025-03-11T21:21:29.428793Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"zip_folder_with_shutil('/kaggle/working/runs', '/kaggle/working/runs')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T21:21:29.431190Z","iopub.execute_input":"2025-03-11T21:21:29.431547Z","iopub.status.idle":"2025-03-11T21:21:29.459924Z","shell.execute_reply.started":"2025-03-11T21:21:29.431517Z","shell.execute_reply":"2025-03-11T21:21:29.458838Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"#!tensorboard --logdir=/kaggle/working/runs --port 6006","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T21:21:29.461037Z","iopub.execute_input":"2025-03-11T21:21:29.461387Z","iopub.status.idle":"2025-03-11T21:21:29.465875Z","shell.execute_reply.started":"2025-03-11T21:21:29.461359Z","shell.execute_reply":"2025-03-11T21:21:29.464685Z"}},"outputs":[],"execution_count":30}]}