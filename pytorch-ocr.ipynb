{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10951507,"sourceType":"datasetVersion","datasetId":6783030}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---------------\n\n# **<font style=\"color:Black\">Create OCR by PyTorch</font>**\n-------------------\n-----------------","metadata":{}},{"cell_type":"code","source":"#!pip install tensorboard\n!tensorboard --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:20:58.034154Z","iopub.execute_input":"2025-03-10T12:20:58.034498Z","iopub.status.idle":"2025-03-10T12:21:17.796493Z","shell.execute_reply.started":"2025-03-10T12:20:58.034463Z","shell.execute_reply":"2025-03-10T12:21:17.795310Z"}},"outputs":[{"name":"stdout","text":"2025-03-10 12:21:02.449542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-10 12:21:02.696509: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-10 12:21:02.769060: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2.17.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nimport shutil\nimport random\nfrom PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter\nimport numpy as np\nimport cv2\n\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F  # Add this import\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:17.798246Z","iopub.execute_input":"2025-03-10T12:21:17.798536Z","iopub.status.idle":"2025-03-10T12:21:29.349016Z","shell.execute_reply.started":"2025-03-10T12:21:17.798509Z","shell.execute_reply":"2025-03-10T12:21:29.347830Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Hyperparameters</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"seed = 3\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:29.350519Z","iopub.execute_input":"2025-03-10T12:21:29.351200Z","iopub.status.idle":"2025-03-10T12:21:29.365014Z","shell.execute_reply.started":"2025-03-10T12:21:29.351160Z","shell.execute_reply":"2025-03-10T12:21:29.363736Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x78967873adf0>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"OUTPUT_DIR = os.path.join('/kaggle','working','synthetic_data','images')\nTENSORBOARD_DIR = os.path.join('/kaggle','working','runs')\nMODEL_DIR = os.path.join('/kaggle','working','model_dir')\nLABELS_FILE = os.path.join('/kaggle','working','synthetic_data','labels.txt')\nNUM_SAMPLES = 5120  # Number of images generated\nIMG_WIDTH = 128\nIMG_HEIGHT = 32\nBATCH_SIZE = 32\nEPOCHS = 20\nLEARNING_RATE = 5e-6 \nWEIGHT_DECAY = 1e-4  \nWARMUP_STEPS = 1000  \nENTROPY_WEIGHT = 2.0\nTEMPERATURE = 0.3\nDROPOUT = 0.7\nBEAM_WIDTH = 10\nLABEL_SMOOTHING = 0.2\nBLANK_PENALTY_WEIGHT = 2.0\nMAX_SEQ_LENGTH = None\nCHARSET = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-*\"  # Group characters\nMAX_TEXT_LENGTH = 10  # Maximum length of text in an image\nFONT_DIR = os.path.join('/kaggle','input','google-fonts','GoogleFontScripts') # Folder with TrueType fonts (.ttf)\nBACKGROUND_DIR = os.path.join('/kaggle','working','backgrounds')  # New folder for background (optional)\nNUMBER_BACKGROUND_IMAGE = 1000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:29.366165Z","iopub.execute_input":"2025-03-10T12:21:29.366558Z","iopub.status.idle":"2025-03-10T12:21:29.380617Z","shell.execute_reply.started":"2025-03-10T12:21:29.366516Z","shell.execute_reply":"2025-03-10T12:21:29.379556Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Utils support functions</font>**\n-------------------","metadata":{}},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Create output folders</font>**","metadata":{}},{"cell_type":"code","source":"os.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(BACKGROUND_DIR, exist_ok=True)\nos.makedirs(TENSORBOARD_DIR, exist_ok=True)\n\n# Creates a new file\nwith open(LABELS_FILE, 'w') as fp:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:29.382862Z","iopub.execute_input":"2025-03-10T12:21:29.383176Z","iopub.status.idle":"2025-03-10T12:21:29.401917Z","shell.execute_reply.started":"2025-03-10T12:21:29.383149Z","shell.execute_reply":"2025-03-10T12:21:29.400699Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Load the font list (add the paths to the .ttf files to the \"fonts\" folder)</font>**","metadata":{}},{"cell_type":"code","source":"font_files = [\n    os.path.join(FONT_DIR, f) for f in os.listdir(FONT_DIR) \n    if f.endswith('.ttf') and os.path.isfile(os.path.join(FONT_DIR, f))\n]\nif not font_files:\n    raise FileNotFoundError(\"No fonts found in 'fonts' folder. Add .ttf files!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:29.403830Z","iopub.execute_input":"2025-03-10T12:21:29.404238Z","iopub.status.idle":"2025-03-10T12:21:31.160357Z","shell.execute_reply.started":"2025-03-10T12:21:29.404201Z","shell.execute_reply":"2025-03-10T12:21:31.159267Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Generating a simple gradient background</font>**","metadata":{}},{"cell_type":"code","source":"def generate_gradient_background(filename, size=(128, 32)):\n    img = Image.new('L', size, color=230)  # Lighter gray as a base\n    draw = ImageDraw.Draw(img)\n    for y in range(size[1]):\n        # Soft gradient with low contrast\n        color = int(230 - 20 * (y / size[1]))  # From light gray to slightly darker\n        draw.line([(0, y), (size[0], y)], fill=color)\n    # Background blur\n    img = img.filter(ImageFilter.GaussianBlur(radius=2))\n    img.save(os.path.join(BACKGROUND_DIR, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:31.161467Z","iopub.execute_input":"2025-03-10T12:21:31.161810Z","iopub.status.idle":"2025-03-10T12:21:31.167939Z","shell.execute_reply.started":"2025-03-10T12:21:31.161747Z","shell.execute_reply":"2025-03-10T12:21:31.166802Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Generate a background with noise (paper texture)</font>**","metadata":{}},{"cell_type":"code","source":"def generate_paper_texture(filename, size=(128, 32)):\n    img = Image.new('L', size, color=220)  # Lighter gray\n    noise = np.random.normal(0, 5, size).astype(np.uint8)  # Less noise\n    noise_img = Image.fromarray(noise)\n    img.paste(noise_img, (0, 0), noise_img)\n    # Blur for a softer effect\n    img = img.filter(ImageFilter.GaussianBlur(radius=1.5))\n    img.save(os.path.join(BACKGROUND_DIR, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:31.169067Z","iopub.execute_input":"2025-03-10T12:21:31.169455Z","iopub.status.idle":"2025-03-10T12:21:31.186525Z","shell.execute_reply.started":"2025-03-10T12:21:31.169417Z","shell.execute_reply":"2025-03-10T12:21:31.185239Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Creating multiple backgrounds</font>**","metadata":{}},{"cell_type":"code","source":"for i in range(NUMBER_BACKGROUND_IMAGE):\n    generate_gradient_background(f\"gradient_{i}.png\")\n    generate_paper_texture(f\"paper_{i}.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:31.187455Z","iopub.execute_input":"2025-03-10T12:21:31.187795Z","iopub.status.idle":"2025-03-10T12:21:32.381992Z","shell.execute_reply.started":"2025-03-10T12:21:31.187742Z","shell.execute_reply":"2025-03-10T12:21:32.380983Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Load backgrounds (optional, add images to the \"backgrounds\" folder)</font>**","metadata":{}},{"cell_type":"code","source":"background_files = (\n    [os.path.join(BACKGROUND_DIR, f) for f in os.listdir(BACKGROUND_DIR) \n     if f.endswith(('.png', '.jpg', '.jpeg'))] if os.path.exists(BACKGROUND_DIR) else []\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.383011Z","iopub.execute_input":"2025-03-10T12:21:32.383283Z","iopub.status.idle":"2025-03-10T12:21:32.393493Z","shell.execute_reply.started":"2025-03-10T12:21:32.383258Z","shell.execute_reply":"2025-03-10T12:21:32.392218Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Random text generation function</font>**","metadata":{}},{"cell_type":"code","source":"def generate_random_text(max_length):\n    length = random.randint(1, max_length)\n    return ''.join(random.choice(CHARSET) for _ in range(length))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.394696Z","iopub.execute_input":"2025-03-10T12:21:32.395038Z","iopub.status.idle":"2025-03-10T12:21:32.443752Z","shell.execute_reply.started":"2025-03-10T12:21:32.395009Z","shell.execute_reply":"2025-03-10T12:21:32.442665Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Functions for adding noise and distortions</font>**","metadata":{}},{"cell_type":"code","source":"def add_noise_and_distortion(img):\n    img_array = np.array(img)\n    # Mild Gaussian noise with lower intensity\n    if random.random() > 0.5:  # 50% šance\n        noise = np.random.normal(0, random.randint(5, 15), img_array.shape).astype(np.uint8)\n        img_array = cv2.add(img_array, noise)\n    # Subtle perspective distortion\n    rows, cols = img_array.shape\n    src_points = np.float32([[0, 0], [cols-1, 0], [0, rows-1], [cols-1, rows-1]])\n    dst_points = np.float32([\n        [random.uniform(0, 3), random.uniform(0, 3)],\n        [cols-1-random.uniform(0, 3), random.uniform(0, 3)],\n        [random.uniform(0, 3), rows-1-random.uniform(0, 3)],\n        [cols-1-random.uniform(0, 3), rows-1-random.uniform(0, 3)]\n    ])\n    matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n    img_array = cv2.warpPerspective(img_array, matrix, (cols, rows))\n    return Image.fromarray(img_array)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.444700Z","iopub.execute_input":"2025-03-10T12:21:32.445035Z","iopub.status.idle":"2025-03-10T12:21:32.463284Z","shell.execute_reply.started":"2025-03-10T12:21:32.445010Z","shell.execute_reply":"2025-03-10T12:21:32.462214Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Improved image generation feature</font>**","metadata":{}},{"cell_type":"code","source":"def generate_synthetic_image(text, font_path, img_size=(IMG_WIDTH, IMG_HEIGHT)):\n    # Background\n    if background_files:\n        bg_path = random.choice(background_files)\n        img = Image.open(bg_path).convert('L').resize(img_size)\n    else:\n        img = Image.new('L', img_size, color=230)\n        draw = ImageDraw.Draw(img)\n        for y in range(img_size[1]):\n            color = int(230 - 20 * (y / img_size[1]))\n            draw.line([(0, y), (img_size[0], y)], fill=color)\n        img = img.filter(ImageFilter.GaussianBlur(radius=2))\n\n    draw = ImageDraw.Draw(img)\n\n    # Iterative font and text editing\n    font_size = random.randint(20, min(IMG_HEIGHT-2, 28))\n    max_attempts = 5  # Limiting the number of attempts\n    for attempt in range(max_attempts):\n        font = ImageFont.truetype(font_path, font_size)\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width, text_height = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n\n        if text_width <= IMG_WIDTH - 10:  # Text will fit\n            break\n        elif len(text) > 1:  # Shorten the text if it is too long.\n            text = text[:len(text)//2]\n        else:  # Reduce font size\n            font_size = max(10, font_size - 5)  # Minimum size 10\n\n    # If that doesn't work, use a minimal font and single-letter text.\n    if text_width > IMG_WIDTH - 10:\n        text = text[0]  # Use the first letter\n        font_size = 10\n        font = ImageFont.truetype(font_path, font_size)\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width, text_height = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n\n    # Text position\n    x = random.randint(5, max(5, IMG_WIDTH - text_width - 5))\n    y = random.randint(5, max(5, IMG_HEIGHT - text_height - 5))\n\n    # Highlighting text\n    text_color = random.randint(0, 50)\n    outline_color = 200\n    for offset_x in [-1, 0, 1]:\n        for offset_y in [-1, 0, 1]:\n            if offset_x != 0 or offset_y != 0:\n                draw.text((x + offset_x, y + offset_y), text, font=font, fill=outline_color)\n    draw.text((x, y), text, font=font, fill=text_color)\n\n    # Noise and distortion\n    img = add_noise_and_distortion(img)\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.464445Z","iopub.execute_input":"2025-03-10T12:21:32.464853Z","iopub.status.idle":"2025-03-10T12:21:32.485229Z","shell.execute_reply.started":"2025-03-10T12:21:32.464807Z","shell.execute_reply":"2025-03-10T12:21:32.484149Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Function for splitting labels</font>**","metadata":{}},{"cell_type":"code","source":"def split_labels(labels, label_lengths):\n    \"\"\"Split a flat tensor of labels into a list of label sequences based on lengths.\"\"\"\n    split_labels = []\n    start = 0\n    for length in label_lengths:\n        split_labels.append(labels[start:start + length])\n        start += length\n    return split_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.488602Z","iopub.execute_input":"2025-03-10T12:21:32.488948Z","iopub.status.idle":"2025-03-10T12:21:32.507732Z","shell.execute_reply.started":"2025-03-10T12:21:32.488917Z","shell.execute_reply":"2025-03-10T12:21:32.506662Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Function of Beam search decoding</font>**","metadata":{}},{"cell_type":"code","source":"def beam_search_decode(output, idx_to_char, target_lengths=None, beam_width=10, blank_penalty=-0.5, length_penalty=-0.5, global_step=None):\n    probs = output.softmax(2).cpu().numpy()\n    T, B, C = probs.shape\n    predictions = []\n    \n    for b in range(B):\n        sequence_probs = [(0.0, [], 1.0)]\n        max_length = target_lengths[b].item() * 2 if target_lengths is not None else T\n        for t in range(T):\n            new_sequences = []\n            for log_prob, seq, prob in sequence_probs:\n                if len(seq) >= max_length:\n                    new_sequences.append((log_prob, seq, prob))\n                    continue\n                top_k_probs, top_k_idx = torch.topk(torch.tensor(probs[t, b]), beam_width)\n                for k_prob, k_idx in zip(top_k_probs, top_k_idx):\n                    new_seq = seq + [k_idx.item()]\n                    new_prob = prob * k_prob.item()\n                    new_log_prob = log_prob + np.log(k_prob.item())\n                    if k_idx.item() == 0:\n                        new_log_prob += blank_penalty\n                    new_log_prob += length_penalty * len(new_seq)\n                    new_sequences.append((new_log_prob, new_seq, new_prob))\n            sequence_probs = sorted(new_sequences, key=lambda x: x[0], reverse=True)[:beam_width]\n        \n        best_seq = sequence_probs[0][1]\n        decoded = []\n        prev = -1\n        for idx in best_seq:\n            if idx != 0 and idx != prev:\n                decoded.append(idx_to_char.get(idx, ''))\n            prev = idx\n        predictions.append(''.join(decoded) if decoded else '<empty>')\n    \n        # Logging token distribution into TensorBoard\n        token_counts = Counter(best_seq)\n        for token, count in token_counts.items():\n            writer.add_scalar(f'Token_Distribution/token_{token}', count, global_step)\n        print(f\"Token distribution (Batch {b}): {dict(token_counts)}\")\n    \n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.509313Z","iopub.execute_input":"2025-03-10T12:21:32.509683Z","iopub.status.idle":"2025-03-10T12:21:32.529530Z","shell.execute_reply.started":"2025-03-10T12:21:32.509647Z","shell.execute_reply":"2025-03-10T12:21:32.528407Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Custom collate function</font>**","metadata":{}},{"cell_type":"code","source":"def custom_collate_fn(batch):\n    images, labels, label_lengths = zip(*batch)\n    # Stack images (all same size)\n    images = torch.stack(images, dim=0)\n    # Concatenate labels into a flat tensor\n    labels = torch.cat(labels, dim=0)\n    # Convert label_lengths to tensor\n    label_lengths = torch.tensor(label_lengths, dtype=torch.long)\n    return images, labels, label_lengths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.530622Z","iopub.execute_input":"2025-03-10T12:21:32.530960Z","iopub.status.idle":"2025-03-10T12:21:32.551597Z","shell.execute_reply.started":"2025-03-10T12:21:32.530931Z","shell.execute_reply":"2025-03-10T12:21:32.550499Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Generování datasetu</font>**","metadata":{}},{"cell_type":"code","source":"def create_synthetic_dataset(num_samples):\n    labels = []\n    for i in range(num_samples):\n        text = generate_random_text(MAX_TEXT_LENGTH)\n        if not text:\n            continue\n        font_path = random.choice(font_files)\n        img = generate_synthetic_image(text, font_path)\n        img_name = f\"img_{i:05d}.png\"\n        img_path = os.path.join(OUTPUT_DIR, img_name)\n        img.save(img_path)\n        labels.append(f\"{img_name}\\t{text}\")  # Use tab (\\t) instead of space\n        if i % 100 == 0:\n            print(f\"Generated {i}/{num_samples} images\")\n\n    if labels:\n        with open(LABELS_FILE, 'w') as f:\n            f.write(\"\\n\".join(labels))\n        print(f\"Dataset generated! Images saved in '{OUTPUT_DIR}', labels in '{LABELS_FILE}'\")\n    else:\n        print(\"No labels generated!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.552666Z","iopub.execute_input":"2025-03-10T12:21:32.553033Z","iopub.status.idle":"2025-03-10T12:21:32.575167Z","shell.execute_reply.started":"2025-03-10T12:21:32.553006Z","shell.execute_reply":"2025-03-10T12:21:32.574018Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Mapping characters to indices and back</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"char_to_idx = {char: idx + 1 for idx, char in enumerate(CHARSET)}  # 0 is reserved for blank (CTC)\nidx_to_char = {idx: char for char, idx in char_to_idx.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.577277Z","iopub.execute_input":"2025-03-10T12:21:32.577624Z","iopub.status.idle":"2025-03-10T12:21:32.590825Z","shell.execute_reply.started":"2025-03-10T12:21:32.577594Z","shell.execute_reply":"2025-03-10T12:21:32.589655Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Custom dataset</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"class OCRDataset(Dataset):\n    def __init__(self, image_dir, labels_file):\n        self.image_dir = image_dir\n        self.labels_file = labels_file\n        self.data = []\n        with open(labels_file, 'r') as f:\n            for line in f:\n                if not line.strip():  # Skip empty lines\n                    continue\n                image_path, label = line.strip().split('\\t')\n                label_length = len(label)\n                self.data.append((image_path, label, label_length))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image_path, label, label_length = self.data[idx]\n        image = Image.open(os.path.join(self.image_dir, image_path)).convert('L')\n        image = transforms.ToTensor()(image)\n        label_encoded = torch.tensor([char_to_idx[c] for c in label], dtype=torch.long)\n        return image, label_encoded, label_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.591856Z","iopub.execute_input":"2025-03-10T12:21:32.592201Z","iopub.status.idle":"2025-03-10T12:21:32.608789Z","shell.execute_reply.started":"2025-03-10T12:21:32.592173Z","shell.execute_reply":"2025-03-10T12:21:32.607611Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Custom CTC Loss with Blank Penalty</font>**\n-------------------\n> CTC Loss with Entropy Regularization","metadata":{}},{"cell_type":"code","source":"class CTCLossWithBlankPenalty(nn.Module):\n    def __init__(self, blank=0, zero_infinity=True, blank_penalty_weight=2.0, entropy_weight=2.0, label_smoothing=0.2):\n        super().__init__()\n        self.ctc_loss = nn.CTCLoss(blank=blank, zero_infinity=zero_infinity)\n        self.blank_penalty_weight = blank_penalty_weight\n        self.entropy_weight = entropy_weight\n        self.label_smoothing = label_smoothing\n\n    def forward(self, log_probs, targets, input_lengths, target_lengths):\n        ctc_loss = self.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n        blank_probs = log_probs[:, :, 0].exp().mean()\n        blank_penalty = torch.clamp(-torch.log(1 - blank_probs + 1e-6) * self.blank_penalty_weight, max=1.0)  # Klipování\n        entropy = -(log_probs.exp() * log_probs).sum(dim=-1).mean()\n        total_loss = ctc_loss + blank_penalty + self.entropy_weight * entropy\n        return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.609701Z","iopub.execute_input":"2025-03-10T12:21:32.610043Z","iopub.status.idle":"2025-03-10T12:21:32.625052Z","shell.execute_reply.started":"2025-03-10T12:21:32.610016Z","shell.execute_reply":"2025-03-10T12:21:32.623894Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Model definition (CNN + RNN + CTC)</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"class OCRModel(nn.Module):\n    def __init__(self, num_chars):\n        super(OCRModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.rnn = nn.LSTM(128 * (IMG_HEIGHT // 4), 256, num_layers=2, bidirectional=True, dropout=DROPOUT)\n        self.fc = nn.Linear(256 * 2, num_chars)\n        self.dropout = nn.Dropout(DROPOUT)\n\n    def forward(self, x, max_length=None):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        batch, channels, height, width = x.size()\n        x = x.view(batch, channels * height, width).permute(2, 0, 1)\n        if max_length is not None:\n            x = x[:max_length]\n        x, _ = self.rnn(x)\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.626156Z","iopub.execute_input":"2025-03-10T12:21:32.626438Z","iopub.status.idle":"2025-03-10T12:21:32.643387Z","shell.execute_reply.started":"2025-03-10T12:21:32.626413Z","shell.execute_reply":"2025-03-10T12:21:32.642270Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Training</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, criterion, optimizer, device, epoch, warmup_steps=WARMUP_STEPS):\n    model.train()\n    total_loss = 0\n    global_step = epoch * len(train_loader)\n    \n    for batch_idx, (imgs, labels, label_lengths) in enumerate(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n        label_lengths = label_lengths.to(device)\n\n        if global_step < warmup_steps:\n            lr_scale = min(1.0, float(global_step + 1) / warmup_steps)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = LEARNING_RATE * lr_scale\n\n        optimizer.zero_grad()\n        # Dynamic setting of MAX_SEQ_LENGTH according to the maximum label length in the batch\n        max_label_length = label_lengths.max().item() * 2\n        outputs = model(imgs, max_length=max_label_length)\n        outputs = outputs / TEMPERATURE\n        outputs = outputs.log_softmax(2)\n\n        batch_size = imgs.size(0)\n        seq_length = outputs.size(0)\n        input_lengths = torch.full((batch_size,), seq_length, dtype=torch.long).to(device)\n\n        loss = criterion(outputs, labels, input_lengths, label_lengths)\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"Warning: NaN or Inf loss at batch {batch_idx}. Skipping...\")\n            continue\n\n        loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.2)\n        optimizer.step()\n        total_loss += loss.item()\n        global_step += 1\n\n        if batch_idx % 10 == 0:\n            with torch.no_grad():\n                pred_texts = beam_search_decode(outputs, idx_to_char, label_lengths, global_step=global_step)\n                raw_outputs = outputs.argmax(2).cpu().numpy()[:3]\n                blank_probs = outputs[:, :, 0].exp().mean().item()\n                label_sequences = split_labels(labels, label_lengths)\n                ground_truth = [''.join([idx_to_char.get(idx.item(), '') for idx in label_seq])\n                                for label_seq in label_sequences[:3]]\n                # Average prediction length\n                pred_lengths = [len(p) for p in pred_texts]\n                avg_pred_length = sum(pred_lengths) / len(pred_lengths) if pred_lengths else 0\n                \n                # Logging into TensorBoard\n                writer.add_scalar('Loss/train_batch', loss.item(), global_step)\n                writer.add_scalar('Blank_Probability/train_batch', blank_probs, global_step)\n                writer.add_scalar('Gradient_Norm/train_batch', grad_norm.item(), global_step)\n                writer.add_scalar('Prediction_Length/avg_length', avg_pred_length, global_step)\n                \n                print(f\"Batch {batch_idx}, Gradient norm: {grad_norm.item():.4f}\")\n                print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n                print(f\"Avg Blank Probability: {blank_probs:.4f}\")\n                print(f\"Sample predictions: {pred_texts[:3]}\")\n                print(f\"Ground Truth (first 3): {ground_truth}\")\n                print(f\"Raw outputs (first 3): {raw_outputs}\")\n                print(f\"Avg Pred Length: {avg_pred_length:.2f}, Input length: {seq_length}, Label lengths: {label_lengths[:3].tolist()}\")\n\n    avg_loss = total_loss / len(train_loader)\n    writer.add_scalar('Loss/train_epoch', avg_loss, epoch)\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.644450Z","iopub.execute_input":"2025-03-10T12:21:32.644805Z","iopub.status.idle":"2025-03-10T12:21:32.669328Z","shell.execute_reply.started":"2025-03-10T12:21:32.644753Z","shell.execute_reply":"2025-03-10T12:21:32.668358Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Inference (prediction)</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"def decode_prediction(output, idx_to_char):\n    probs = output.softmax(2)\n    max_probs, preds = probs.max(dim=2)\n    preds = preds.cpu().numpy()\n    max_probs = max_probs.cpu().numpy()\n    texts = []\n    for i, (pred, prob) in enumerate(zip(preds.T, max_probs.T)):\n        print(f\"Raw prediction {i} (pre-filter): {pred}, Max probs: {prob}\")\n        # Dynamic threshold: 75th percentile of max probs in this sequence\n        threshold = np.percentile(prob, 75)\n        print(f\"Dynamic threshold for prediction {i}: {threshold:.4f}\")\n        pred_text = []\n        prev = -1\n        for idx, p in zip(pred, prob):\n            if idx != 0 and idx != prev and p > threshold:\n                pred_text.append(idx_to_char.get(idx, ''))\n            prev = idx\n        decoded = ''.join(pred_text)\n        texts.append(decoded if decoded else '<empty>')\n    return texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.670509Z","iopub.execute_input":"2025-03-10T12:21:32.670928Z","iopub.status.idle":"2025-03-10T12:21:32.693085Z","shell.execute_reply.started":"2025-03-10T12:21:32.670890Z","shell.execute_reply":"2025-03-10T12:21:32.692084Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Main launch</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Inicializace TensorBoard writeru\n    log_dir = \"runs/ocr_experiment\"\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    writer = SummaryWriter(log_dir)\n\n    if not font_files:\n        print(\"Download some TrueType fonts (.ttf) and place them in the 'fonts' folder!\")\n    else:\n        create_synthetic_dataset(NUM_SAMPLES)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    full_dataset = OCRDataset(image_dir=OUTPUT_DIR, labels_file=LABELS_FILE)\n\n    for i in range(5):\n        img, label, length = full_dataset[i]\n        plt.imshow(img.squeeze(), cmap='gray')\n        plt.title(''.join([idx_to_char.get(idx.item(), '') for idx in label[:length]]))\n        plt.show()\n    \n    if len(full_dataset) == 0:\n        print(\"Dataset is empty! Check labels.txt or image directory.\")\n    else:\n        # Curriculum phases with pre-filtering\n        model = OCRModel(num_chars=len(CHARSET)).to(device)\n        criterion = CTCLossWithBlankPenalty(\n            blank=0, zero_infinity=True, blank_penalty_weight=BLANK_PENALTY_WEIGHT,\n            entropy_weight=ENTROPY_WEIGHT, label_smoothing=LABEL_SMOOTHING\n        )\n        \n        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n\n        best_loss = float('inf')\n        \n        for epoch in range(EPOCHS):\n            # Filter full dataset based on curriculum phase\n            if epoch < 10:\n                filtered_data = [(img, lbl, lbl_len) for img, lbl, lbl_len in full_dataset.data if lbl_len <= 5]\n                sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]  # lbl je řetězec\n                print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n            elif epoch < 15:\n                filtered_data = [(img, lbl, lbl_len) for img, lbl, lbl_len in full_dataset.data if lbl_len <= 7]\n                sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]\n                print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n            else:\n                filtered_data = full_dataset.data\n                sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]\n                print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n\n            # Create a new dataset with filtered data\n            curr_dataset = OCRDataset(image_dir=OUTPUT_DIR, labels_file=LABELS_FILE)\n            curr_dataset.data = filtered_data  # Overwrite with filtered data\n\n            # Split into train and validation\n            train_size = int(0.8 * len(curr_dataset))\n            val_size = len(curr_dataset) - train_size\n            train_dataset, val_dataset = torch.utils.data.random_split(curr_dataset, [train_size, val_size])\n            print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n            \n            # Create data loaders\n            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n            \n            # Training with TensorBoard logging\n            model.train()\n            total_loss = 0\n            global_step = epoch * len(train_loader)\n            for batch_idx, (imgs, labels, label_lengths) in enumerate(train_loader):\n                imgs, labels = imgs.to(device), labels.to(device)\n                label_lengths = label_lengths.to(device)\n\n                if global_step < WARMUP_STEPS:\n                    lr_scale = min(1.0, float(global_step + 1) / WARMUP_STEPS)\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = LEARNING_RATE * lr_scale\n\n                optimizer.zero_grad()\n                outputs = model(imgs)\n                outputs = outputs / TEMPERATURE\n                outputs = outputs.log_softmax(2)\n\n                batch_size = imgs.size(0)\n                seq_length = outputs.size(0)\n                input_lengths = torch.full((batch_size,), seq_length, dtype=torch.long).to(device)\n\n                loss = criterion(outputs, labels, input_lengths, label_lengths)\n                if torch.isnan(loss) or torch.isinf(loss):\n                    print(f\"Warning: NaN or Inf loss at batch {batch_idx}. Skipping...\")\n                    continue\n\n                loss.backward()\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n                optimizer.step()\n                total_loss += loss.item()\n                global_step += 1\n\n                if batch_idx % 10 == 0:\n                    with torch.no_grad():\n                        pred_texts = beam_search_decode(outputs, idx_to_char, label_lengths)\n                        raw_outputs = outputs.argmax(2).cpu().numpy()[:3]\n                        blank_probs = outputs[:, :, 0].exp().mean().item()\n                        label_sequences = split_labels(labels, label_lengths)\n                        ground_truth = [''.join([idx_to_char.get(idx.item(), '') for idx in label_seq])\n                                        for label_seq in label_sequences[:3]]\n                        \n                        # Logování do TensorBoard\n                        writer.add_scalar('Loss/train_batch', loss.item(), global_step)\n                        writer.add_scalar('Blank_Probability/train_batch', blank_probs, global_step)\n                        writer.add_scalar('Gradient_Norm/train_batch', grad_norm.item(), global_step)\n                        \n                        print(f\"Batch {batch_idx}, Gradient norm: {grad_norm.item():.4f}\")\n                        print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n                        print(f\"Avg Blank Probability: {blank_probs:.4f}\")\n                        print(f\"Sample predictions: {pred_texts[:3]}\")\n                        print(f\"Ground Truth (first 3): {ground_truth}\")\n                        print(f\"Raw outputs (first 3): {raw_outputs}\")\n                        print(f\"Input length: {seq_length}, Label lengths: {label_lengths[:3].tolist()}\")\n\n            avg_loss = total_loss / len(train_loader)\n            writer.add_scalar('Loss/train_epoch', avg_loss, epoch)\n            print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n\n            # Validation with TensorBoard logging\n            model.eval()\n            val_loss = 0\n            val_blank_probs = 0\n            with torch.no_grad():\n                for batch_idx, (imgs, labels, label_lengths) in enumerate(val_loader):\n                    imgs, labels = imgs.to(device), labels.to(device)\n                    label_lengths = label_lengths.to(device)\n                    outputs = model(imgs)\n                    outputs = outputs.log_softmax(2)\n                    seq_length = outputs.size(0)\n                    input_lengths = torch.full((imgs.size(0),), seq_length, dtype=torch.long).to(device)\n                    val_loss += criterion(outputs, labels, input_lengths, label_lengths).item()\n                    val_blank_probs += outputs[:, :, 0].exp().mean().item()\n                \n                val_loss /= len(val_loader)\n                val_blank_probs /= len(val_loader)\n                pred_texts = beam_search_decode(outputs, idx_to_char, label_lengths)\n                label_sequences = split_labels(labels, label_lengths)\n                ground_truth = [''.join([idx_to_char.get(idx.item(), '') for idx in label_seq])\n                                for label_seq in label_sequences[:5]]\n                \n                # Validation logging to TensorBoard\n                writer.add_scalar('Loss/val_epoch', val_loss, epoch)\n                writer.add_scalar('Blank_Probability/val_epoch', val_blank_probs, epoch)\n                print(f\"Validation Loss: {val_loss:.4f}\")\n                print(\"Validation Predictions:\", pred_texts[:5])\n                print(\"Ground Truth:\", ground_truth)\n\n            scheduler.step()\n            print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']}\")\n\n            if val_loss < best_loss:\n                best_loss = val_loss\n                torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'best_ocr_model.pth'))\n\n        torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'final_ocr_model.pth'))\n    \n    # Closing the TensorBoard writer\n    writer.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:21:32.694193Z","iopub.execute_input":"2025-03-10T12:21:32.694482Z","iopub.status.idle":"2025-03-10T12:41:55.778598Z","shell.execute_reply.started":"2025-03-10T12:21:32.694457Z","shell.execute_reply":"2025-03-10T12:41:55.777540Z"}},"outputs":[{"name":"stdout","text":"Generated 0/5120 images\nGenerated 100/5120 images\nGenerated 200/5120 images\nGenerated 300/5120 images\nGenerated 400/5120 images\nGenerated 500/5120 images\nGenerated 600/5120 images\nGenerated 700/5120 images\nGenerated 800/5120 images\nGenerated 900/5120 images\nGenerated 1000/5120 images\nGenerated 1100/5120 images\nGenerated 1200/5120 images\nGenerated 1300/5120 images\nGenerated 1400/5120 images\nGenerated 1500/5120 images\nGenerated 1600/5120 images\nGenerated 1700/5120 images\nGenerated 1800/5120 images\nGenerated 1900/5120 images\nGenerated 2000/5120 images\nGenerated 2100/5120 images\nGenerated 2200/5120 images\nGenerated 2300/5120 images\nGenerated 2400/5120 images\nGenerated 2500/5120 images\nGenerated 2600/5120 images\nGenerated 2700/5120 images\nGenerated 2800/5120 images\nGenerated 2900/5120 images\nGenerated 3000/5120 images\nGenerated 3100/5120 images\nGenerated 3200/5120 images\nGenerated 3300/5120 images\nGenerated 3400/5120 images\nGenerated 3500/5120 images\nGenerated 3600/5120 images\nGenerated 3700/5120 images\nGenerated 3800/5120 images\nGenerated 3900/5120 images\nGenerated 4000/5120 images\nGenerated 4100/5120 images\nGenerated 4200/5120 images\nGenerated 4300/5120 images\nGenerated 4400/5120 images\nGenerated 4500/5120 images\nGenerated 4600/5120 images\nGenerated 4700/5120 images\nGenerated 4800/5120 images\nGenerated 4900/5120 images\nGenerated 5000/5120 images\nGenerated 5100/5120 images\nDataset generated! Images saved in '/kaggle/working/synthetic_data/images', labels in '/kaggle/working/synthetic_data/labels.txt'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1, Filtered data size: 2604, Sample labels: ['qV8i', 'i', 'PnB']\nTrain size: 2083, Val size: 521\nToken distribution (Batch 0): {12: 1, 22: 1, 55: 1, 1: 1}\nToken distribution (Batch 1): {5: 1, 12: 1, 10: 2, 56: 1, 6: 1, 1: 2}\nToken distribution (Batch 2): {11: 1, 22: 1, 63: 1, 10: 1, 1: 1, 33: 1}\nToken distribution (Batch 3): {11: 1, 5: 1, 34: 1, 14: 1, 1: 3, 47: 1, 19: 1, 27: 1}\nToken distribution (Batch 4): {19: 2, 27: 1, 11: 1, 35: 1, 33: 1, 1: 1, 8: 1}\nToken distribution (Batch 5): {12: 1, 32: 1, 56: 1, 1: 3, 22: 1, 11: 1}\nToken distribution (Batch 6): {6: 1, 1: 5, 47: 1, 55: 1, 10: 1, 32: 1}\nToken distribution (Batch 7): {47: 1, 10: 1}\nToken distribution (Batch 8): {1: 3, 56: 1, 48: 1, 6: 1, 40: 1, 55: 1, 43: 1, 4: 1}\nToken distribution (Batch 9): {27: 1, 1: 2, 10: 1, 34: 1, 4: 1, 56: 1, 47: 1}\nToken distribution (Batch 10): {1: 1, 24: 1}\nToken distribution (Batch 11): {1: 5, 56: 1, 42: 1, 32: 1, 47: 1, 22: 1}\nToken distribution (Batch 12): {1: 1, 8: 1, 24: 1, 10: 1}\nToken distribution (Batch 13): {50: 1, 1: 3, 48: 1, 5: 1, 32: 1, 35: 1}\nToken distribution (Batch 14): {1: 2}\nToken distribution (Batch 15): {19: 1, 32: 1, 43: 1, 10: 1}\nToken distribution (Batch 16): {56: 1, 11: 1, 12: 1, 38: 1, 10: 1, 25: 1}\nToken distribution (Batch 17): {10: 1, 1: 1}\nToken distribution (Batch 18): {10: 2, 42: 1, 1: 1, 56: 1, 6: 1}\nToken distribution (Batch 19): {56: 1, 62: 1, 10: 1, 57: 1, 27: 1, 4: 1, 22: 1, 1: 2, 47: 1}\nToken distribution (Batch 20): {10: 3, 4: 1, 34: 1, 19: 1, 1: 1, 6: 1}\nToken distribution (Batch 21): {34: 1, 47: 1, 56: 1, 10: 1, 4: 1, 12: 1, 60: 1, 27: 1}\nToken distribution (Batch 22): {27: 1, 44: 1, 1: 1, 63: 2, 7: 1, 47: 1, 32: 1, 56: 1, 12: 1}\nToken distribution (Batch 23): {1: 1, 19: 1}\nToken distribution (Batch 24): {32: 2, 57: 1, 48: 1, 25: 1, 30: 1, 10: 1, 40: 1}\nToken distribution (Batch 25): {33: 1, 50: 1, 1: 1, 34: 1}\nToken distribution (Batch 26): {32: 1, 1: 1, 22: 1, 2: 1, 4: 1, 24: 1}\nToken distribution (Batch 27): {10: 1, 6: 1, 12: 1, 1: 2, 56: 1}\nToken distribution (Batch 28): {43: 1, 1: 2, 22: 1, 6: 1, 42: 1}\nToken distribution (Batch 29): {42: 1, 27: 1, 1: 3, 29: 1}\nToken distribution (Batch 30): {34: 2, 23: 1, 1: 2, 6: 1, 54: 1, 43: 1}\nToken distribution (Batch 31): {32: 2, 34: 1, 11: 1, 42: 1, 1: 2, 19: 1}\nBatch 0, Gradient norm: 32.3709\nEpoch 1, Batch 0/66, Loss: 58.0275\nAvg Blank Probability: 0.0146\nSample predictions: ['lv2a', 'elj3faja', 'kv-jaG']\nGround Truth (first 3): ['Up', '4Kv4', 'OFf']\nRaw outputs (first 3): [[12  5 11 11 19 12  6 47  1 27  1  1  1 50  1 19 56 10 10 56 10 34 27  1\n  32 33 32 10 43 42 34 32]\n [22 12 22  5 27 32  1 10 56  1 24 56  8  1  1 32 11  1 42 62  4 47 44 19\n  57 50  1  6  1 27 23 34]\n [55 10 63 34 19 56 47 11 48  1  1 42 24  1  1 43 12 32  1 10 34 56  1  1\n  48  1 22 12  1  1  1 11]]\nInput length: 32, Label lengths: [2, 4, 3]\nToken distribution (Batch 0): {43: 1, 10: 1}\nToken distribution (Batch 1): {56: 1, 1: 1}\nToken distribution (Batch 2): {1: 1, 19: 1, 22: 1, 12: 1, 42: 1, 10: 1}\nToken distribution (Batch 3): {12: 1, 1: 1, 10: 1, 56: 2, 14: 1}\nToken distribution (Batch 4): {6: 1, 1: 4, 32: 2, 10: 1, 25: 1, 5: 1}\nToken distribution (Batch 5): {1: 1, 60: 1, 32: 1, 12: 1, 47: 1, 30: 1}\nToken distribution (Batch 6): {36: 1, 61: 1, 12: 1, 1: 4, 22: 1}\nToken distribution (Batch 7): {43: 1, 6: 1, 10: 1, 47: 1, 22: 1, 1: 1}\nToken distribution (Batch 8): {29: 1, 1: 1, 43: 1, 47: 1, 28: 1, 10: 1}\nToken distribution (Batch 9): {6: 1, 16: 1, 56: 1, 1: 1, 12: 1, 47: 1, 32: 1, 5: 1, 50: 1, 25: 1}\nToken distribution (Batch 10): {13: 1, 1: 3, 42: 1, 43: 1, 27: 1, 32: 1}\nToken distribution (Batch 11): {30: 1, 1: 2, 47: 1, 11: 1, 10: 1, 5: 1, 35: 1}\nToken distribution (Batch 12): {12: 1, 48: 1, 10: 2, 27: 1, 42: 1, 50: 1, 1: 1, 34: 1, 47: 1}\nToken distribution (Batch 13): {4: 1, 34: 1, 32: 2, 19: 1, 27: 1, 11: 1, 43: 1}\nToken distribution (Batch 14): {25: 1, 10: 1}\nToken distribution (Batch 15): {12: 1, 5: 1, 50: 1, 1: 1, 28: 1, 32: 2, 47: 1}\nToken distribution (Batch 16): {10: 1, 63: 1, 22: 1, 6: 1}\nToken distribution (Batch 17): {63: 1, 22: 1, 6: 2, 47: 1, 1: 1}\nToken distribution (Batch 18): {14: 1, 42: 1, 12: 1, 34: 1, 5: 1, 19: 1, 22: 1, 4: 1}\nToken distribution (Batch 19): {1: 3, 43: 1, 62: 1, 4: 1}\nToken distribution (Batch 20): {29: 1, 24: 1, 47: 1, 10: 1, 22: 1, 1: 1}\nToken distribution (Batch 21): {6: 1, 24: 1, 1: 1, 60: 1, 11: 1, 10: 1, 47: 1, 19: 1}\nToken distribution (Batch 22): {4: 1, 1: 2, 43: 1}\nToken distribution (Batch 23): {56: 1, 1: 3, 34: 1, 11: 1}\nToken distribution (Batch 24): {34: 1, 47: 1, 22: 2, 6: 1, 56: 2, 62: 1, 32: 1, 43: 1}\nToken distribution (Batch 25): {11: 1, 30: 2, 1: 1, 50: 1, 34: 2, 19: 1}\nToken distribution (Batch 26): {42: 1, 47: 1}\nToken distribution (Batch 27): {6: 1, 1: 1, 43: 1, 27: 1}\nToken distribution (Batch 28): {1: 1, 19: 1}\nToken distribution (Batch 29): {43: 1, 1: 1, 47: 1, 11: 1}\nToken distribution (Batch 30): {1: 5, 56: 1, 4: 1, 50: 1}\nToken distribution (Batch 31): {56: 1, 17: 1}\nBatch 10, Gradient norm: 43.5791\nEpoch 1, Batch 10/66, Loss: 62.1690\nAvg Blank Probability: 0.0148\nSample predictions: ['Qj', '3a', 'asvlPj']\nGround Truth (first 3): ['X', 'K', '6eE']\nRaw outputs (first 3): [[43 56  1 12  6  1 36 43 29  6 13 30 12  4 25 12 10 63 14  1 29  6  4 56\n  34 11 42  6  1 43  1 56]\n [10  1 19  1  1 60 61  6  1 16  1  1 48 34 10  5 63 22 42 43 24 24  1  1\n  47 30 47  1 19  1 56 17]\n [ 1 34 22 10  1 32 12 10 43 56 42  1 10 32  2 50 22  6 12 62 47  1 43 34\n  22 30  6 43  1 47  1 10]]\nInput length: 32, Label lengths: [1, 1, 3]\nToken distribution (Batch 0): {47: 2, 6: 1, 27: 2, 30: 1, 25: 1, 43: 1, 19: 1, 10: 1}\nToken distribution (Batch 1): {34: 1, 35: 1, 32: 2, 19: 1, 25: 1, 13: 1, 12: 1}\nToken distribution (Batch 2): {19: 3, 6: 1, 12: 1, 48: 1, 4: 1, 55: 1, 1: 2}\nToken distribution (Batch 3): {14: 1, 34: 1, 32: 1, 35: 1}\nToken distribution (Batch 4): {24: 1, 19: 1, 22: 1, 32: 1}\nToken distribution (Batch 5): {42: 1, 17: 2, 34: 1, 19: 1, 32: 1}\nToken distribution (Batch 6): {12: 2, 57: 1, 32: 1, 56: 1, 7: 1, 11: 1, 1: 1, 24: 1, 27: 1}\nToken distribution (Batch 7): {6: 1, 47: 2, 1: 2, 7: 1, 46: 1, 19: 1, 11: 1, 27: 1}\nToken distribution (Batch 8): {47: 1, 5: 1, 16: 1, 1: 2, 33: 1}\nToken distribution (Batch 9): {1: 1, 19: 2, 6: 1}\nToken distribution (Batch 10): {10: 2}\nToken distribution (Batch 11): {6: 1, 56: 1, 11: 2, 24: 1, 36: 1, 32: 1, 1: 1, 33: 1, 50: 1}\nToken distribution (Batch 12): {1: 1, 47: 1, 56: 1, 42: 1}\nToken distribution (Batch 13): {1: 2, 11: 1, 47: 1}\nToken distribution (Batch 14): {1: 1, 24: 1, 47: 1, 11: 1, 32: 2, 42: 1, 12: 1, 56: 1, 50: 1}\nToken distribution (Batch 15): {12: 1, 24: 1, 50: 2}\nToken distribution (Batch 16): {22: 2, 1: 2, 35: 1, 25: 1, 50: 1, 10: 1, 32: 1, 42: 1}\nToken distribution (Batch 17): {8: 1, 43: 1, 47: 2, 22: 1, 1: 2, 34: 1}\nToken distribution (Batch 18): {63: 1, 1: 2, 5: 1, 48: 1, 22: 1}\nToken distribution (Batch 19): {6: 1, 63: 1}\nToken distribution (Batch 20): {63: 2, 10: 3, 25: 1}\nToken distribution (Batch 21): {6: 1, 47: 2, 16: 1, 10: 1, 32: 1, 25: 1, 50: 1}\nToken distribution (Batch 22): {1: 2, 27: 1, 22: 1, 63: 1, 33: 1}\nToken distribution (Batch 23): {6: 1, 10: 3, 34: 2, 9: 1, 1: 1}\nToken distribution (Batch 24): {56: 2, 27: 1, 13: 1, 4: 1, 16: 1, 19: 1, 47: 1}\nToken distribution (Batch 25): {1: 4, 27: 1, 29: 1, 12: 1, 22: 1}\nToken distribution (Batch 26): {56: 1, 24: 1, 16: 1, 4: 1, 11: 1, 47: 1}\nToken distribution (Batch 27): {45: 1, 43: 1, 10: 1, 1: 4, 4: 1, 27: 1, 12: 1}\nToken distribution (Batch 28): {1: 4, 7: 1, 13: 1, 4: 1, 47: 1, 5: 1, 27: 1}\nToken distribution (Batch 29): {32: 1, 10: 2, 6: 1, 43: 1, 60: 1, 48: 1, 22: 1}\nToken distribution (Batch 30): {1: 3, 32: 4, 56: 1, 43: 1, 47: 1}\nToken distribution (Batch 31): {4: 2, 1: 1, 5: 1}\nBatch 20, Gradient norm: 11500.5732\nEpoch 1, Batch 20/66, Loss: 50.8253\nAvg Blank Probability: 0.0146\nSample predictions: ['UfADyQUsAj', 'HIFsFyml', 'sflVd2asa']\nGround Truth (first 3): ['u6XUi', 'qUYe', '*afWc']\nRaw outputs (first 3): [[47 34 19 14 24 42 12  6 47  1 10  6  1  1  1 12 22  8 63  6 63  6  1  6\n  56  1 56 45  1 32  1  4]\n [ 6 35 19 34 19 17 57 47  5 19 10 56 47  1 24 24  1 43  1 63 10 47 27 10\n  27 27 24 43  7 10 32  4]\n [27  0  6 32 22 34 32  1 16 19 12 11 56 11 47 50 35 47  5  1 25 47 22 10\n  13  1 16 10 13  6  1  1]]\nInput length: 32, Label lengths: [5, 4, 5]\nToken distribution (Batch 0): {34: 1, 10: 1, 25: 1, 47: 1}\nToken distribution (Batch 1): {1: 1, 25: 1, 32: 3, 38: 1, 22: 1, 61: 1, 18: 1, 10: 1}\nToken distribution (Batch 2): {34: 2, 43: 1, 1: 3, 47: 1, 32: 1, 7: 1, 4: 1}\nToken distribution (Batch 3): {10: 1, 47: 1}\nToken distribution (Batch 4): {1: 2, 24: 1, 10: 2, 32: 1, 35: 1, 42: 1}\nToken distribution (Batch 5): {6: 1, 30: 1, 1: 5, 5: 1, 56: 1, 11: 1}\nToken distribution (Batch 6): {50: 1, 11: 1, 63: 1, 13: 1, 1: 2, 30: 1, 10: 1, 24: 1, 12: 1}\nToken distribution (Batch 7): {20: 1, 1: 1}\nToken distribution (Batch 8): {56: 1, 32: 1, 60: 1, 24: 1}\nToken distribution (Batch 9): {5: 1, 7: 1}\nToken distribution (Batch 10): {35: 1, 4: 1, 1: 3, 6: 1, 34: 1, 56: 1}\nToken distribution (Batch 11): {32: 1, 22: 1, 47: 2}\nToken distribution (Batch 12): {1: 3, 32: 2, 12: 1, 10: 2, 22: 1, 11: 1}\nToken distribution (Batch 13): {27: 2, 6: 1, 1: 1, 34: 1, 5: 1}\nToken distribution (Batch 14): {4: 1, 12: 1}\nToken distribution (Batch 15): {1: 2, 11: 1, 19: 1, 6: 1, 30: 1, 50: 1, 27: 1}\nToken distribution (Batch 16): {1: 3, 11: 1, 19: 1, 61: 1, 22: 1, 63: 1, 4: 1, 56: 1}\nToken distribution (Batch 17): {48: 1, 30: 1, 63: 1, 34: 1, 60: 1, 35: 1, 43: 1, 56: 1}\nToken distribution (Batch 18): {63: 1, 11: 2, 32: 3, 24: 1, 1: 2, 35: 1}\nToken distribution (Batch 19): {21: 1, 56: 3, 61: 1, 25: 1, 6: 1, 5: 1}\nToken distribution (Batch 20): {19: 1, 32: 1, 63: 1, 10: 1, 1: 2, 57: 1, 5: 1, 36: 1, 22: 1}\nToken distribution (Batch 21): {1: 2, 56: 1, 34: 1, 11: 2, 6: 1, 27: 1}\nToken distribution (Batch 22): {10: 1, 25: 1}\nToken distribution (Batch 23): {1: 1, 6: 2, 10: 1, 11: 1, 42: 1}\nToken distribution (Batch 24): {34: 1, 32: 2, 1: 1, 29: 1, 60: 1, 27: 1, 50: 1, 43: 1, 19: 1}\nToken distribution (Batch 25): {10: 2, 36: 1, 32: 1}\nToken distribution (Batch 26): {34: 1, 63: 1, 41: 1, 4: 1}\nToken distribution (Batch 27): {6: 1, 10: 1, 34: 1, 1: 2, 37: 1}\nToken distribution (Batch 28): {11: 1, 47: 1, 1: 2}\nToken distribution (Batch 29): {10: 2, 55: 1, 32: 1, 56: 1, 22: 1}\nToken distribution (Batch 30): {1: 1, 11: 1}\nToken distribution (Batch 31): {4: 1, 47: 1, 33: 1, 35: 1}\nBatch 30, Gradient norm: 39.7367\nEpoch 1, Batch 30/66, Loss: 62.6836\nAvg Blank Probability: 0.0147\nSample predictions: ['HjyU', 'ayFLv8FrFj', 'HQaUHaFgda']\nGround Truth (first 3): ['9K', 'jEXUx', '8aW40']\nRaw outputs (first 3): [[34  1 34 10  1  6 50 20 56  5 35 32  1 27  4  1  1 48 63 21 19  1 10  1\n  34 10 34  6 11 10  1  4]\n [10 25 43 47  1 30 11  1 32  7  4 22 32  6 12 11  1 30 11 56 32 56 25  6\n  32 10 63 10 47 55 11 47]\n [25 32  1 34 24  1 63 50 60 14  1 47  1  1 30 19 11 63 11 61 63 34  4  6\n  32 36 41 34  1 10 47 33]]\nInput length: 32, Label lengths: [2, 5, 5]\nToken distribution (Batch 0): {27: 1, 10: 1}\nToken distribution (Batch 1): {11: 1, 4: 1}\nToken distribution (Batch 2): {34: 1, 1: 3, 50: 1, 25: 1, 35: 1, 55: 1}\nToken distribution (Batch 3): {19: 1, 47: 1, 20: 1, 12: 1}\nToken distribution (Batch 4): {34: 1, 48: 1, 1: 2, 42: 1, 25: 1, 10: 1, 16: 1}\nToken distribution (Batch 5): {34: 1, 47: 1, 1: 1, 19: 1}\nToken distribution (Batch 6): {11: 1, 47: 3, 1: 2, 19: 1, 16: 1, 32: 1, 22: 1}\nToken distribution (Batch 7): {19: 1, 1: 1, 56: 2, 35: 1, 40: 1}\nToken distribution (Batch 8): {34: 1, 25: 2, 63: 1, 10: 1, 56: 1, 11: 2, 27: 1, 36: 1}\nToken distribution (Batch 9): {1: 1, 35: 1, 34: 1, 56: 2, 42: 1, 11: 1, 5: 1}\nToken distribution (Batch 10): {34: 3, 35: 1, 1: 1, 32: 1, 12: 1, 29: 1, 7: 1, 22: 1}\nToken distribution (Batch 11): {48: 1, 4: 1, 59: 1, 5: 1, 56: 2, 23: 1, 26: 1, 12: 1, 10: 1}\nToken distribution (Batch 12): {10: 2, 6: 2}\nToken distribution (Batch 13): {4: 1, 8: 1}\nToken distribution (Batch 14): {1: 2, 35: 1, 42: 1}\nToken distribution (Batch 15): {43: 2, 1: 4, 25: 1, 22: 1, 35: 1, 27: 1}\nToken distribution (Batch 16): {22: 1, 12: 1}\nToken distribution (Batch 17): {1: 2, 32: 2, 47: 1, 12: 1, 43: 1, 28: 1, 10: 2}\nToken distribution (Batch 18): {11: 1, 5: 2, 10: 2, 12: 1}\nToken distribution (Batch 19): {2: 1, 6: 2, 43: 1}\nToken distribution (Batch 20): {11: 1, 1: 2, 35: 1, 5: 2, 10: 1, 32: 1, 4: 1, 47: 1}\nToken distribution (Batch 21): {11: 2, 6: 1, 12: 1}\nToken distribution (Batch 22): {1: 1, 47: 1, 32: 1, 22: 1, 27: 1, 4: 1}\nToken distribution (Batch 23): {56: 2, 50: 1, 6: 1, 10: 1, 43: 1, 5: 2, 1: 2}\nToken distribution (Batch 24): {10: 1, 43: 1, 11: 1, 28: 1, 1: 1, 6: 1}\nToken distribution (Batch 25): {55: 1, 1: 4, 10: 1, 22: 2, 27: 1, 11: 1}\nToken distribution (Batch 26): {10: 1, 29: 1}\nToken distribution (Batch 27): {10: 1, 19: 1}\nToken distribution (Batch 28): {1: 2, 16: 1, 30: 1, 61: 1, 11: 1, 12: 1, 32: 1, 47: 1, 55: 1}\nToken distribution (Batch 29): {10: 1, 11: 1, 19: 1, 1: 1, 30: 1, 4: 1}\nToken distribution (Batch 30): {32: 2, 10: 4, 1: 1, 19: 1, 30: 1, 4: 1}\nToken distribution (Batch 31): {10: 2, 11: 2, 12: 1, 1: 1, 50: 1, 5: 1, 35: 1, 19: 1}\nBatch 40, Gradient norm: 424.5784\nEpoch 1, Batch 40/66, Loss: 61.6472\nAvg Blank Probability: 0.0147\nSample predictions: ['Aj', 'kd', 'HaXyaI2']\nGround Truth (first 3): ['K', 'z', '5*m2']\nRaw outputs (first 3): [[27 11 34 19 34 34 11 19 34  1 34 48 10  4  1 43 22  1 11  2 11 11  1 56\n  10 55 10 10  1 10 32 10]\n [10  4  1 47 48 47 47  1 25 35 35  4  6  8 35  1 12 32  5  6  1  6 47 50\n  43  1 29 19 16 11 10 11]\n [29 55  1 20  1  1 47 56 63 34  1 59 10 32  1  1 27 32 10 43 35 12 32  6\n  11 10  1 10  1 19  1 11]]\nInput length: 32, Label lengths: [1, 1, 4]\nToken distribution (Batch 0): {1: 1, 56: 1}\nToken distribution (Batch 1): {11: 1, 1: 1}\nToken distribution (Batch 2): {34: 2, 47: 2, 11: 1, 4: 1, 56: 1, 32: 1}\nToken distribution (Batch 3): {1: 2, 6: 1, 55: 1, 10: 2}\nToken distribution (Batch 4): {10: 2, 1: 2, 23: 1, 56: 1}\nToken distribution (Batch 5): {24: 1, 42: 1, 32: 1, 5: 1}\nToken distribution (Batch 6): {1: 1, 22: 1}\nToken distribution (Batch 7): {10: 1, 6: 1, 1: 2, 4: 1, 47: 1, 34: 1, 27: 1, 25: 1, 19: 1}\nToken distribution (Batch 8): {42: 1, 32: 1, 10: 1, 40: 1}\nToken distribution (Batch 9): {22: 1, 24: 1, 1: 5, 17: 1, 12: 1, 36: 1}\nToken distribution (Batch 10): {1: 1, 22: 1}\nToken distribution (Batch 11): {63: 1, 47: 1, 34: 1, 10: 2, 11: 1, 7: 1, 30: 1}\nToken distribution (Batch 12): {24: 1, 10: 2, 1: 2, 32: 1, 20: 1, 13: 1, 56: 1, 6: 1}\nToken distribution (Batch 13): {12: 1, 5: 1}\nToken distribution (Batch 14): {1: 1, 22: 1, 34: 1, 10: 1}\nToken distribution (Batch 15): {19: 2, 33: 1, 12: 1, 1: 1, 22: 1}\nToken distribution (Batch 16): {16: 1, 1: 5, 35: 1, 34: 2, 56: 1}\nToken distribution (Batch 17): {50: 1, 19: 1, 56: 1, 10: 1}\nToken distribution (Batch 18): {10: 1, 4: 1, 11: 1, 1: 3, 63: 2, 34: 1, 6: 1}\nToken distribution (Batch 19): {47: 1, 23: 1, 5: 1, 1: 1, 48: 1, 25: 2, 6: 1, 24: 1, 27: 1}\nToken distribution (Batch 20): {63: 1, 1: 3, 56: 1, 4: 1, 32: 1, 18: 1}\nToken distribution (Batch 21): {1: 3, 48: 1, 6: 1, 11: 1}\nToken distribution (Batch 22): {24: 1, 1: 1}\nToken distribution (Batch 23): {32: 2, 1: 1, 47: 1}\nToken distribution (Batch 24): {32: 1, 5: 1, 1: 1, 50: 1, 22: 1, 6: 1}\nToken distribution (Batch 25): {10: 1, 27: 1, 56: 1, 19: 2, 11: 1}\nToken distribution (Batch 26): {25: 1, 12: 1}\nToken distribution (Batch 27): {10: 1, 2: 1, 46: 1, 5: 1}\nToken distribution (Batch 28): {1: 2, 6: 1, 4: 1}\nToken distribution (Batch 29): {19: 1, 12: 1, 1: 2, 35: 1, 22: 1, 11: 1, 25: 1}\nToken distribution (Batch 30): {56: 1, 12: 1, 22: 1, 1: 1}\nToken distribution (Batch 31): {1: 3, 10: 1}\nBatch 50, Gradient norm: 43.5713\nEpoch 1, Batch 50/66, Loss: 68.8402\nAvg Blank Probability: 0.0148\nSample predictions: ['a3', 'ka', 'HUkHd3FU']\nGround Truth (first 3): ['0', '3', 'Ox9Z']\nRaw outputs (first 3): [[ 1 11 34  1 10 24  1 10 42 22  1 63 24 12  1 19 16 50 10 47 63  1 24 32\n  32 10 25 10  1 19 56  1]\n [56  1 47  6  1 42 22  6 32 24 22 47 10  5 22 33  1 19  4 23  1 48  1  1\n   5 27 12  2  6 12 12  1]\n [10  7 11  1  1 32 10  1 10  1 11 34  1 10 34 19 35 56 11  5  1  6 32 32\n   1 56  4 46  4  1 22  1]]\nInput length: 32, Label lengths: [1, 1, 4]\nToken distribution (Batch 0): {10: 1, 4: 2, 42: 1, 63: 2, 50: 1, 32: 1, 19: 1, 1: 1}\nToken distribution (Batch 1): {6: 2, 19: 1, 1: 2, 11: 1, 5: 1, 48: 1, 27: 2}\nToken distribution (Batch 2): {11: 1, 61: 1, 32: 1, 1: 1}\nToken distribution (Batch 3): {6: 1, 35: 2, 4: 1, 48: 1, 1: 1, 10: 2}\nToken distribution (Batch 4): {1: 2, 34: 2, 25: 2, 12: 1, 54: 1}\nToken distribution (Batch 5): {34: 1, 10: 2, 25: 1, 56: 1, 47: 1, 1: 1, 5: 1, 55: 1, 11: 1}\nToken distribution (Batch 6): {4: 1, 1: 1}\nToken distribution (Batch 7): {46: 1, 35: 1, 32: 1, 19: 3, 7: 1, 50: 1, 10: 1, 25: 1}\nToken distribution (Batch 8): {56: 1, 12: 1, 1: 1, 10: 1}\nToken distribution (Batch 9): {12: 1, 10: 3, 4: 1, 1: 2, 47: 1, 11: 1, 5: 1}\nToken distribution (Batch 10): {35: 1, 11: 1}\nToken distribution (Batch 11): {24: 1, 19: 1, 12: 1, 6: 1, 32: 1, 5: 1}\nToken distribution (Batch 12): {43: 1, 42: 3, 56: 2, 34: 1, 1: 1}\nToken distribution (Batch 13): {43: 1, 4: 1, 1: 1, 6: 1}\nToken distribution (Batch 14): {1: 1, 4: 1}\nToken distribution (Batch 15): {1: 5, 10: 2, 32: 1, 19: 1, 47: 1}\nToken distribution (Batch 16): {10: 2, 4: 2, 9: 1, 14: 1, 32: 1, 43: 2, 1: 1}\nToken distribution (Batch 17): {22: 2, 47: 1, 1: 1}\nToken distribution (Batch 18): {1: 4, 10: 1, 19: 1, 41: 1, 50: 1}\nToken distribution (Batch 19): {11: 1, 43: 1}\nToken distribution (Batch 20): {11: 2, 1: 1, 27: 1, 10: 2}\nToken distribution (Batch 21): {47: 1, 32: 1}\nToken distribution (Batch 22): {33: 1, 19: 1, 1: 1, 45: 1, 9: 1, 6: 2, 27: 1, 32: 1, 31: 1}\nToken distribution (Batch 23): {34: 1, 10: 3, 1: 1, 56: 1, 32: 1, 4: 1}\nToken distribution (Batch 24): {4: 1, 1: 2, 11: 1, 32: 1, 22: 1}\nToken distribution (Batch 25): {34: 1, 1: 2, 22: 1}\nToken distribution (Batch 26): {25: 1, 10: 1, 1: 1, 22: 1}\nToken distribution (Batch 27): {63: 2, 34: 1, 12: 1, 56: 1, 1: 1}\nToken distribution (Batch 28): {35: 1, 27: 1, 19: 1, 6: 1, 22: 1, 25: 1, 16: 1, 50: 1}\nToken distribution (Batch 29): {1: 3, 4: 1, 35: 1, 12: 1, 47: 1, 9: 1, 56: 1, 32: 1}\nToken distribution (Batch 30): {11: 1, 10: 1, 12: 1, 35: 1, 63: 1, 56: 1}\nToken distribution (Batch 31): {11: 1, 16: 1, 19: 1, 1: 1}\nBatch 60, Gradient norm: 4425482.0000\nEpoch 1, Batch 60/66, Loss: 59.9617\nAvg Blank Probability: 0.0148\nSample predictions: ['jdP-XFd-sa', 'fsakeaVAfA', 'k8Fa']\nGround Truth (first 3): ['p46O6', 'oChe-', 'Jf']\nRaw outputs (first 3): [[10  6 11  6  1 34  4 46 56 12 35 24 43 43  1  1 10 22  1 11 11 47 33 34\n   4 34 25 63 35  1 11 11]\n [ 4 19 61 35  1 10  1 35 12 10 11 19 42  4  4  1  4 47 10 43  1 32 19 10\n   1  1 10 34 27  1 10 16]\n [42  1 32  4 34 25 56 32  1  4  1 12 42  1 50 10  9 22 19 34 27  1  1  1\n   1  1  1 12 19  4 12 19]]\nInput length: 32, Label lengths: [5, 5, 2]\nEpoch 1/20, Loss: 64.5544\nToken distribution (Batch 0): {1: 6}\nToken distribution (Batch 1): {1: 4}\nToken distribution (Batch 2): {1: 4}\nToken distribution (Batch 3): {1: 10}\nToken distribution (Batch 4): {1: 10}\nToken distribution (Batch 5): {1: 10}\nToken distribution (Batch 6): {1: 4}\nToken distribution (Batch 7): {1: 6}\nToken distribution (Batch 8): {1: 10}\nValidation Loss: 62.3988\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['Ems', 'Dt', '4z', 'OdrNd', '8QWWA']\nCurrent Learning Rate: 3.3412440590062887e-07\nEpoch 2, Filtered data size: 2604, Sample labels: ['qV8i', 'i', 'PnB']\nTrain size: 2083, Val size: 521\nToken distribution (Batch 0): {16: 1, 25: 1, 10: 1, 1: 2, 43: 1, 36: 1, 5: 1}\nToken distribution (Batch 1): {47: 1, 5: 2, 32: 1, 1: 1, 23: 1}\nToken distribution (Batch 2): {6: 1, 42: 1, 10: 1, 56: 1}\nToken distribution (Batch 3): {1: 4, 37: 1, 34: 1}\nToken distribution (Batch 4): {11: 1, 12: 1, 5: 1, 4: 1}\nToken distribution (Batch 5): {6: 1, 1: 1, 47: 2, 10: 1, 56: 1}\nToken distribution (Batch 6): {5: 1, 10: 1, 11: 1, 7: 1, 6: 1, 12: 1}\nToken distribution (Batch 7): {1: 2, 11: 1, 19: 1, 25: 1, 48: 1, 6: 1, 34: 1}\nToken distribution (Batch 8): {12: 1, 56: 1, 16: 1, 21: 1, 5: 1, 41: 1, 1: 1, 47: 1}\nToken distribution (Batch 9): {11: 1, 22: 2, 10: 2, 12: 1, 6: 1, 43: 1, 48: 1, 1: 1}\nToken distribution (Batch 10): {1: 2, 42: 1, 4: 1, 32: 1, 35: 1}\nToken distribution (Batch 11): {34: 2, 12: 1, 11: 1, 1: 1, 16: 1, 35: 1, 43: 1}\nToken distribution (Batch 12): {34: 1, 33: 1, 50: 1, 5: 1}\nToken distribution (Batch 13): {16: 1, 1: 2, 6: 1}\nToken distribution (Batch 14): {1: 3, 50: 1, 11: 1, 56: 1}\nToken distribution (Batch 15): {12: 1, 10: 2, 63: 1, 22: 1, 1: 1}\nToken distribution (Batch 16): {33: 1, 12: 1, 56: 1, 10: 1, 35: 1, 43: 1, 5: 1, 1: 1}\nToken distribution (Batch 17): {47: 2, 10: 1, 20: 1, 56: 2, 1: 1, 43: 1}\nToken distribution (Batch 18): {56: 1, 10: 1, 35: 1, 55: 1, 19: 1, 1: 1, 41: 1, 47: 1}\nToken distribution (Batch 19): {11: 1, 1: 7, 43: 1, 24: 1}\nToken distribution (Batch 20): {56: 1, 10: 2, 27: 1, 1: 2, 11: 1, 34: 1, 42: 1, 12: 1}\nToken distribution (Batch 21): {11: 2, 57: 1, 10: 2, 34: 1, 1: 1, 26: 1}\nToken distribution (Batch 22): {11: 1, 1: 2, 6: 1, 5: 1, 56: 2, 32: 1}\nToken distribution (Batch 23): {22: 1, 10: 1}\nToken distribution (Batch 24): {22: 1, 12: 1, 5: 1, 1: 1}\nToken distribution (Batch 25): {22: 1, 42: 1, 11: 1, 34: 1}\nToken distribution (Batch 26): {1: 2, 4: 1, 12: 1, 43: 1, 11: 1}\nToken distribution (Batch 27): {32: 1, 16: 1, 25: 2, 19: 1, 1: 1}\nToken distribution (Batch 28): {57: 1, 5: 1, 1: 1, 23: 1, 12: 1, 6: 1}\nToken distribution (Batch 29): {34: 1, 4: 1, 1: 1, 9: 1}\nToken distribution (Batch 30): {12: 1, 50: 1, 34: 1, 42: 1, 10: 1, 28: 1, 26: 1, 9: 1, 1: 2}\nToken distribution (Batch 31): {1: 2, 43: 1, 34: 1}\nBatch 0, Gradient norm: 30.7102\nEpoch 2, Batch 0/66, Loss: 51.0931\nAvg Blank Probability: 0.0151\nSample predictions: ['pyjaQJea', 'UeFeaw', 'fPj3']\nGround Truth (first 3): ['VNbV', 'pn9', 'K3']\nRaw outputs (first 3): [[16 47  6  1 11  6  5  1 12 11  1 34 34 16  1 12 33 47 56 11 56 11 11 22\n  22 22  1 32 57 34 12  1]\n [25  5 42 37 12  1 10 11 56 22  1 12 33  1 50 10 12 10 10  1 10 57  1 10\n  12 42  1 16  5  4 50 43]\n [10 32 10  1  5 47 11 19 16 10 42 11 50  6 11 63 56 20 35  1 27 11  1  5\n   5 11  4 25  1  1 34  1]]\nInput length: 32, Label lengths: [4, 3, 2]\nToken distribution (Batch 0): {10: 1, 35: 1, 57: 1, 22: 1, 19: 1, 6: 1}\nToken distribution (Batch 1): {5: 1, 19: 1, 11: 3, 10: 1, 1: 1, 47: 1}\nToken distribution (Batch 2): {1: 2}\nToken distribution (Batch 3): {4: 1, 22: 1}\nToken distribution (Batch 4): {1: 1, 12: 1}\nToken distribution (Batch 5): {1: 1, 25: 1, 11: 1, 35: 1, 42: 1, 13: 1}\nToken distribution (Batch 6): {34: 1, 14: 1, 10: 1, 1: 3}\nToken distribution (Batch 7): {42: 1, 22: 1, 10: 2, 19: 1, 48: 1, 6: 2}\nToken distribution (Batch 8): {1: 3, 9: 1, 5: 1, 42: 1, 19: 1, 11: 1}\nToken distribution (Batch 9): {1: 1, 19: 1, 34: 1, 12: 1}\nToken distribution (Batch 10): {11: 1, 1: 3, 42: 1, 10: 1}\nToken distribution (Batch 11): {11: 2, 1: 2, 22: 1, 56: 1}\nToken distribution (Batch 12): {10: 1, 43: 1, 11: 1, 27: 1, 25: 1, 5: 1}\nToken distribution (Batch 13): {11: 1, 6: 1}\nToken distribution (Batch 14): {12: 1, 10: 1, 35: 1, 1: 1}\nToken distribution (Batch 15): {34: 1, 10: 1}\nToken distribution (Batch 16): {11: 1, 47: 2, 35: 1, 48: 1, 19: 1, 9: 1, 7: 1}\nToken distribution (Batch 17): {4: 1, 56: 1, 11: 1, 35: 1, 47: 2}\nToken distribution (Batch 18): {1: 3, 10: 2, 32: 1}\nToken distribution (Batch 19): {5: 1, 16: 1, 34: 2, 1: 2, 10: 1, 4: 2, 22: 1}\nToken distribution (Batch 20): {6: 3, 48: 1, 35: 1, 47: 1}\nToken distribution (Batch 21): {1: 1, 32: 1}\nToken distribution (Batch 22): {10: 1, 1: 1, 4: 1, 48: 1}\nToken distribution (Batch 23): {47: 1, 35: 1}\nToken distribution (Batch 24): {12: 1, 1: 1, 22: 1, 6: 2, 42: 1, 11: 2}\nToken distribution (Batch 25): {10: 2, 43: 1, 25: 2, 1: 1}\nToken distribution (Batch 26): {1: 2, 48: 1, 24: 1, 12: 1, 56: 1, 27: 1, 4: 1, 30: 1, 25: 1}\nToken distribution (Batch 27): {19: 2, 10: 1, 27: 1, 1: 2}\nToken distribution (Batch 28): {14: 1, 4: 1, 32: 1, 35: 1}\nToken distribution (Batch 29): {4: 1, 34: 1, 43: 1, 10: 1, 47: 1, 1: 2, 11: 1}\nToken distribution (Batch 30): {10: 2, 1: 1, 47: 1, 56: 1, 27: 1, 20: 1, 59: 1, 40: 1, 11: 1}\nToken distribution (Batch 31): {63: 1, 43: 1, 35: 1, 42: 1}\nBatch 10, Gradient norm: 42.2775\nEpoch 2, Batch 10/66, Loss: 66.7526\nAvg Blank Probability: 0.0152\nSample predictions: ['jI4vsf', 'eskjkaUk', 'a']\nGround Truth (first 3): ['CVN', '76rX', 'w']\nRaw outputs (first 3): [[10  5  1  4  1  1 34 42  1  1 11 11 10 11 12 34 11  4  1  5  6  1 10 47\n  12 10  1 19 14  4 10 63]\n [35 19  1 22 12 25 14 22  1 19  1  1 43  6 10 10 47 56  1 16 48 32  1 35\n   1 43  1 10  4 34 10 43]\n [57 11 19 12  1 11 10 10  9 34  1 22 11 32 35 10 47 11 10 34 35 34  4 11\n  22 25 48 27 32 43  1 35]]\nInput length: 32, Label lengths: [3, 4, 1]\nToken distribution (Batch 0): {11: 1, 19: 1, 1: 2, 10: 1, 50: 1, 34: 1, 25: 1, 48: 1, 29: 1}\nToken distribution (Batch 1): {1: 4, 48: 2, 4: 1, 10: 1}\nToken distribution (Batch 2): {25: 1, 47: 1, 1: 1, 10: 2, 19: 2, 34: 1}\nToken distribution (Batch 3): {1: 2, 34: 1, 10: 1, 56: 3, 7: 1, 32: 2}\nToken distribution (Batch 4): {47: 1, 22: 1, 60: 1, 25: 1}\nToken distribution (Batch 5): {56: 2, 1: 3, 42: 1, 10: 1, 44: 1, 34: 1, 32: 1}\nToken distribution (Batch 6): {1: 2, 50: 1, 22: 1, 48: 1, 32: 1, 5: 1, 56: 1}\nToken distribution (Batch 7): {10: 1, 22: 1, 4: 1, 63: 1, 47: 2, 43: 1, 1: 1, 9: 1, 18: 1}\nToken distribution (Batch 8): {1: 2, 47: 2, 4: 1, 11: 1}\nToken distribution (Batch 9): {1: 6, 10: 1, 32: 2, 63: 1}\nToken distribution (Batch 10): {47: 1, 12: 2, 1: 2, 27: 1, 22: 1, 7: 1, 55: 1, 35: 1}\nToken distribution (Batch 11): {42: 1, 10: 1, 32: 3, 1: 2, 50: 1}\nToken distribution (Batch 12): {10: 1, 42: 1, 1: 2}\nToken distribution (Batch 13): {16: 1, 10: 1}\nToken distribution (Batch 14): {22: 1, 1: 1, 32: 2, 16: 1, 56: 1, 6: 1, 11: 1}\nToken distribution (Batch 15): {43: 1, 10: 1, 24: 1, 19: 1, 35: 1, 60: 1, 47: 1, 55: 1, 11: 1, 57: 1}\nToken distribution (Batch 16): {10: 1, 1: 2, 20: 1}\nToken distribution (Batch 17): {50: 1, 21: 1, 56: 1, 34: 1}\nToken distribution (Batch 18): {24: 1, 11: 1, 1: 1, 47: 1, 33: 1, 10: 1}\nToken distribution (Batch 19): {43: 1, 38: 1, 24: 1, 32: 1}\nToken distribution (Batch 20): {10: 1, 4: 2, 11: 1}\nToken distribution (Batch 21): {63: 1, 56: 1, 25: 1, 22: 1, 48: 1, 1: 2, 5: 1}\nToken distribution (Batch 22): {10: 1, 11: 1, 50: 1, 1: 2, 5: 1}\nToken distribution (Batch 23): {63: 1, 10: 2, 1: 1, 32: 1, 27: 1, 22: 1, 43: 1}\nToken distribution (Batch 24): {16: 2, 22: 1, 43: 1, 1: 2, 32: 2, 13: 1, 57: 1}\nToken distribution (Batch 25): {63: 1, 32: 1, 4: 2, 1: 3, 10: 1}\nToken distribution (Batch 26): {34: 2, 30: 1, 11: 2, 19: 1, 24: 1, 17: 1, 36: 1, 1: 1}\nToken distribution (Batch 27): {12: 1, 46: 1}\nToken distribution (Batch 28): {24: 1, 55: 1, 1: 2, 32: 1, 34: 2, 4: 1, 27: 1, 33: 1}\nToken distribution (Batch 29): {56: 1, 43: 1, 32: 1, 35: 1}\nToken distribution (Batch 30): {1: 3, 56: 1, 6: 1, 19: 1, 47: 1, 42: 1}\nToken distribution (Batch 31): {34: 2, 6: 1, 1: 3, 32: 1, 19: 1, 10: 1, 7: 1}\nBatch 20, Gradient norm: 31.3680\nEpoch 2, Batch 20/66, Loss: 49.4360\nAvg Blank Probability: 0.0152\nSample predictions: ['ksajXHyVaC', 'aVdjaVa', 'yUajsjsH']\nGround Truth (first 3): ['4hA6-', 'qLqg', 'LE5c']\nRaw outputs (first 3): [[11  1 25  1 47 56  1 10  1  1 47 42 10 16 22 43 10 50 24 43 10 63 10 63\n  16 63 34 12 24 56  1 34]\n [19 48 47 34 22 56 50 22 47  1 12 10 42 10  1 10  1 21 11 38  4 56 11 10\n  16 32 30 46 55 43  1  6]\n [ 1  4  1 10 60  1 22  4  1 10  1 32  1 10 32 24  1 56  1 24  4 25 50 10\n  22  4 11 12  1 32  1  1]]\nInput length: 32, Label lengths: [5, 4, 4]\nToken distribution (Batch 0): {10: 1, 1: 2, 32: 1, 19: 1, 22: 1, 47: 1, 4: 1}\nToken distribution (Batch 1): {27: 1, 1: 1, 19: 1, 42: 1}\nToken distribution (Batch 2): {4: 1, 1: 1}\nToken distribution (Batch 3): {10: 1, 12: 1, 57: 1, 25: 1}\nToken distribution (Batch 4): {11: 1, 35: 1, 1: 1, 9: 1}\nToken distribution (Batch 5): {35: 1, 20: 2, 19: 1, 27: 1, 34: 1, 22: 1, 6: 1}\nToken distribution (Batch 6): {42: 1, 34: 2, 32: 2, 13: 1}\nToken distribution (Batch 7): {6: 1, 19: 1}\nToken distribution (Batch 8): {47: 2, 1: 3, 32: 1, 34: 1, 35: 1}\nToken distribution (Batch 9): {42: 1, 47: 1, 28: 1, 56: 1, 32: 1, 1: 2, 19: 1}\nToken distribution (Batch 10): {11: 1, 42: 2, 5: 1, 6: 1, 56: 1, 1: 2}\nToken distribution (Batch 11): {11: 1, 35: 2, 32: 1, 30: 1, 42: 1, 47: 1, 1: 1}\nToken distribution (Batch 12): {10: 1, 57: 1, 11: 1, 32: 1}\nToken distribution (Batch 13): {14: 1, 63: 1, 1: 2}\nToken distribution (Batch 14): {1: 1, 4: 1, 56: 2, 10: 1, 21: 1, 26: 1, 32: 1, 19: 1, 50: 1}\nToken distribution (Batch 15): {34: 1, 32: 2, 5: 1, 1: 3, 47: 1}\nToken distribution (Batch 16): {63: 2, 56: 1, 10: 1, 47: 1, 36: 1}\nToken distribution (Batch 17): {63: 1, 25: 1, 9: 1, 10: 1, 21: 1, 43: 1}\nToken distribution (Batch 18): {47: 1, 10: 1}\nToken distribution (Batch 19): {63: 1, 42: 1, 56: 1, 32: 1, 23: 1, 6: 1, 40: 1, 1: 1}\nToken distribution (Batch 20): {47: 1, 1: 1}\nToken distribution (Batch 21): {19: 1, 47: 2, 11: 1, 4: 2, 32: 1, 1: 1}\nToken distribution (Batch 22): {1: 2}\nToken distribution (Batch 23): {1: 1, 34: 1}\nToken distribution (Batch 24): {4: 1, 28: 1}\nToken distribution (Batch 25): {11: 2, 1: 2, 12: 1, 19: 1, 5: 1, 35: 1, 22: 1, 32: 1}\nToken distribution (Batch 26): {63: 1, 35: 1, 10: 1, 12: 1, 56: 1, 32: 1}\nToken distribution (Batch 27): {34: 1, 25: 1, 1: 1, 11: 1}\nToken distribution (Batch 28): {34: 2, 32: 1, 10: 1, 6: 1, 46: 1}\nToken distribution (Batch 29): {1: 1, 32: 1}\nToken distribution (Batch 30): {35: 2, 13: 1, 1: 2, 42: 1, 10: 1, 56: 1}\nToken distribution (Batch 31): {63: 1, 47: 1, 12: 1, 42: 1, 51: 1, 34: 1}\nBatch 30, Gradient norm: 47.5307\nEpoch 2, Batch 30/66, Loss: 69.6601\nAvg Blank Probability: 0.0151\nSample predictions: ['jaFsvUad', 'AasP', 'da']\nGround Truth (first 3): ['3CFh', 'TK', 'B']\nRaw outputs (first 3): [[10 27  4 10 11 35 42  6 47 42 11 11 10 14  1 34 63 63 47 63 47 19  1  1\n   4 11 63 34 34  1 35 63]\n [ 1  1  1 12 35 20 34 19  1 47 42 35 57 63  4 32 56 25 10 42  1 47  1 34\n  28  1 35 25 32 32 13 47]\n [32 19  1 57  1 19 34 32  1 28 42 32 11  1 56 32 10  9 38 56 32 11 27 47\n  11 11 10  1 34 22  1 12]]\nInput length: 32, Label lengths: [4, 2, 1]\nToken distribution (Batch 0): {7: 2, 59: 1, 57: 1, 55: 1, 47: 1, 4: 2, 24: 1, 5: 1}\nToken distribution (Batch 1): {11: 1, 42: 1, 32: 1, 16: 1}\nToken distribution (Batch 2): {1: 1, 44: 1, 11: 2}\nToken distribution (Batch 3): {1: 2}\nToken distribution (Batch 4): {11: 1, 12: 1}\nToken distribution (Batch 5): {35: 1, 56: 1, 33: 1, 4: 1, 14: 1, 12: 1, 1: 2, 32: 1, 63: 1}\nToken distribution (Batch 6): {10: 2}\nToken distribution (Batch 7): {4: 1, 34: 1}\nToken distribution (Batch 8): {11: 1, 32: 1, 1: 4, 50: 1, 55: 2, 9: 1}\nToken distribution (Batch 9): {60: 1, 10: 1}\nToken distribution (Batch 10): {30: 1, 47: 1}\nToken distribution (Batch 11): {5: 1, 11: 1}\nToken distribution (Batch 12): {11: 1, 1: 3, 12: 1, 48: 1}\nToken distribution (Batch 13): {10: 2, 63: 1, 1: 3, 53: 1, 6: 1, 5: 1, 42: 1}\nToken distribution (Batch 14): {1: 1, 6: 1, 24: 1, 35: 1}\nToken distribution (Batch 15): {1: 2, 42: 1, 6: 2, 11: 1, 28: 1, 27: 1, 56: 1, 10: 1}\nToken distribution (Batch 16): {19: 1, 34: 1}\nToken distribution (Batch 17): {1: 1, 4: 1, 19: 1, 33: 1}\nToken distribution (Batch 18): {22: 1, 10: 1}\nToken distribution (Batch 19): {10: 2, 34: 1, 42: 1, 6: 1, 47: 1, 12: 1, 32: 1}\nToken distribution (Batch 20): {1: 2, 19: 1, 12: 1, 16: 1, 6: 1}\nToken distribution (Batch 21): {34: 1, 27: 1}\nToken distribution (Batch 22): {1: 1, 16: 1, 42: 2, 48: 1, 34: 1, 10: 1, 50: 1}\nToken distribution (Batch 23): {1: 1, 27: 2, 41: 1}\nToken distribution (Batch 24): {56: 1, 19: 1, 30: 1, 1: 2, 43: 1, 6: 1, 7: 1}\nToken distribution (Batch 25): {34: 2, 1: 1, 10: 2, 22: 1}\nToken distribution (Batch 26): {34: 1, 33: 1}\nToken distribution (Batch 27): {11: 1, 33: 1}\nToken distribution (Batch 28): {25: 1, 42: 1, 56: 1, 62: 1, 48: 1, 11: 1, 32: 1, 10: 1}\nToken distribution (Batch 29): {35: 1, 34: 1}\nToken distribution (Batch 30): {43: 1, 5: 1}\nToken distribution (Batch 31): {25: 1, 43: 1, 4: 2, 55: 2, 10: 1, 1: 1, 12: 1, 19: 1}\nBatch 40, Gradient norm: 66.9147\nEpoch 2, Batch 40/66, Loss: 84.9788\nAvg Blank Probability: 0.0155\nSample predictions: ['g642Udxde', 'kPFp', 'aRk']\nGround Truth (first 3): ['-Bk62', 'Ad', '7-']\nRaw outputs (first 3): [[ 7 11  1  1 11 35 10  4 11 60 30  5 11 10  1  1 19  1 22 10  1 34  1  1\n  56 34 34 11 25 35 43 25]\n [ 7 42 44  1 12 56 10 34 32 10 47 11  1 63  6 42 34  4 10 34 19 27 16 27\n  19  1 33 33 42 34  5 43]\n [59 32 11 47 43 33  1  1  1 61 42  6 12  1 24  6 12 19  1 42  1  1 42 27\n  30 10  1 22 56 35 11  4]]\nInput length: 32, Label lengths: [5, 2, 2]\nToken distribution (Batch 0): {11: 2, 1: 3, 6: 1, 10: 1, 22: 1}\nToken distribution (Batch 1): {1: 2, 9: 1, 50: 2, 19: 2, 47: 1}\nToken distribution (Batch 2): {30: 1, 1: 1}\nToken distribution (Batch 3): {63: 1, 6: 1, 56: 1, 32: 1, 5: 1, 1: 1}\nToken distribution (Batch 4): {25: 1, 47: 1}\nToken distribution (Batch 5): {1: 4, 56: 1, 6: 2, 63: 1, 47: 1, 19: 1}\nToken distribution (Batch 6): {56: 2, 47: 1, 4: 2, 1: 3, 23: 1, 30: 1}\nToken distribution (Batch 7): {6: 1, 1: 1, 18: 1, 10: 2, 47: 1, 4: 1, 29: 1}\nToken distribution (Batch 8): {63: 1, 34: 2, 1: 2, 56: 1, 10: 2, 47: 1, 22: 1}\nToken distribution (Batch 9): {43: 1, 19: 2, 34: 1, 56: 1, 30: 1, 20: 1, 42: 1, 27: 1, 10: 1}\nToken distribution (Batch 10): {12: 1, 10: 2, 1: 1, 42: 1, 11: 1}\nToken distribution (Batch 11): {1: 3, 12: 1, 34: 1, 32: 1, 11: 1, 55: 1}\nToken distribution (Batch 12): {1: 2, 19: 1, 50: 1, 47: 2, 33: 1, 56: 1}\nToken distribution (Batch 13): {6: 1, 1: 1, 5: 1, 4: 1, 40: 1, 10: 1}\nToken distribution (Batch 14): {1: 3, 43: 1, 2: 1, 6: 1, 10: 1, 13: 1}\nToken distribution (Batch 15): {30: 1, 50: 1, 16: 1, 12: 1, 22: 1, 25: 1}\nToken distribution (Batch 16): {1: 2}\nToken distribution (Batch 17): {11: 1, 9: 1, 1: 2, 12: 1, 10: 2, 48: 1}\nToken distribution (Batch 18): {1: 2, 4: 2, 34: 1, 47: 1}\nToken distribution (Batch 19): {11: 2, 34: 1, 42: 1}\nToken distribution (Batch 20): {1: 5, 4: 1, 22: 1, 27: 1, 35: 1, 10: 1}\nToken distribution (Batch 21): {13: 1, 43: 2, 56: 1, 27: 1, 4: 1, 1: 1, 34: 1}\nToken distribution (Batch 22): {10: 2, 1: 2, 57: 1, 25: 1, 6: 1, 34: 2, 56: 1}\nToken distribution (Batch 23): {22: 2, 12: 1, 25: 1}\nToken distribution (Batch 24): {32: 3, 11: 1, 47: 1, 10: 1, 18: 1, 43: 1}\nToken distribution (Batch 25): {1: 3, 2: 1, 24: 1, 13: 1}\nToken distribution (Batch 26): {24: 1, 5: 1, 25: 1, 56: 1, 47: 2}\nToken distribution (Batch 27): {1: 1, 11: 1, 19: 1, 35: 1}\nToken distribution (Batch 28): {19: 1, 4: 1, 33: 1, 10: 1}\nToken distribution (Batch 29): {11: 1, 35: 1, 47: 1, 22: 1}\nToken distribution (Batch 30): {6: 1, 10: 1, 47: 1, 34: 1, 1: 1, 40: 1}\nToken distribution (Batch 31): {1: 2, 4: 1, 11: 1, 34: 1, 22: 1}\nBatch 50, Gradient norm: 36.7494\nEpoch 2, Batch 50/66, Loss: 53.3664\nAvg Blank Probability: 0.0156\nSample predictions: ['kakfjav', 'aiXsaXU', 'Da']\nGround Truth (first 3): ['3uam', 'A6TT', 'E']\nRaw outputs (first 3): [[11  1 30 63 25  1 56  6 63 43 12  1  1  6  1 30  1 11  1 11  1 13 10 22\n  32  1 24  1 19 11  6  1]\n [ 1  9  1  6 47 56 47  1 34 19 10 12 19  1  1 50  0  9  4 34  4 43 10 12\n  32  2  5 11  4 35 10  1]\n [11 50 48 56 42  1  4 18  1 34  1 34  1  5  1 16 16  1 34 42  1 56  1 22\n  11 24 25 19 33 47 47  4]]\nInput length: 32, Label lengths: [4, 4, 1]\nToken distribution (Batch 0): {56: 1, 1: 1, 10: 1, 14: 1}\nToken distribution (Batch 1): {32: 1, 27: 2, 42: 1, 19: 1, 10: 1, 11: 1, 34: 1}\nToken distribution (Batch 2): {1: 1, 10: 1, 5: 2, 32: 1, 25: 1, 61: 1, 19: 1}\nToken distribution (Batch 3): {28: 1, 10: 2, 55: 1, 29: 1, 6: 1, 1: 2}\nToken distribution (Batch 4): {14: 1, 5: 1}\nToken distribution (Batch 5): {10: 1, 1: 1, 32: 2, 27: 1, 9: 1}\nToken distribution (Batch 6): {19: 1, 1: 1, 25: 1, 55: 1}\nToken distribution (Batch 7): {19: 1, 1: 2, 28: 1}\nToken distribution (Batch 8): {34: 1, 1: 1, 43: 1, 47: 1}\nToken distribution (Batch 9): {11: 1, 43: 1, 34: 3, 1: 1, 22: 1, 42: 1, 32: 1, 30: 1}\nToken distribution (Batch 10): {10: 1, 47: 1, 48: 2}\nToken distribution (Batch 11): {16: 1, 1: 3, 47: 1, 12: 1, 23: 1, 32: 1, 5: 1, 6: 1}\nToken distribution (Batch 12): {6: 1, 1: 1, 50: 1, 25: 1, 56: 1, 22: 1, 20: 1, 32: 1}\nToken distribution (Batch 13): {56: 1, 42: 1}\nToken distribution (Batch 14): {10: 1, 1: 1, 6: 1, 42: 1}\nToken distribution (Batch 15): {56: 1, 60: 1}\nToken distribution (Batch 16): {1: 1, 59: 1}\nToken distribution (Batch 17): {34: 1, 1: 3, 47: 1, 43: 1, 63: 1, 11: 1}\nToken distribution (Batch 18): {1: 2, 50: 1, 19: 1, 10: 1, 22: 2, 4: 2, 32: 1}\nToken distribution (Batch 19): {1: 2, 5: 1, 10: 1}\nToken distribution (Batch 20): {6: 1, 1: 1}\nToken distribution (Batch 21): {34: 2, 47: 1, 63: 1, 4: 1, 1: 2, 32: 1}\nToken distribution (Batch 22): {19: 1, 1: 1}\nToken distribution (Batch 23): {12: 1, 32: 1}\nToken distribution (Batch 24): {56: 1, 43: 2, 1: 1}\nToken distribution (Batch 25): {47: 1, 32: 2, 6: 1, 1: 1, 5: 1}\nToken distribution (Batch 26): {12: 1, 6: 1}\nToken distribution (Batch 27): {42: 1, 35: 1, 5: 1, 1: 1, 19: 1, 32: 2, 10: 1}\nToken distribution (Batch 28): {56: 1, 55: 1}\nToken distribution (Batch 29): {1: 4, 50: 3, 48: 1, 47: 1, 40: 1}\nToken distribution (Batch 30): {4: 1, 1: 1, 48: 1, 7: 1, 6: 1, 12: 1}\nToken distribution (Batch 31): {10: 1, 11: 1}\nBatch 60, Gradient norm: 39025.5977\nEpoch 2, Batch 60/66, Loss: 76.0733\nAvg Blank Probability: 0.0158\nSample predictions: ['3ajn', 'FAPsjAkH', 'ajeFey8s']\nGround Truth (first 3): ['tP', 'orH2', 'jxG9']\nRaw outputs (first 3): [[56 32  1 28 14 10 19 19 34 11 10 16  6 56 10 56  1 34  1  1  6 34 19 12\n  56 47 12 42 56  1  4  0]\n [ 1 27 10 10  5  1  1  1  1 43 47  1  1 42  1 60 59  1  1  5  1 34  1 32\n  43 32  6 35 55 50  1 11]\n [10 42  5 55 57 32 25 28 43 34 48  1 50  1  6 32  5 47 50 10 43 47 33  1\n   1  6  1  5 47 48 48 10]]\nInput length: 32, Label lengths: [2, 4, 4]\nEpoch 2/20, Loss: 63.3330\nToken distribution (Batch 0): {1: 8}\nToken distribution (Batch 1): {1: 6}\nToken distribution (Batch 2): {1: 6}\nToken distribution (Batch 3): {1: 4}\nToken distribution (Batch 4): {1: 10}\nToken distribution (Batch 5): {1: 6}\nToken distribution (Batch 6): {1: 8}\nToken distribution (Batch 7): {1: 10}\nToken distribution (Batch 8): {1: 4}\nValidation Loss: 63.2816\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['nxOD', 'DYK', 'kXK', 'Bw', '*4VnO']\nCurrent Learning Rate: 6.662659824518896e-07\nEpoch 3, Filtered data size: 2604, Sample labels: ['qV8i', 'i', 'PnB']\nTrain size: 2083, Val size: 521\nToken distribution (Batch 0): {6: 1, 35: 1, 30: 1, 1: 2, 31: 1}\nToken distribution (Batch 1): {56: 1, 1: 1, 10: 1, 11: 1}\nToken distribution (Batch 2): {1: 1, 10: 1}\nToken distribution (Batch 3): {34: 1, 11: 2, 1: 1, 43: 1, 36: 1}\nToken distribution (Batch 4): {34: 1, 1: 1}\nToken distribution (Batch 5): {1: 3, 35: 1, 56: 1, 12: 1, 22: 1, 34: 1}\nToken distribution (Batch 6): {4: 1, 13: 1, 1: 1, 10: 1}\nToken distribution (Batch 7): {63: 1, 11: 1, 35: 1, 22: 1}\nToken distribution (Batch 8): {12: 1, 42: 1}\nToken distribution (Batch 9): {10: 1, 34: 1, 1: 1, 22: 1}\nToken distribution (Batch 10): {4: 2, 1: 1, 5: 1}\nToken distribution (Batch 11): {24: 1, 1: 2, 19: 1, 4: 1, 34: 1, 43: 1, 10: 1, 27: 1, 32: 1}\nToken distribution (Batch 12): {47: 1, 16: 1, 1: 3, 19: 1, 43: 1, 10: 1}\nToken distribution (Batch 13): {32: 1, 48: 1, 11: 1, 34: 1, 43: 1, 1: 3}\nToken distribution (Batch 14): {57: 1, 1: 2, 34: 1, 19: 1, 11: 1}\nToken distribution (Batch 15): {56: 1, 1: 1, 28: 1, 22: 2, 27: 1, 32: 1, 36: 1}\nToken distribution (Batch 16): {4: 1, 63: 1, 43: 1, 32: 1, 21: 1, 27: 1}\nToken distribution (Batch 17): {5: 1, 1: 1, 28: 1, 10: 1, 32: 1, 55: 1}\nToken distribution (Batch 18): {1: 2, 56: 1, 47: 1}\nToken distribution (Batch 19): {43: 1, 22: 2, 42: 1, 10: 1, 1: 1, 40: 1, 34: 1}\nToken distribution (Batch 20): {34: 1, 47: 1, 1: 3, 50: 1, 22: 1, 7: 1, 10: 1, 27: 1}\nToken distribution (Batch 21): {63: 1, 43: 1, 56: 1, 32: 1, 11: 1, 12: 1, 34: 1, 47: 2, 27: 1}\nToken distribution (Batch 22): {1: 1, 12: 1}\nToken distribution (Batch 23): {35: 1, 22: 1, 55: 1, 1: 3, 42: 1, 6: 1}\nToken distribution (Batch 24): {35: 1, 57: 1, 47: 1, 4: 1, 50: 1, 6: 1, 11: 1, 22: 1}\nToken distribution (Batch 25): {6: 2, 1: 1, 56: 1, 10: 1, 55: 1, 35: 1, 23: 1}\nToken distribution (Batch 26): {22: 1, 1: 2, 35: 1, 55: 1, 6: 1}\nToken distribution (Batch 27): {4: 1, 34: 1}\nToken distribution (Batch 28): {10: 2, 4: 2}\nToken distribution (Batch 29): {4: 1, 32: 1, 1: 1, 46: 1}\nToken distribution (Batch 30): {12: 1, 24: 1}\nToken distribution (Batch 31): {11: 1, 47: 1, 1: 2}\nBatch 0, Gradient norm: 52.7422\nEpoch 3, Batch 0/66, Loss: 66.1974\nAvg Blank Probability: 0.0157\nSample predictions: ['fIDaEa', '3ajk', 'aj']\nGround Truth (first 3): ['NXr', 'Ii', 'Z']\nRaw outputs (first 3): [[ 6 56  1 34 34  1  4 63 12 10  4 24 47 32 57 56  4  5  1 43 34 63  1 35\n  35  6 22  4 10  4 12 11]\n [35  1 10 11  1 35 13 11 42 34  1  1 16 48  1  1 63  1 56 22 47 43 12 22\n  57  1  1 34  4 32 24 47]\n [30 10 34 11 34 56  1 35 47  1  5 19  1 11 34 28 43 28  1 42  1 56  1 55\n  47 56 35  1  4  1 57  1]]\nInput length: 32, Label lengths: [3, 2, 1]\nToken distribution (Batch 0): {25: 1, 10: 3, 19: 2, 55: 1, 1: 1}\nToken distribution (Batch 1): {1: 1, 50: 1}\nToken distribution (Batch 2): {34: 2, 5: 1, 11: 1, 12: 1, 1: 1, 4: 1, 43: 1, 10: 1, 27: 1}\nToken distribution (Batch 3): {12: 1, 10: 1}\nToken distribution (Batch 4): {5: 1, 35: 1}\nToken distribution (Batch 5): {35: 2, 56: 1, 22: 1, 11: 1, 42: 1, 48: 1, 27: 1}\nToken distribution (Batch 6): {10: 1, 32: 1, 42: 1, 4: 1, 1: 3, 43: 1}\nToken distribution (Batch 7): {24: 1, 34: 1}\nToken distribution (Batch 8): {5: 1, 1: 4, 11: 1}\nToken distribution (Batch 9): {63: 1, 47: 1, 9: 1, 35: 1, 43: 1, 51: 1, 25: 1, 7: 1}\nToken distribution (Batch 10): {34: 1, 56: 1}\nToken distribution (Batch 11): {1: 1, 32: 2, 47: 1, 29: 1, 11: 1, 4: 1, 56: 1}\nToken distribution (Batch 12): {50: 1, 10: 1, 11: 1, 1: 2, 24: 1}\nToken distribution (Batch 13): {11: 1, 10: 1}\nToken distribution (Batch 14): {19: 1, 32: 1}\nToken distribution (Batch 15): {61: 1, 25: 1, 12: 1, 20: 2, 5: 1, 56: 2, 63: 1, 1: 1}\nToken distribution (Batch 16): {1: 2}\nToken distribution (Batch 17): {25: 1, 61: 1, 1: 2, 19: 2, 34: 1, 47: 1, 56: 2}\nToken distribution (Batch 18): {6: 2, 24: 1, 34: 1, 40: 1, 1: 1}\nToken distribution (Batch 19): {10: 2, 4: 2, 56: 1, 45: 1, 1: 1, 32: 1}\nToken distribution (Batch 20): {35: 1, 1: 1}\nToken distribution (Batch 21): {1: 1, 10: 1}\nToken distribution (Batch 22): {51: 1, 30: 1}\nToken distribution (Batch 23): {11: 1, 35: 1, 32: 1, 23: 1, 7: 1, 1: 1}\nToken distribution (Batch 24): {63: 1, 6: 1}\nToken distribution (Batch 25): {6: 3, 1: 1, 10: 1, 4: 1}\nToken distribution (Batch 26): {11: 1, 40: 1, 32: 2}\nToken distribution (Batch 27): {1: 1, 10: 1, 24: 1, 43: 1}\nToken distribution (Batch 28): {6: 1, 32: 1, 47: 1, 7: 1}\nToken distribution (Batch 29): {10: 3, 25: 1, 5: 1, 12: 1}\nToken distribution (Batch 30): {4: 1, 56: 1, 10: 1, 32: 1}\nToken distribution (Batch 31): {56: 1, 35: 1, 24: 1, 4: 1, 18: 1, 6: 1, 27: 1, 1: 3}\nBatch 10, Gradient norm: 25096.9453\nEpoch 3, Batch 10/66, Loss: 78.7336\nAvg Blank Probability: 0.0160\nSample predictions: ['yjs2jsja', 'aX', 'HekladQjA']\nGround Truth (first 3): ['2b6t', 'z', 'UM8yk']\nRaw outputs (first 3): [[25  1 34 12  5 35 10 24  5 63 34  1 50 11 19 61  1 25  6 10 35  1 51 11\n  63  6 11  1  6 10  4 56]\n [10 50 34 10 35 56 32 34  1 47 56 32 10 10 32 25  1 61  6  4  1 10 30 35\n   6  1 40  0 32 10 56 35]\n [19  1  5 10  1 22 42 35  1  9 34 32 11  1 32 12 34  1 24 56  4 11 63 32\n   1  6 32 24 47 25 10 24]]\nInput length: 32, Label lengths: [4, 1, 5]\nToken distribution (Batch 0): {6: 1, 27: 1, 63: 1, 19: 1, 56: 1, 11: 1, 1: 2}\nToken distribution (Batch 1): {10: 1, 14: 1, 1: 4, 35: 1, 24: 1}\nToken distribution (Batch 2): {48: 1, 32: 1, 20: 1, 1: 2, 43: 1}\nToken distribution (Batch 3): {12: 2, 7: 1, 10: 1}\nToken distribution (Batch 4): {1: 2, 34: 1, 45: 1, 5: 1, 38: 1}\nToken distribution (Batch 5): {4: 2, 47: 1, 19: 1, 1: 3, 22: 1, 42: 1, 61: 1}\nToken distribution (Batch 6): {1: 2, 35: 1, 43: 2, 6: 1, 24: 1, 10: 1}\nToken distribution (Batch 7): {1: 2}\nToken distribution (Batch 8): {1: 2, 6: 1, 32: 1, 26: 1, 22: 1}\nToken distribution (Batch 9): {10: 1, 1: 2, 27: 1}\nToken distribution (Batch 10): {6: 1, 22: 1, 10: 1, 1: 1, 50: 1, 32: 1, 56: 1, 43: 1}\nToken distribution (Batch 11): {56: 1, 57: 1}\nToken distribution (Batch 12): {11: 3, 34: 1, 10: 1, 55: 1, 25: 1, 5: 1}\nToken distribution (Batch 13): {30: 1, 6: 1, 35: 1, 45: 1, 29: 1, 32: 1, 1: 1, 16: 1}\nToken distribution (Batch 14): {43: 1, 56: 2, 6: 2, 1: 1}\nToken distribution (Batch 15): {4: 1, 35: 1, 1: 1, 56: 2, 32: 1}\nToken distribution (Batch 16): {47: 1, 19: 1, 6: 1, 1: 1}\nToken distribution (Batch 17): {63: 1, 28: 1, 1: 2, 11: 1, 22: 1}\nToken distribution (Batch 18): {11: 3, 50: 1, 35: 1, 32: 1, 40: 1, 29: 1, 10: 1, 56: 1}\nToken distribution (Batch 19): {47: 1, 30: 1, 56: 1, 43: 1, 6: 1, 1: 1}\nToken distribution (Batch 20): {42: 1, 4: 1, 11: 1, 27: 1}\nToken distribution (Batch 21): {56: 1, 55: 1, 30: 1, 34: 1, 1: 1, 9: 1}\nToken distribution (Batch 22): {35: 1, 1: 1}\nToken distribution (Batch 23): {30: 1, 12: 1, 56: 1, 57: 1, 22: 2, 9: 1, 29: 1, 8: 1, 48: 1}\nToken distribution (Batch 24): {63: 3, 30: 1, 1: 1, 5: 1, 10: 2, 11: 1, 34: 1}\nToken distribution (Batch 25): {11: 1, 1: 1}\nToken distribution (Batch 26): {63: 1, 4: 1, 56: 1, 51: 1}\nToken distribution (Batch 27): {16: 1, 32: 2, 35: 1, 43: 1, 4: 1, 1: 2, 30: 1, 48: 1}\nToken distribution (Batch 28): {6: 1, 16: 1}\nToken distribution (Batch 29): {16: 1, 1: 2, 22: 1, 5: 1, 42: 1}\nToken distribution (Batch 30): {1: 1, 10: 2, 6: 1, 9: 1, 19: 1, 11: 1, 14: 1, 28: 1, 4: 1}\nToken distribution (Batch 31): {1: 2, 42: 2, 34: 1, 22: 1}\nBatch 20, Gradient norm: 1198570.8750\nEpoch 3, Batch 20/66, Loss: 59.4235\nAvg Blank Probability: 0.0162\nSample predictions: ['fA-s3ka', 'jnaIxa', 'VFtaQ']\nGround Truth (first 3): ['AaGy', 'JnBX', 'QVW']\nRaw outputs (first 3): [[ 6 10 48 12  1  4  1  1  1 10  6 56 11 30 43  4 47 63 11 47 42 56 35 30\n  63 11 63 16  6 16  1  1]\n [27 14 32  7 34  4 35  1  6  1 22 57 11  6 56 35 19 28 11 30  4 55  1 12\n  30  1  4 32 16  1 10 42]\n [63  1 20 10 45 47 43  5 32 27 10  4 11 35 56  1  6  1 50 56 11 30  4 56\n   1 11 56 32 11  1  6 42]]\nInput length: 32, Label lengths: [4, 4, 3]\nToken distribution (Batch 0): {12: 1, 32: 2, 55: 1, 1: 1, 7: 1, 5: 1, 47: 1}\nToken distribution (Batch 1): {57: 1, 43: 1, 22: 1, 1: 1}\nToken distribution (Batch 2): {11: 1, 27: 1}\nToken distribution (Batch 3): {4: 1, 24: 1, 19: 1, 43: 1, 32: 1, 50: 1}\nToken distribution (Batch 4): {12: 1, 42: 1, 11: 1, 22: 1, 10: 1, 35: 1, 32: 1, 1: 1}\nToken distribution (Batch 5): {27: 1, 1: 1, 9: 1, 10: 2, 47: 1, 42: 1, 5: 1}\nToken distribution (Batch 6): {14: 1, 24: 1, 10: 1, 56: 1}\nToken distribution (Batch 7): {16: 2, 32: 2, 30: 1, 10: 1, 11: 1, 5: 1}\nToken distribution (Batch 8): {12: 1, 1: 1, 61: 1, 27: 1, 4: 1, 32: 1}\nToken distribution (Batch 9): {4: 1, 32: 3, 51: 1, 1: 1, 43: 1, 56: 1, 55: 1, 10: 1}\nToken distribution (Batch 10): {11: 1, 22: 1}\nToken distribution (Batch 11): {12: 1, 30: 1, 32: 1, 36: 1, 50: 1, 10: 1, 35: 1, 13: 1, 16: 1, 56: 1}\nToken distribution (Batch 12): {34: 1, 10: 1, 22: 1, 47: 1}\nToken distribution (Batch 13): {5: 1, 35: 1, 10: 1, 11: 1}\nToken distribution (Batch 14): {56: 2, 34: 1, 1: 2, 35: 1}\nToken distribution (Batch 15): {42: 2, 1: 1, 32: 1, 12: 1, 5: 1, 7: 1, 47: 1}\nToken distribution (Batch 16): {24: 1, 22: 1, 46: 1, 35: 1}\nToken distribution (Batch 17): {42: 1, 12: 1, 59: 1, 1: 2, 10: 1, 56: 1, 24: 1}\nToken distribution (Batch 18): {1: 1, 47: 1, 28: 1, 12: 1, 56: 1, 27: 1}\nToken distribution (Batch 19): {1: 3, 6: 1, 63: 1, 47: 2, 25: 1, 56: 1, 36: 1}\nToken distribution (Batch 20): {25: 1, 10: 1, 34: 1, 35: 2, 12: 1, 1: 1, 6: 1, 22: 1, 18: 1}\nToken distribution (Batch 21): {1: 3, 12: 1, 47: 1, 11: 1}\nToken distribution (Batch 22): {11: 1, 10: 1, 1: 1, 32: 1}\nToken distribution (Batch 23): {30: 1, 19: 2, 24: 1, 34: 1, 43: 1}\nToken distribution (Batch 24): {1: 2, 22: 1, 6: 1, 12: 1, 34: 1}\nToken distribution (Batch 25): {1: 3, 19: 1, 35: 2, 11: 2}\nToken distribution (Batch 26): {1: 2, 4: 1, 25: 1, 34: 1, 5: 1}\nToken distribution (Batch 27): {1: 2, 5: 1, 10: 1, 56: 1, 25: 1}\nToken distribution (Batch 28): {44: 1, 1: 2, 10: 1, 5: 1, 47: 1, 11: 1, 24: 1}\nToken distribution (Batch 29): {10: 1, 5: 1, 1: 1, 47: 1, 22: 1, 20: 1}\nToken distribution (Batch 30): {34: 1, 47: 1, 63: 1, 1: 1, 56: 1, 24: 1, 10: 1, 55: 1}\nToken distribution (Batch 31): {4: 1, 12: 1}\nBatch 30, Gradient norm: 40.8673\nEpoch 3, Batch 30/66, Loss: 54.6170\nAvg Blank Probability: 0.0164\nSample predictions: ['lF2ageU', '4Qva', 'kA']\nGround Truth (first 3): ['yz2K', 'Pn', 'd']\nRaw outputs (first 3): [[12 57 11  4 12 27 14 16 12  4 11 12 34  5 56 42 24 42  1  1 25  1 11 30\n   1  1  1  1 44 10 34  4]\n [32 43 27 24 42  1 24 32  1 32 22 30 10 35 34  1 22 12 47  6 10 12 10 19\n   1 19  4  5  1  5 47 12]\n [32 22  7 19 11  9 10 32 61 51 61 32 22 10 56 32 46 59 28 63 34  1  1 24\n   0 35  1  1 10  1 63  1]]\nInput length: 32, Label lengths: [4, 2, 1]\nToken distribution (Batch 0): {42: 1, 10: 1, 12: 1, 24: 1}\nToken distribution (Batch 1): {22: 1, 1: 2, 12: 2, 4: 1, 56: 1, 20: 1, 47: 1, 63: 1}\nToken distribution (Batch 2): {32: 1, 55: 1}\nToken distribution (Batch 3): {1: 3, 22: 1, 6: 2, 47: 2, 32: 1, 63: 1}\nToken distribution (Batch 4): {34: 2, 10: 1, 6: 1, 1: 1, 43: 1}\nToken distribution (Batch 5): {1: 1, 17: 1}\nToken distribution (Batch 6): {4: 1, 10: 1}\nToken distribution (Batch 7): {48: 1, 5: 1}\nToken distribution (Batch 8): {34: 3, 63: 1, 35: 1, 46: 1}\nToken distribution (Batch 9): {32: 1, 1: 2, 36: 1, 25: 1, 61: 1, 11: 1, 55: 1, 12: 1, 46: 1}\nToken distribution (Batch 10): {34: 1, 1: 1}\nToken distribution (Batch 11): {56: 1, 22: 1, 17: 1, 24: 1, 55: 1, 36: 1, 34: 1, 1: 1}\nToken distribution (Batch 12): {32: 1, 24: 1, 10: 1, 1: 2, 47: 1, 60: 1, 12: 1, 4: 1, 42: 1}\nToken distribution (Batch 13): {1: 1, 12: 1, 51: 1, 32: 1}\nToken distribution (Batch 14): {1: 2, 4: 1, 32: 1}\nToken distribution (Batch 15): {4: 1, 48: 1, 19: 1, 30: 1, 32: 2, 1: 2, 50: 1, 47: 1}\nToken distribution (Batch 16): {10: 1, 1: 1}\nToken distribution (Batch 17): {6: 1, 1: 2, 43: 1}\nToken distribution (Batch 18): {34: 1, 11: 2, 47: 1, 1: 1, 27: 1, 55: 1, 35: 1}\nToken distribution (Batch 19): {26: 1, 12: 1, 32: 1, 34: 1}\nToken distribution (Batch 20): {10: 1, 42: 1, 43: 1, 48: 1, 13: 1, 1: 3, 30: 1, 32: 1}\nToken distribution (Batch 21): {47: 1, 1: 2, 20: 1, 55: 1, 43: 1}\nToken distribution (Batch 22): {23: 1, 19: 1, 1: 2, 22: 2}\nToken distribution (Batch 23): {12: 1, 1: 2, 32: 1, 34: 1, 43: 1, 55: 1, 6: 1}\nToken distribution (Batch 24): {12: 1, 1: 2, 10: 1, 22: 1, 25: 1}\nToken distribution (Batch 25): {61: 1, 12: 1}\nToken distribution (Batch 26): {1: 3, 25: 1, 43: 1, 11: 1, 22: 1, 32: 1}\nToken distribution (Batch 27): {1: 1, 13: 1, 14: 1, 23: 1, 61: 1, 56: 1}\nToken distribution (Batch 28): {63: 1, 42: 1, 1: 1, 47: 1, 11: 2, 6: 1, 10: 1}\nToken distribution (Batch 29): {56: 1, 22: 1, 6: 1, 63: 1, 1: 1, 16: 1, 20: 1, 12: 1}\nToken distribution (Batch 30): {1: 3, 56: 1}\nToken distribution (Batch 31): {47: 1, 22: 1}\nBatch 40, Gradient norm: 54.8033\nEpoch 3, Batch 40/66, Loss: 67.8507\nAvg Blank Probability: 0.0166\nSample predictions: ['Pjlx', 'vald3tUl-', 'F2']\nGround Truth (first 3): ['HJ', 'lCXSq', 's']\nRaw outputs (first 3): [[42 22 32  1 34  1  4 48 34 32 34 56 32  1  1  4 10  6 34 26 10 47 23 12\n  12 61  1  1 63 56  1 47]\n [10  1 55  1 10 17 10  5 63  1  1 22 24 12  4 48  1  1 11 12 42  1 19  1\n   1 12 25 13 42 22  1 22]\n [12  1  1  0  6 34 33  4 35 36 32 17 10 51  1 19 35 43 11 32 43  1  1 32\n  10 32  1 14  1  6  1 47]]\nInput length: 32, Label lengths: [2, 5, 1]\nToken distribution (Batch 0): {17: 1, 19: 1}\nToken distribution (Batch 1): {43: 1, 10: 1, 25: 1, 31: 1, 55: 1, 42: 1, 23: 1, 11: 1, 1: 2}\nToken distribution (Batch 2): {10: 1, 35: 1}\nToken distribution (Batch 3): {42: 1, 1: 2, 4: 1, 55: 1, 7: 1, 35: 1, 27: 1, 32: 1, 19: 1}\nToken distribution (Batch 4): {19: 1, 47: 1, 30: 1, 1: 1, 38: 1, 32: 1, 56: 1, 27: 1}\nToken distribution (Batch 5): {4: 1, 50: 1, 10: 1, 32: 1}\nToken distribution (Batch 6): {1: 2, 10: 1, 25: 1, 12: 1, 34: 1}\nToken distribution (Batch 7): {4: 1, 1: 1, 48: 1, 34: 1}\nToken distribution (Batch 8): {1: 4, 10: 2, 6: 1, 11: 1, 21: 1, 34: 1}\nToken distribution (Batch 9): {6: 1, 1: 2, 10: 1}\nToken distribution (Batch 10): {4: 1, 25: 1, 1: 2}\nToken distribution (Batch 11): {1: 1, 47: 1}\nToken distribution (Batch 12): {32: 1, 11: 1}\nToken distribution (Batch 13): {6: 2, 10: 1, 1: 1}\nToken distribution (Batch 14): {56: 3, 42: 1, 12: 1, 27: 1, 1: 1, 14: 1}\nToken distribution (Batch 15): {5: 1, 35: 1, 10: 2, 1: 2, 19: 1, 34: 1}\nToken distribution (Batch 16): {11: 1, 47: 1}\nToken distribution (Batch 17): {1: 1, 43: 1}\nToken distribution (Batch 18): {42: 1, 50: 1, 1: 2, 21: 1, 27: 1, 32: 2, 34: 2}\nToken distribution (Batch 19): {47: 1, 11: 1}\nToken distribution (Batch 20): {34: 1, 27: 1, 4: 1, 1: 1, 22: 1, 11: 1}\nToken distribution (Batch 21): {19: 1, 22: 1}\nToken distribution (Batch 22): {10: 1, 56: 1, 42: 1, 1: 1}\nToken distribution (Batch 23): {6: 1, 1: 4, 4: 1, 22: 1, 19: 1}\nToken distribution (Batch 24): {1: 2, 63: 1, 42: 1, 32: 1, 35: 1}\nToken distribution (Batch 25): {16: 1, 47: 1, 12: 1, 1: 1}\nToken distribution (Batch 26): {22: 4, 10: 1, 6: 1}\nToken distribution (Batch 27): {63: 2, 27: 1, 42: 1}\nToken distribution (Batch 28): {48: 1, 34: 1, 1: 1, 50: 1, 30: 1, 32: 1, 11: 1, 61: 1}\nToken distribution (Batch 29): {32: 2, 1: 3, 47: 1, 40: 1, 34: 2, 6: 1}\nToken distribution (Batch 30): {63: 1, 34: 1, 1: 1, 19: 1}\nToken distribution (Batch 31): {1: 2, 50: 1, 34: 2, 11: 2, 27: 2, 19: 1}\nBatch 50, Gradient norm: 61.6643\nEpoch 3, Batch 50/66, Loss: 70.1027\nAvg Blank Probability: 0.0169\nSample predictions: ['qs', 'QjyE2Pwka', 'jI']\nGround Truth (first 3): ['h', 'HzLVh', 'P']\nRaw outputs (first 3): [[17 43 10 42 19  4  0  4  1  6  4  1 32  6 56  5 11  1 42 47 34 19 10  6\n   1 16 22 63 48 32 63  1]\n [19 10 35  1 47 50 10  1  1  1 25 47 11 10 42 35 47 43 50 11 27 22 56  1\n  63 47 22 27 34  1 34 50]\n [ 6 25 27  4 30 10 25 48  1  1  1 30 27  1 12 10 47 27  1 47  4  1 42  1\n  42 12 10 63  1  1  1 34]]\nInput length: 32, Label lengths: [1, 5, 1]\nToken distribution (Batch 0): {63: 1, 6: 1, 24: 1, 47: 1}\nToken distribution (Batch 1): {1: 2, 56: 1, 41: 1, 19: 1, 6: 1, 20: 1, 48: 1}\nToken distribution (Batch 2): {10: 1, 19: 1, 5: 1, 11: 1, 1: 1, 7: 1, 27: 1, 34: 1}\nToken distribution (Batch 3): {47: 1, 12: 1, 48: 2, 1: 3, 11: 1, 20: 1, 53: 1}\nToken distribution (Batch 4): {34: 1, 1: 1}\nToken distribution (Batch 5): {1: 2}\nToken distribution (Batch 6): {4: 2, 27: 1, 63: 1}\nToken distribution (Batch 7): {1: 3, 63: 1, 34: 1, 22: 1, 27: 1, 10: 1}\nToken distribution (Batch 8): {4: 1, 5: 1}\nToken distribution (Batch 9): {22: 1, 16: 1, 1: 1, 46: 1}\nToken distribution (Batch 10): {35: 1, 1: 1, 19: 1, 50: 1, 34: 2, 10: 1, 20: 1}\nToken distribution (Batch 11): {56: 1, 1: 1, 22: 1, 11: 1}\nToken distribution (Batch 12): {10: 1, 56: 2, 4: 1, 1: 3, 57: 1}\nToken distribution (Batch 13): {47: 1, 23: 1, 11: 2, 24: 1, 50: 1, 27: 1, 1: 1, 5: 1, 32: 1}\nToken distribution (Batch 14): {24: 1, 12: 2, 10: 1, 47: 1, 1: 1}\nToken distribution (Batch 15): {33: 1, 22: 1, 11: 1, 32: 1}\nToken distribution (Batch 16): {1: 1, 34: 1, 10: 2, 4: 1, 12: 1}\nToken distribution (Batch 17): {11: 1, 1: 3}\nToken distribution (Batch 18): {1: 1, 19: 1}\nToken distribution (Batch 19): {10: 1, 24: 1, 13: 1, 1: 2, 6: 1, 43: 2, 35: 1, 56: 1}\nToken distribution (Batch 20): {6: 1, 46: 1}\nToken distribution (Batch 21): {1: 1, 31: 1}\nToken distribution (Batch 22): {34: 2, 6: 1, 35: 1, 1: 2, 27: 1, 20: 1, 55: 1, 22: 1}\nToken distribution (Batch 23): {34: 2, 50: 1, 47: 1}\nToken distribution (Batch 24): {10: 2, 56: 1, 32: 1, 47: 1, 42: 1}\nToken distribution (Batch 25): {22: 1, 1: 1, 4: 1, 12: 2, 34: 1}\nToken distribution (Batch 26): {12: 1, 1: 1, 47: 1, 43: 1, 10: 2}\nToken distribution (Batch 27): {11: 1, 10: 3, 34: 1, 5: 1, 1: 2, 25: 1, 40: 1}\nToken distribution (Batch 28): {6: 1, 1: 1, 35: 1, 34: 1, 47: 1, 12: 1}\nToken distribution (Batch 29): {4: 2, 11: 1, 34: 1, 19: 2, 10: 1, 32: 1, 1: 1, 42: 1}\nToken distribution (Batch 30): {47: 1, 1: 2, 5: 1, 20: 1, 6: 1}\nToken distribution (Batch 31): {56: 1, 1: 3, 6: 2, 21: 1, 29: 1, 14: 1, 32: 1}\nBatch 60, Gradient norm: 123.5599\nEpoch 3, Batch 60/66, Loss: 62.6717\nAvg Blank Probability: 0.0172\nSample predictions: ['-fxU', 'a3OsaftV', 'jsekagAH']\nGround Truth (first 3): ['x*', 'z2xI', 'ddhf']\nRaw outputs (first 3): [[63  1 10 47 34  1  4  1  4 22 35 56 10 47 24 33  1 11  1 10  0  1 34 34\n  10 22 12 11  6  4 47 56]\n [ 6 56 19 12  1  1 27 63  5 16  1  1 56 23 12 22 34  1 19 24 46 31  6 50\n  56  1  1 10  1 11  1  1]\n [24 41  5 48 22 30  4  1 10  1 19 22  4 11 10 11 10  1  1 13 35  1 35 47\n  32  4 47 34 35 34  1  1]]\nInput length: 32, Label lengths: [2, 4, 4]\nEpoch 3/20, Loss: 62.7180\nToken distribution (Batch 0): {1: 10}\nToken distribution (Batch 1): {1: 6}\nToken distribution (Batch 2): {1: 10}\nToken distribution (Batch 3): {1: 10}\nToken distribution (Batch 4): {1: 10}\nToken distribution (Batch 5): {1: 10}\nToken distribution (Batch 6): {1: 4}\nToken distribution (Batch 7): {1: 10}\nToken distribution (Batch 8): {1: 8}\nValidation Loss: 62.0800\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['XgM1X', 'UVE', 'oChe-', 'NjDj6', 'i9ud8']\nCurrent Learning Rate: 9.903077819202327e-07\nEpoch 4, Filtered data size: 2604, Sample labels: ['qV8i', 'i', 'PnB']\nTrain size: 2083, Val size: 521\nToken distribution (Batch 0): {35: 1, 1: 1, 6: 1, 22: 1, 32: 2, 27: 1, 50: 1}\nToken distribution (Batch 1): {5: 1, 63: 1, 4: 1, 6: 1}\nToken distribution (Batch 2): {6: 1, 48: 1}\nToken distribution (Batch 3): {56: 2, 35: 1, 19: 2, 6: 2, 10: 1, 1: 1, 20: 1}\nToken distribution (Batch 4): {10: 1, 34: 1, 4: 1, 32: 1, 35: 1, 5: 1, 1: 2}\nToken distribution (Batch 5): {16: 1, 7: 1, 48: 2, 25: 1, 47: 1, 27: 1, 1: 1, 42: 1, 6: 1}\nToken distribution (Batch 6): {47: 1, 10: 1, 50: 2, 9: 1, 27: 1, 48: 1, 32: 1}\nToken distribution (Batch 7): {1: 2, 47: 1, 22: 1, 27: 1, 19: 1}\nToken distribution (Batch 8): {1: 1, 34: 1}\nToken distribution (Batch 9): {1: 1, 56: 1, 43: 1, 28: 1}\nToken distribution (Batch 10): {1: 4, 35: 1, 40: 1, 42: 1, 9: 1}\nToken distribution (Batch 11): {61: 1, 47: 1, 1: 1, 50: 1, 4: 1, 10: 1, 35: 1, 25: 1}\nToken distribution (Batch 12): {4: 1, 12: 1, 11: 1, 5: 1}\nToken distribution (Batch 13): {4: 1, 47: 1}\nToken distribution (Batch 14): {27: 1, 32: 1, 19: 1, 42: 1}\nToken distribution (Batch 15): {35: 1, 34: 2, 12: 1, 47: 1, 44: 1, 1: 3, 11: 1}\nToken distribution (Batch 16): {1: 1, 4: 1}\nToken distribution (Batch 17): {56: 2, 10: 1, 1: 1, 24: 1, 53: 1}\nToken distribution (Batch 18): {10: 2, 35: 1, 34: 1, 19: 1, 42: 1}\nToken distribution (Batch 19): {10: 1, 32: 1}\nToken distribution (Batch 20): {20: 1, 25: 1, 1: 1, 30: 1, 10: 1, 6: 1}\nToken distribution (Batch 21): {1: 2, 32: 1, 10: 1}\nToken distribution (Batch 22): {11: 1, 10: 1, 63: 1, 1: 1, 61: 1, 35: 1}\nToken distribution (Batch 23): {30: 1, 47: 1}\nToken distribution (Batch 24): {19: 1, 11: 1}\nToken distribution (Batch 25): {1: 1, 47: 1}\nToken distribution (Batch 26): {22: 1, 11: 2, 1: 4, 4: 1}\nToken distribution (Batch 27): {1: 1, 25: 1, 14: 1, 6: 1, 45: 1, 61: 1}\nToken distribution (Batch 28): {1: 1, 30: 1, 34: 2, 47: 1, 10: 1, 44: 1, 13: 1}\nToken distribution (Batch 29): {63: 1, 12: 1, 35: 1, 1: 2, 19: 1, 24: 1, 21: 1}\nToken distribution (Batch 30): {32: 1, 1: 1}\nToken distribution (Batch 31): {47: 2, 18: 1, 19: 1, 10: 1, 27: 1, 55: 1, 35: 2, 9: 1}\nBatch 0, Gradient norm: 64.8046\nEpoch 4, Batch 0/66, Loss: 69.8123\nAvg Blank Probability: 0.0174\nSample predictions: ['IafvFAX', 'e-df', 'fV']\nGround Truth (first 3): ['r8lu', 'Xf', '-']\nRaw outputs (first 3): [[35  5  6 56 10 16 47  1  1  1  1 61  4  4 27 35  1 56 10 10 20  1 11 30\n  19  1 22  1  1 63 32 47]\n [ 1 63 48 35 34  7 10 47 34 56 35 47 12 47 32 34  4 10  0 32 25 32 10 47\n   0 47 11 25 30 12  1 18]\n [ 6  4  1 19  4 48 50 22 61 43 40  1 11  1 19 12 11  1 34 35  1 10 63 61\n  19  5  1 14 34 35  1 19]]\nInput length: 32, Label lengths: [4, 2, 1]\nToken distribution (Batch 0): {34: 2, 32: 1, 18: 1, 47: 2, 56: 1, 22: 1}\nToken distribution (Batch 1): {22: 1, 1: 3}\nToken distribution (Batch 2): {6: 1, 1: 4, 4: 1, 10: 1, 55: 1}\nToken distribution (Batch 3): {32: 1, 1: 2, 24: 1, 21: 1, 6: 1, 42: 1, 11: 1, 25: 1, 34: 1}\nToken distribution (Batch 4): {1: 2, 22: 1, 32: 3, 4: 1, 27: 2, 42: 1}\nToken distribution (Batch 5): {30: 1, 1: 1, 22: 2, 32: 1, 48: 1, 41: 1, 34: 1, 6: 1, 47: 1}\nToken distribution (Batch 6): {42: 2, 47: 1, 1: 3, 24: 2, 5: 1, 19: 1}\nToken distribution (Batch 7): {4: 1, 32: 1, 5: 1, 47: 1, 1: 2, 11: 2, 22: 1, 35: 1}\nToken distribution (Batch 8): {6: 2, 32: 1, 1: 2, 11: 1}\nToken distribution (Batch 9): {10: 1, 4: 1}\nToken distribution (Batch 10): {32: 1, 43: 1}\nToken distribution (Batch 11): {12: 1, 34: 5, 35: 1, 6: 1, 29: 1, 32: 1}\nToken distribution (Batch 12): {42: 1, 19: 3, 20: 1, 62: 1, 32: 1, 55: 1}\nToken distribution (Batch 13): {12: 1, 47: 1}\nToken distribution (Batch 14): {1: 1, 10: 1}\nToken distribution (Batch 15): {10: 2, 1: 4, 2: 1, 56: 2, 50: 1}\nToken distribution (Batch 16): {63: 1, 11: 1, 1: 2, 6: 3, 34: 1, 50: 1, 10: 1}\nToken distribution (Batch 17): {6: 2, 5: 1, 10: 1}\nToken distribution (Batch 18): {63: 1, 6: 2, 11: 1, 50: 1, 20: 1}\nToken distribution (Batch 19): {63: 1, 30: 1}\nToken distribution (Batch 20): {11: 1, 24: 1}\nToken distribution (Batch 21): {32: 1, 1: 1}\nToken distribution (Batch 22): {12: 2, 7: 1, 48: 1, 1: 1, 50: 1, 6: 1, 34: 1, 25: 1, 10: 1}\nToken distribution (Batch 23): {34: 1, 1: 1}\nToken distribution (Batch 24): {1: 1, 12: 1, 10: 1, 42: 1}\nToken distribution (Batch 25): {35: 1, 56: 1, 10: 1, 32: 1, 1: 2, 34: 1, 19: 2, 5: 1}\nToken distribution (Batch 26): {1: 3, 30: 1, 10: 1, 56: 1}\nToken distribution (Batch 27): {13: 1, 12: 1, 34: 1, 24: 1, 10: 2}\nToken distribution (Batch 28): {34: 2}\nToken distribution (Batch 29): {1: 1, 12: 1}\nToken distribution (Batch 30): {10: 1, 32: 1, 16: 1, 1: 3}\nToken distribution (Batch 31): {6: 1, 47: 1}\nBatch 10, Gradient norm: 73.7940\nEpoch 4, Batch 10/66, Loss: 72.3960\nAvg Blank Probability: 0.0178\nSample predictions: ['HFrUHU3v', 'va', 'fadja2a']\nGround Truth (first 3): ['zypb', 'P0', 'hrNK']\nRaw outputs (first 3): [[34 22  6 32  1 30 42  4  6 10 32 12 42 12  1 10 63  6 63 63 11 32 12 34\n   1 35  1 13 34  1 10  6]\n [32  1  1  1 22  1 47 32 32  4 43 34 19 47 10  1 11  5  6 30 24  1 12  1\n  12 56 30 12 34 12 32 47]\n [18  1  4 24 32 22  1  5  1  4 10 34 20 10  1  2  1 10  6 10  1  1  7  1\n  10 10 10 34 42 11 16  1]]\nInput length: 32, Label lengths: [4, 2, 4]\nToken distribution (Batch 0): {63: 1, 47: 1, 12: 1, 35: 1}\nToken distribution (Batch 1): {20: 1, 34: 1, 43: 1, 1: 3, 21: 1, 28: 1, 9: 1, 32: 1}\nToken distribution (Batch 2): {1: 1, 22: 1, 48: 1, 43: 1}\nToken distribution (Batch 3): {25: 1, 6: 1, 55: 1, 11: 1, 51: 1, 35: 1, 43: 1, 27: 1}\nToken distribution (Batch 4): {10: 1, 52: 1}\nToken distribution (Batch 5): {34: 1, 63: 1}\nToken distribution (Batch 6): {27: 2, 35: 1, 6: 1, 1: 1, 10: 1}\nToken distribution (Batch 7): {19: 1, 24: 1, 20: 1, 1: 1}\nToken distribution (Batch 8): {13: 1, 11: 1, 56: 1, 9: 1, 5: 1, 1: 1}\nToken distribution (Batch 9): {10: 1, 1: 1, 32: 1, 6: 1}\nToken distribution (Batch 10): {47: 2, 16: 1, 8: 1, 4: 1, 32: 1, 10: 1, 1: 1}\nToken distribution (Batch 11): {33: 1, 4: 2, 32: 1, 12: 1, 47: 1}\nToken distribution (Batch 12): {34: 3, 1: 1}\nToken distribution (Batch 13): {47: 2, 10: 1, 12: 1}\nToken distribution (Batch 14): {1: 5, 47: 2, 6: 1, 56: 1, 25: 1}\nToken distribution (Batch 15): {12: 1, 1: 1}\nToken distribution (Batch 16): {1: 1, 56: 1, 10: 1, 47: 1}\nToken distribution (Batch 17): {1: 3, 47: 1}\nToken distribution (Batch 18): {24: 1, 10: 1, 1: 1, 34: 1, 32: 3, 27: 1}\nToken distribution (Batch 19): {6: 1, 33: 1, 19: 1, 1: 1, 5: 1, 11: 1, 14: 1, 43: 1}\nToken distribution (Batch 20): {1: 2}\nToken distribution (Batch 21): {12: 1, 5: 1, 1: 1, 19: 1, 32: 1, 10: 1}\nToken distribution (Batch 22): {11: 2, 1: 1, 22: 1, 42: 1, 24: 1}\nToken distribution (Batch 23): {27: 2, 42: 2, 47: 1, 16: 1}\nToken distribution (Batch 24): {28: 1, 1: 1, 34: 1, 19: 1, 63: 1, 10: 1}\nToken distribution (Batch 25): {1: 2, 24: 1, 25: 1, 11: 1, 32: 1}\nToken distribution (Batch 26): {6: 1, 33: 1, 34: 1, 35: 1, 7: 1, 11: 1}\nToken distribution (Batch 27): {22: 1, 32: 2, 6: 1}\nToken distribution (Batch 28): {43: 1, 61: 1, 32: 1, 35: 1, 10: 1, 27: 1}\nToken distribution (Batch 29): {10: 1, 34: 1, 30: 1, 32: 2, 20: 1, 7: 1, 1: 2, 47: 1}\nToken distribution (Batch 30): {1: 2, 4: 1, 6: 1, 35: 1, 32: 2, 23: 1}\nToken distribution (Batch 31): {1: 2, 10: 1, 22: 1}\nBatch 20, Gradient norm: 67.8976\nEpoch 4, Batch 20/66, Loss: 61.0115\nAvg Blank Probability: 0.0180\nSample predictions: ['-UlI', 'tHQauBaiFa', 'avVQ']\nGround Truth (first 3): ['WJ', 'tgEPL', '-k']\nRaw outputs (first 3): [[63 20  1 25 10 34 27 19 13 10 47 33 34 47  1 12  1  1 24  6  1 12 11 27\n  28  1  6 22 43 10  1  1]\n [47 34 22  6 52 63 35 24 11  1 16  4  1 47  1  1 56 47 10 33  1  5  1 42\n   1 24 33 32 61 34  4 10]\n [12 43 48 55 13  4  6 20 56 32  8  4 34 10 47 38 10  1  1 19 16  1 11 47\n  34  1 34  6 32 30  1  1]]\nInput length: 32, Label lengths: [2, 5, 2]\nToken distribution (Batch 0): {27: 1, 19: 1, 1: 4, 56: 1, 10: 1}\nToken distribution (Batch 1): {1: 1, 10: 2, 25: 1}\nToken distribution (Batch 2): {47: 1, 34: 2, 32: 2, 1: 2, 50: 1}\nToken distribution (Batch 3): {1: 2, 6: 1, 56: 1}\nToken distribution (Batch 4): {23: 1, 19: 2, 56: 1}\nToken distribution (Batch 5): {34: 1, 43: 1, 27: 2, 30: 1, 4: 2, 22: 1}\nToken distribution (Batch 6): {32: 3, 47: 1, 50: 1, 31: 1, 63: 1, 17: 1, 35: 1, 43: 1}\nToken distribution (Batch 7): {1: 2, 10: 1, 27: 2, 6: 1}\nToken distribution (Batch 8): {10: 2, 47: 1, 13: 1}\nToken distribution (Batch 9): {34: 1, 56: 2, 5: 1, 1: 1, 11: 1, 13: 1, 4: 1}\nToken distribution (Batch 10): {47: 1, 1: 1, 33: 1, 5: 1}\nToken distribution (Batch 11): {12: 1, 33: 1}\nToken distribution (Batch 12): {50: 1, 35: 1, 10: 1, 1: 3}\nToken distribution (Batch 13): {4: 1, 22: 1, 34: 2, 6: 1, 28: 1, 19: 1, 43: 1, 56: 1, 11: 1}\nToken distribution (Batch 14): {6: 1, 12: 1, 56: 1, 1: 1, 10: 2, 27: 1, 34: 1}\nToken distribution (Batch 15): {25: 1, 10: 1, 22: 1, 32: 1}\nToken distribution (Batch 16): {22: 1, 47: 1, 1: 2}\nToken distribution (Batch 17): {56: 1, 22: 1, 27: 1, 36: 1}\nToken distribution (Batch 18): {47: 1, 1: 3, 10: 1, 43: 1, 11: 1, 22: 1}\nToken distribution (Batch 19): {1: 1, 63: 1}\nToken distribution (Batch 20): {47: 1, 27: 2, 19: 2, 6: 1, 23: 1, 11: 2, 32: 1}\nToken distribution (Batch 21): {20: 1, 12: 1}\nToken distribution (Batch 22): {42: 2, 19: 2, 4: 1, 27: 1, 32: 1, 25: 1, 47: 1, 20: 1}\nToken distribution (Batch 23): {48: 1, 13: 1, 1: 4, 12: 1, 23: 1}\nToken distribution (Batch 24): {10: 1, 34: 1}\nToken distribution (Batch 25): {4: 1, 1: 2, 12: 2, 34: 1}\nToken distribution (Batch 26): {1: 1, 12: 1, 18: 1, 32: 1}\nToken distribution (Batch 27): {43: 1, 10: 1, 41: 1, 16: 1}\nToken distribution (Batch 28): {56: 1, 1: 1}\nToken distribution (Batch 29): {12: 1, 6: 1, 11: 1, 1: 1, 4: 1, 21: 1}\nToken distribution (Batch 30): {4: 1, 19: 1, 27: 1, 56: 1}\nToken distribution (Batch 31): {1: 1, 56: 1, 42: 1, 34: 2, 9: 1}\nBatch 30, Gradient norm: 62.4164\nEpoch 4, Batch 30/66, Loss: 63.0691\nAvg Blank Probability: 0.0184\nSample predictions: ['Asa3aj', 'ajy', 'UHFaFHaX']\nGround Truth (first 3): ['DwBi', 'G5', 'SqlG']\nRaw outputs (first 3): [[27  1 47  1 23 34 32  1 10 34 47 12 50  4  6 25 22 56 47  1 47 20 42 48\n  10  4  1 43 56 12  4  1]\n [19 10 34  6 19 43  0 10 47 56  1 33 35 22 12 10 47 22  1 63 27 12 19 13\n  34  1 12 10  1  6 19 56]\n [ 1 10 32  1 19 27 50 27 10  5 33  1 10 34 56 22  1 27 10 48 19 42  4  1\n   5 12 18 41 34 11 27 42]]\nInput length: 32, Label lengths: [4, 2, 4]\nToken distribution (Batch 0): {48: 1, 12: 1}\nToken distribution (Batch 1): {6: 1, 32: 1}\nToken distribution (Batch 2): {1: 1, 34: 1, 19: 1, 35: 1}\nToken distribution (Batch 3): {10: 1, 5: 2, 30: 1, 1: 1, 46: 1}\nToken distribution (Batch 4): {32: 1, 35: 1, 57: 1, 50: 1, 19: 1, 10: 1, 56: 2}\nToken distribution (Batch 5): {22: 1, 1: 1}\nToken distribution (Batch 6): {1: 3, 34: 1, 43: 1, 11: 2, 24: 1, 10: 1, 5: 1}\nToken distribution (Batch 7): {1: 3, 10: 1, 42: 1, 47: 1}\nToken distribution (Batch 8): {30: 3, 24: 1, 4: 1, 11: 1, 19: 1, 5: 1}\nToken distribution (Batch 9): {43: 1, 35: 1, 56: 2, 51: 1, 34: 1, 1: 2}\nToken distribution (Batch 10): {32: 2, 1: 2}\nToken distribution (Batch 11): {10: 1, 4: 1, 24: 1, 1: 2, 30: 1, 48: 1, 9: 1, 11: 1, 5: 1}\nToken distribution (Batch 12): {32: 2, 10: 1, 1: 2, 35: 1, 25: 1, 56: 1}\nToken distribution (Batch 13): {24: 1, 56: 2, 1: 1}\nToken distribution (Batch 14): {4: 1, 50: 1, 6: 1, 1: 2, 47: 1}\nToken distribution (Batch 15): {1: 2, 34: 2}\nToken distribution (Batch 16): {27: 1, 48: 1, 1: 1, 47: 1}\nToken distribution (Batch 17): {32: 1, 30: 1, 34: 1, 35: 1, 6: 1, 4: 2, 1: 1, 48: 1, 19: 1}\nToken distribution (Batch 18): {47: 1, 24: 2, 35: 2, 34: 1, 13: 1, 50: 1, 61: 1, 27: 1}\nToken distribution (Batch 19): {1: 2}\nToken distribution (Batch 20): {29: 1, 34: 1, 1: 3, 55: 1, 6: 1, 63: 1}\nToken distribution (Batch 21): {50: 1, 1: 1}\nToken distribution (Batch 22): {42: 1, 22: 2, 27: 1, 20: 1, 43: 2, 47: 1, 32: 1, 30: 1}\nToken distribution (Batch 23): {34: 1, 1: 2, 11: 2, 19: 1}\nToken distribution (Batch 24): {1: 3, 10: 1, 11: 3, 48: 1, 43: 1, 35: 1}\nToken distribution (Batch 25): {51: 1, 56: 1}\nToken distribution (Batch 26): {12: 1, 34: 1, 1: 4, 32: 1, 10: 1, 27: 1, 24: 1}\nToken distribution (Batch 27): {63: 1, 10: 1, 1: 3, 11: 2, 24: 1, 9: 1, 43: 1}\nToken distribution (Batch 28): {48: 1, 10: 2, 43: 1, 33: 1, 56: 1}\nToken distribution (Batch 29): {11: 1, 32: 1, 47: 1, 22: 1, 10: 1, 1: 1, 42: 1, 43: 1}\nToken distribution (Batch 30): {4: 1, 1: 3, 11: 1, 12: 2, 20: 1, 51: 1, 34: 1}\nToken distribution (Batch 31): {14: 1, 1: 1}\nBatch 40, Gradient norm: 3103.3423\nEpoch 4, Batch 40/66, Loss: 62.1004\nAvg Blank Probability: 0.0187\nSample predictions: ['Vl', 'fF', 'aHsI']\nGround Truth (first 3): ['o', 'D', 'ce']\nRaw outputs (first 3): [[48  6  1 10 32 22  1  1 30 43 32 10 32 24  4  1 27 32 47  1 29 50 42 34\n   1 51 12 63 48 11  4 14]\n [12 32 34  5 35  1 34 10 24 35 32  4 10 56 50 34 48 30 24  1 34  1 22  1\n  10 56 34 10 10 32  1  1]\n [ 5 56 19  5 57  1  1 42 30 56  1 24  1 56  6  1  1 34 35 36  1 42 27 11\n  11 22  1  1 43 47  1  1]]\nInput length: 32, Label lengths: [1, 1, 2]\nToken distribution (Batch 0): {11: 1, 1: 2, 24: 1, 60: 1, 6: 1, 31: 1, 56: 1}\nToken distribution (Batch 1): {10: 1, 1: 1, 32: 2}\nToken distribution (Batch 2): {10: 1, 16: 1}\nToken distribution (Batch 3): {30: 1, 9: 1, 16: 1, 1: 1, 11: 1, 34: 1}\nToken distribution (Batch 4): {6: 1, 42: 1}\nToken distribution (Batch 5): {10: 1, 55: 1}\nToken distribution (Batch 6): {1: 2, 48: 1, 10: 1, 11: 2, 34: 1, 24: 1}\nToken distribution (Batch 7): {47: 1, 12: 1, 20: 1, 4: 1}\nToken distribution (Batch 8): {25: 2, 34: 3, 10: 1, 11: 1, 20: 1}\nToken distribution (Batch 9): {1: 1, 47: 1, 63: 1, 5: 1}\nToken distribution (Batch 10): {34: 1, 12: 1}\nToken distribution (Batch 11): {5: 1, 1: 3, 12: 1, 10: 1, 29: 1, 16: 1}\nToken distribution (Batch 12): {25: 2, 12: 1, 32: 1, 1: 1, 34: 1}\nToken distribution (Batch 13): {19: 1, 4: 1, 42: 1, 43: 1, 22: 1, 34: 1}\nToken distribution (Batch 14): {6: 1, 32: 1, 47: 1, 43: 1}\nToken distribution (Batch 15): {47: 2, 34: 2, 19: 1, 32: 1, 35: 1, 27: 1}\nToken distribution (Batch 16): {1: 1, 10: 1}\nToken distribution (Batch 17): {63: 1, 6: 1, 10: 1, 7: 1, 32: 1, 25: 1}\nToken distribution (Batch 18): {12: 2, 1: 2, 34: 1, 32: 2, 5: 1, 42: 1, 56: 1}\nToken distribution (Batch 19): {16: 1, 1: 1, 10: 1, 4: 1}\nToken distribution (Batch 20): {22: 1, 7: 1, 47: 1, 34: 1, 57: 1, 1: 1}\nToken distribution (Batch 21): {47: 1, 1: 1}\nToken distribution (Batch 22): {1: 4, 13: 1, 6: 1, 35: 1, 32: 1, 19: 1, 43: 1}\nToken distribution (Batch 23): {56: 1, 1: 1}\nToken distribution (Batch 24): {63: 1, 1: 3, 12: 1, 47: 1, 7: 1, 11: 1, 40: 1, 5: 1}\nToken distribution (Batch 25): {4: 1, 1: 1}\nToken distribution (Batch 26): {27: 2, 10: 1, 1: 4, 11: 1, 6: 1, 23: 1}\nToken distribution (Batch 27): {10: 1, 20: 1, 19: 1, 14: 1}\nToken distribution (Batch 28): {24: 1, 22: 2, 34: 1, 19: 1, 35: 1}\nToken distribution (Batch 29): {6: 1, 10: 1, 1: 2, 11: 1, 22: 1, 24: 1, 32: 1, 56: 2}\nToken distribution (Batch 30): {32: 1, 11: 1}\nToken distribution (Batch 31): {10: 1, 1: 2, 32: 1, 22: 1, 47: 1, 6: 1, 26: 1}\nBatch 50, Gradient norm: 145268.4062\nEpoch 4, Batch 50/66, Loss: 69.5823\nAvg Blank Probability: 0.0192\nSample predictions: ['kax7faE3', 'jaF', 'jp']\nGround Truth (first 3): ['V6lI', '55', '3']\nRaw outputs (first 3): [[11 10 10 30  6 10  1 47 25  1 34  5 25 19  6 47  1 63  0 16 22 47  1 56\n  63  4 27 10 24  6 32 10]\n [ 1  1 16  9  0 55 48 12 34 47  0  1 12  4 32 34 10  6  1  1  7  1  1  1\n   1  1 10 20 22 10 11  1]\n [24 32  1 16 34  1 10 20 10 63 12 12 32 42 47 19 32 10 12 10 47 11 13 56\n  12 11 27 19 22  1  1  1]]\nInput length: 32, Label lengths: [4, 2, 1]\nToken distribution (Batch 0): {1: 3, 55: 1, 34: 1, 27: 1, 47: 1, 61: 1}\nToken distribution (Batch 1): {43: 1, 1: 2, 50: 2, 47: 1, 10: 2, 20: 1, 11: 1}\nToken distribution (Batch 2): {12: 1, 34: 2, 9: 1, 1: 2, 32: 1, 22: 1}\nToken distribution (Batch 3): {12: 2, 1: 1, 47: 1, 30: 1, 10: 1, 19: 1, 11: 1}\nToken distribution (Batch 4): {34: 1, 1: 2, 55: 1}\nToken distribution (Batch 5): {61: 1, 10: 1, 48: 1, 47: 1, 19: 1, 1: 1}\nToken distribution (Batch 6): {56: 1, 43: 1, 1: 1, 55: 1}\nToken distribution (Batch 7): {34: 1, 1: 3, 19: 1, 22: 1, 27: 1, 10: 1}\nToken distribution (Batch 8): {25: 1, 11: 1, 12: 1, 1: 3, 50: 1, 22: 1}\nToken distribution (Batch 9): {11: 1, 6: 1, 21: 1, 1: 1}\nToken distribution (Batch 10): {11: 1, 63: 1, 1: 1, 19: 1, 27: 1, 56: 1}\nToken distribution (Batch 11): {24: 2, 32: 2, 50: 1, 1: 1, 22: 1, 10: 1}\nToken distribution (Batch 12): {33: 1, 47: 1, 1: 1, 35: 1}\nToken distribution (Batch 13): {10: 1, 1: 2, 12: 1}\nToken distribution (Batch 14): {48: 1, 47: 1, 1: 1, 59: 1, 22: 1, 10: 1}\nToken distribution (Batch 15): {61: 1, 47: 1, 32: 1, 4: 1, 1: 2, 16: 1, 11: 1, 12: 2}\nToken distribution (Batch 16): {1: 2}\nToken distribution (Batch 17): {10: 1, 5: 1, 7: 1, 19: 1}\nToken distribution (Batch 18): {34: 1, 6: 1, 11: 1, 1: 1, 27: 1, 32: 2, 20: 1}\nToken distribution (Batch 19): {10: 1, 1: 1, 11: 2, 47: 1, 55: 1}\nToken distribution (Batch 20): {11: 1, 10: 1, 27: 1, 47: 1, 56: 1, 30: 1, 1: 1, 60: 1}\nToken distribution (Batch 21): {10: 1, 22: 1}\nToken distribution (Batch 22): {5: 1, 63: 1, 19: 1, 10: 1, 43: 2, 56: 1, 25: 1, 30: 1, 47: 1}\nToken distribution (Batch 23): {10: 1, 34: 1}\nToken distribution (Batch 24): {6: 1, 30: 1, 11: 1, 1: 1, 22: 1, 32: 1, 57: 1, 56: 1}\nToken distribution (Batch 25): {33: 1, 25: 1, 34: 1, 4: 2, 22: 1, 47: 1, 42: 1, 20: 1, 5: 1}\nToken distribution (Batch 26): {57: 1, 12: 1, 32: 1, 43: 1}\nToken distribution (Batch 27): {22: 1, 1: 2, 10: 1, 12: 1, 5: 1, 27: 1, 61: 1, 7: 1, 56: 1}\nToken distribution (Batch 28): {47: 1, 10: 1, 32: 1, 1: 1, 56: 1, 55: 1}\nToken distribution (Batch 29): {1: 3, 27: 1, 6: 1, 22: 1, 4: 1, 34: 1}\nToken distribution (Batch 30): {63: 3, 19: 1, 33: 1, 61: 1}\nToken distribution (Batch 31): {35: 1, 1: 1, 22: 2}\nBatch 60, Gradient norm: 54.8274\nEpoch 4, Batch 60/66, Loss: 53.6539\nAvg Blank Probability: 0.0198\nSample predictions: ['a2HAaU8', 'QaXUjtakjX', 'lHiaFHav']\nGround Truth (first 3): ['TDXV', 'jfCBl', 'vOUX']\nRaw outputs (first 3): [[ 1 43 12 12 34 61 56 34 25 11 11 24 33 10 48 61  1 10 34 10 11 10  5 10\n   6 33 57 22 47  1 63 35]\n [ 1  1 34  1  1 10  0  1 11  6 63 32 47  1 47 47  1  5  6  1 10 22 63 34\n  30 25 12  1 10  1 63  1]\n [55 50  9 12  1 48  1  1 12 21  1 50  1  1  1 32 24  7 11 11  0 34 19 10\n   0 34 32  1 32  1  0 22]]\nInput length: 32, Label lengths: [4, 5, 4]\nEpoch 4/20, Loss: 61.9858\nToken distribution (Batch 0): {1: 6}\nToken distribution (Batch 1): {1: 4}\nToken distribution (Batch 2): {1: 6}\nToken distribution (Batch 3): {1: 8}\nToken distribution (Batch 4): {1: 10}\nToken distribution (Batch 5): {1: 4}\nToken distribution (Batch 6): {1: 8}\nToken distribution (Batch 7): {1: 4}\nToken distribution (Batch 8): {1: 10}\nValidation Loss: 62.6920\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['sIU', 'Dt', 'w19', 'h44Q', 'VjYzO']\nCurrent Learning Rate: 1.3061255637118674e-06\nEpoch 5, Filtered data size: 2604, Sample labels: ['qV8i', 'i', 'PnB']\nTrain size: 2083, Val size: 521\nToken distribution (Batch 0): {11: 2, 34: 1, 9: 1, 32: 1, 1: 1, 10: 1, 19: 1}\nToken distribution (Batch 1): {11: 2, 27: 1, 10: 1, 56: 1, 43: 1}\nToken distribution (Batch 2): {1: 4, 11: 1, 6: 1, 10: 1, 34: 2, 56: 1}\nToken distribution (Batch 3): {63: 1, 12: 1, 25: 1, 1: 1, 10: 1, 42: 1}\nToken distribution (Batch 4): {12: 1, 10: 1}\nToken distribution (Batch 5): {11: 1, 1: 1}\nToken distribution (Batch 6): {10: 2}\nToken distribution (Batch 7): {10: 1, 42: 1, 48: 2, 11: 1, 4: 1}\nToken distribution (Batch 8): {11: 1, 24: 1}\nToken distribution (Batch 9): {13: 1, 34: 1, 47: 1, 32: 2, 1: 1, 10: 1, 5: 1}\nToken distribution (Batch 10): {61: 1, 34: 1, 6: 1, 35: 1}\nToken distribution (Batch 11): {11: 1, 27: 1, 14: 1, 47: 1, 34: 1, 1: 2, 59: 1}\nToken distribution (Batch 12): {56: 1, 32: 1, 1: 1, 10: 1}\nToken distribution (Batch 13): {14: 1, 1: 1, 35: 2, 42: 1, 10: 2, 5: 1}\nToken distribution (Batch 14): {6: 2, 56: 1, 41: 1, 1: 1, 5: 1}\nToken distribution (Batch 15): {22: 1, 4: 1, 47: 1, 1: 1, 29: 1, 27: 1}\nToken distribution (Batch 16): {61: 1, 10: 1}\nToken distribution (Batch 17): {47: 2, 1: 1, 32: 1, 35: 1, 10: 1}\nToken distribution (Batch 18): {62: 1, 1: 1, 20: 1, 32: 2, 19: 1, 35: 1, 42: 1}\nToken distribution (Batch 19): {63: 1, 11: 1, 4: 1, 56: 1, 22: 1, 28: 1}\nToken distribution (Batch 20): {43: 1, 1: 2, 27: 1, 16: 1, 4: 1, 10: 1, 6: 1}\nToken distribution (Batch 21): {10: 1, 63: 1, 47: 1, 34: 1}\nToken distribution (Batch 22): {1: 1, 16: 1, 34: 2, 42: 1, 12: 1, 35: 1, 4: 1, 7: 1, 5: 1}\nToken distribution (Batch 23): {1: 2, 11: 3, 19: 1, 32: 1, 27: 1, 41: 1, 47: 1}\nToken distribution (Batch 24): {33: 1, 12: 1}\nToken distribution (Batch 25): {1: 1, 63: 1, 32: 1, 35: 1}\nToken distribution (Batch 26): {48: 1, 34: 1, 32: 1, 1: 1}\nToken distribution (Batch 27): {4: 2, 24: 1, 1: 2, 34: 1, 32: 1, 11: 1, 22: 1, 10: 1}\nToken distribution (Batch 28): {32: 1, 56: 1, 12: 1, 34: 1}\nToken distribution (Batch 29): {57: 1, 20: 1, 34: 1, 35: 1}\nToken distribution (Batch 30): {12: 1, 34: 1, 27: 1, 47: 1, 32: 1, 48: 1}\nToken distribution (Batch 31): {35: 1, 1: 1}\nBatch 0, Gradient norm: 108.8428\nEpoch 5, Batch 0/66, Loss: 65.1005\nAvg Blank Probability: 0.0202\nSample predictions: ['kHiFajsk', 'kAj3kQ', 'akafjH3Ha']\nGround Truth (first 3): ['*m4W', 'FZD', '*glGz']\nRaw outputs (first 3): [[11 11  1 63 12 11 10  0 11 13 61 11 56 14  6 22 61 47 62 63 43 10  1  1\n  33  1 48  4 32 57 12 35]\n [34 27 11 12 10  1 10 42 24 34 34 27 32  1  6  4 10  1  1  0  1  0 16 11\n  12 63 34 24 56 20 34  1]\n [ 9  0  1 25  1 10  1 48  6 47  6 14  1 35 56 47 25 47 20  4  0 47 34  1\n  50 32 32  1 12 34 27 63]]\nInput length: 32, Label lengths: [4, 3, 5]\nToken distribution (Batch 0): {11: 1, 1: 1, 47: 1, 19: 1, 42: 1, 56: 1, 38: 1, 9: 1, 12: 1, 35: 1}\nToken distribution (Batch 1): {1: 2, 47: 1, 10: 2, 12: 2, 43: 1, 6: 1, 18: 1}\nToken distribution (Batch 2): {11: 2, 29: 1, 44: 1, 35: 1, 10: 1}\nToken distribution (Batch 3): {1: 2, 61: 2, 16: 1, 4: 1, 19: 1, 55: 1, 34: 1, 6: 1}\nToken distribution (Batch 4): {50: 1, 25: 1, 32: 3, 27: 1, 10: 1, 43: 1}\nToken distribution (Batch 5): {27: 1, 48: 1, 34: 1, 24: 1, 10: 1, 12: 1, 5: 1, 32: 1}\nToken distribution (Batch 6): {11: 1, 1: 2, 34: 2, 61: 1, 13: 1, 32: 1}\nToken distribution (Batch 7): {1: 3, 7: 1, 32: 1, 47: 1, 19: 1, 22: 1}\nToken distribution (Batch 8): {1: 1, 22: 1, 43: 1, 27: 1}\nToken distribution (Batch 9): {12: 1, 47: 1}\nToken distribution (Batch 10): {42: 1, 1: 1, 10: 1, 11: 2, 27: 2, 54: 1, 32: 1, 7: 1}\nToken distribution (Batch 11): {42: 1, 10: 1, 1: 1, 41: 1}\nToken distribution (Batch 12): {55: 1, 35: 1, 1: 2, 42: 1, 50: 1, 5: 1, 48: 1, 12: 2}\nToken distribution (Batch 13): {1: 1, 53: 1, 42: 1, 34: 1, 12: 2, 11: 2, 26: 1, 35: 1}\nToken distribution (Batch 14): {6: 3, 1: 1, 42: 1, 32: 2, 10: 1}\nToken distribution (Batch 15): {10: 1, 55: 1}\nToken distribution (Batch 16): {34: 2, 23: 1, 11: 1, 1: 3, 32: 1, 25: 1, 26: 1}\nToken distribution (Batch 17): {1: 3, 6: 1, 10: 1, 19: 2, 22: 1}\nToken distribution (Batch 18): {34: 1, 24: 1, 6: 1, 22: 1, 43: 1, 1: 1}\nToken distribution (Batch 19): {11: 1, 34: 1, 1: 4, 22: 1, 47: 1}\nToken distribution (Batch 20): {34: 1, 4: 1, 42: 1, 27: 1}\nToken distribution (Batch 21): {1: 1, 4: 1, 30: 1, 32: 1}\nToken distribution (Batch 22): {1: 3, 34: 1}\nToken distribution (Batch 23): {1: 2, 12: 2, 27: 1, 32: 2, 22: 1, 20: 2}\nToken distribution (Batch 24): {1: 2, 56: 1, 27: 1, 4: 1, 10: 1}\nToken distribution (Batch 25): {50: 1, 11: 1, 22: 1, 18: 1}\nToken distribution (Batch 26): {35: 1, 27: 2, 43: 2, 1: 1, 24: 1, 11: 1, 22: 1, 19: 1}\nToken distribution (Batch 27): {1: 1, 6: 1, 38: 1, 12: 1}\nToken distribution (Batch 28): {24: 1, 1: 3, 25: 1, 10: 2, 61: 1}\nToken distribution (Batch 29): {35: 1, 34: 1, 1: 1, 42: 1, 19: 1, 5: 1, 56: 1, 6: 1}\nToken distribution (Batch 30): {36: 1, 1: 1, 11: 1, 43: 1}\nToken distribution (Batch 31): {23: 1, 32: 1}\nBatch 10, Gradient norm: 54.2235\nEpoch 5, Batch 10/66, Loss: 51.5136\nAvg Blank Probability: 0.0206\nSample predictions: ['kaUsP3LilI', 'aUjljQaflr', 'kCRIkj']\nGround Truth (first 3): ['4ylL0', 'xDoww', 'Azb']\nRaw outputs (first 3): [[11  1 11  1 50 27 11  1  1 12 42 42 55  1  6 10 34  1 34 11 34  1  1  1\n   1 50 35  1 24 35 36 23]\n [ 1 47 29  1 25 48  1  7 22 47  1 10 35  0  1 55 23  1 24 34  4  4  1  1\n  56 11  0  6  1 34  1 32]\n [47  0 44 61 32 34 34 32 43  0 10  1  1  0 42 32  0  1  6  1 42 30 34 12\n  27  0 43 38 25  1 11  1]]\nInput length: 32, Label lengths: [5, 5, 3]\nToken distribution (Batch 0): {1: 3, 47: 1, 34: 2, 3: 1, 5: 1}\nToken distribution (Batch 1): {6: 1, 22: 1, 1: 2}\nToken distribution (Batch 2): {63: 1, 56: 1, 34: 1, 55: 2, 61: 1, 9: 1, 1: 1, 12: 1, 11: 1}\nToken distribution (Batch 3): {63: 1, 19: 1, 22: 1, 50: 2, 32: 1}\nToken distribution (Batch 4): {56: 2}\nToken distribution (Batch 5): {13: 1, 1: 1}\nToken distribution (Batch 6): {6: 1, 35: 2, 10: 2, 34: 2, 30: 1}\nToken distribution (Batch 7): {22: 1, 10: 2, 1: 1}\nToken distribution (Batch 8): {1: 1, 56: 2, 34: 1, 19: 1, 12: 1, 22: 2}\nToken distribution (Batch 9): {11: 2, 30: 1, 6: 1, 4: 1, 1: 2, 63: 1}\nToken distribution (Batch 10): {35: 1, 10: 1, 25: 1, 9: 1, 32: 2, 6: 1, 1: 1}\nToken distribution (Batch 11): {1: 1, 48: 1}\nToken distribution (Batch 12): {43: 2, 19: 2, 22: 1, 28: 1, 27: 2, 25: 1, 29: 1}\nToken distribution (Batch 13): {1: 2, 56: 1, 42: 1, 43: 1, 32: 1}\nToken distribution (Batch 14): {14: 1, 43: 2, 6: 1, 27: 1, 56: 1}\nToken distribution (Batch 15): {1: 2}\nToken distribution (Batch 16): {47: 1, 1: 1, 32: 1, 10: 1, 4: 1, 19: 1}\nToken distribution (Batch 17): {1: 1, 34: 1}\nToken distribution (Batch 18): {24: 1, 1: 4, 20: 1, 11: 1, 25: 1}\nToken distribution (Batch 19): {10: 1, 30: 1, 34: 2, 35: 1, 32: 1, 22: 1, 1: 1}\nToken distribution (Batch 20): {4: 1, 47: 1, 1: 2, 50: 1, 32: 1}\nToken distribution (Batch 21): {10: 1, 4: 1, 1: 1, 35: 1}\nToken distribution (Batch 22): {10: 1, 1: 1, 27: 1, 22: 1}\nToken distribution (Batch 23): {63: 1, 43: 1, 34: 1, 30: 1, 5: 1, 1: 1}\nToken distribution (Batch 24): {4: 1, 32: 1, 34: 2, 1: 2, 42: 2, 12: 1, 35: 1}\nToken distribution (Batch 25): {4: 1, 42: 2, 22: 2, 34: 1, 50: 2}\nToken distribution (Batch 26): {5: 1, 43: 1, 12: 1, 25: 1, 35: 2, 55: 1, 4: 1}\nToken distribution (Batch 27): {19: 2, 1: 1, 11: 1, 55: 1, 48: 1}\nToken distribution (Batch 28): {56: 1, 12: 1, 42: 1, 32: 2, 27: 1, 34: 1, 35: 1}\nToken distribution (Batch 29): {38: 1, 1: 2, 32: 1, 19: 1, 47: 1, 42: 1, 13: 1}\nToken distribution (Batch 30): {19: 1, 1: 1, 25: 1, 47: 1}\nToken distribution (Batch 31): {19: 1, 1: 1}\nBatch 20, Gradient norm: 68.3799\nEpoch 5, Batch 20/66, Loss: 59.7234\nAvg Blank Probability: 0.0210\nSample predictions: ['aUHcea', 'fva', '-3H28ial2k']\nGround Truth (first 3): ['FqIm', 'by', '-RY61']\nRaw outputs (first 3): [[ 1  6 63 63 56 13  6 22  1 11 35  1 43  1 14  1 47  1 24 10  4 10 10 63\n   4  4  5 19 56 38 19 19]\n [47 22 56 19 56  1 35 10 56 30 10 48  0  1 43  1  1 34  1 30 47  4  1  0\n  32 42 43 19 12  1  1  1]\n [ 0  1 34 22 19 10 35  0 34  6 25 11 22 56  6  4 32  1  1 34  1  1 27 34\n  34 22 12  1 42 32 25 50]]\nInput length: 32, Label lengths: [4, 2, 5]\nToken distribution (Batch 0): {11: 1, 34: 3, 1: 2, 50: 1, 22: 1, 12: 1, 47: 1}\nToken distribution (Batch 1): {10: 2, 1: 1, 47: 1}\nToken distribution (Batch 2): {34: 2, 11: 2, 10: 1, 1: 1, 57: 1, 22: 1, 19: 1, 32: 1}\nToken distribution (Batch 3): {1: 2, 34: 1, 32: 1, 43: 1, 18: 1, 26: 1, 11: 1, 22: 1, 27: 1}\nToken distribution (Batch 4): {10: 1, 11: 1, 22: 1, 6: 1}\nToken distribution (Batch 5): {11: 1, 4: 1}\nToken distribution (Batch 6): {12: 1, 32: 1}\nToken distribution (Batch 7): {50: 1, 6: 1, 43: 1, 34: 1, 12: 1, 47: 1}\nToken distribution (Batch 8): {4: 1, 34: 1, 1: 1, 30: 1, 48: 1, 10: 1}\nToken distribution (Batch 9): {50: 1, 45: 1, 19: 1, 42: 1, 10: 1, 11: 1, 12: 1, 26: 1, 25: 1, 35: 1}\nToken distribution (Batch 10): {11: 1, 29: 1, 56: 1, 10: 1}\nToken distribution (Batch 11): {1: 3, 42: 1, 25: 1, 32: 1}\nToken distribution (Batch 12): {4: 1, 12: 2, 22: 1, 32: 1, 11: 1, 27: 1, 24: 1, 1: 2}\nToken distribution (Batch 13): {22: 1, 11: 1}\nToken distribution (Batch 14): {59: 1, 1: 2, 6: 1, 4: 1, 12: 1}\nToken distribution (Batch 15): {1: 2, 14: 1, 50: 1}\nToken distribution (Batch 16): {34: 1, 4: 1}\nToken distribution (Batch 17): {12: 1, 1: 2, 50: 1, 56: 1, 6: 1, 34: 1, 10: 1}\nToken distribution (Batch 18): {24: 1, 63: 1, 19: 1, 32: 1, 11: 1, 9: 1}\nToken distribution (Batch 19): {1: 3, 10: 1, 32: 1, 6: 1}\nToken distribution (Batch 20): {4: 1, 6: 1}\nToken distribution (Batch 21): {61: 1, 1: 3, 4: 1, 47: 2, 10: 1, 26: 1, 32: 1}\nToken distribution (Batch 22): {6: 1, 1: 2, 11: 2, 56: 1, 27: 1, 34: 1, 4: 1, 5: 1}\nToken distribution (Batch 23): {32: 1, 33: 1, 42: 1, 6: 1, 41: 1, 12: 1, 1: 1, 34: 1}\nToken distribution (Batch 24): {11: 2, 1: 2, 34: 1, 47: 1}\nToken distribution (Batch 25): {12: 1, 11: 1, 1: 1, 21: 1, 41: 1, 10: 1, 50: 1, 22: 1}\nToken distribution (Batch 26): {10: 1, 1: 1}\nToken distribution (Batch 27): {1: 1, 47: 1, 44: 1, 10: 1, 55: 1, 5: 2, 34: 1, 56: 1, 19: 1}\nToken distribution (Batch 28): {6: 1, 10: 1}\nToken distribution (Batch 29): {1: 1, 24: 1, 27: 1, 4: 1}\nToken distribution (Batch 30): {1: 1, 24: 1}\nToken distribution (Batch 31): {4: 1, 43: 2, 63: 1, 6: 1, 27: 1, 32: 1, 10: 1, 5: 1, 28: 1}\nBatch 30, Gradient norm: 76.5357\nEpoch 5, Batch 30/66, Loss: 63.8877\nAvg Blank Probability: 0.0219\nSample predictions: ['kHaXavlHU', 'jaU', 'Hkja4vsFkH']\nGround Truth (first 3): ['uJJX-', 'Pv', 'xM7tm']\nRaw outputs (first 3): [[11  0 34  1 10 11 12 50  0 50 11  0  4 22  0  1 34 12 24  1  4 61  6 32\n  11 12  0  1  6  1  0  4]\n [34 10 11 34 11  4 32  6 34  0 29 42 12  0  1 14  0  1  0 10  6  1  1 33\n   0 11  1 47 10 24 24 43]\n [34  1 10 32  0 27  1  0  1 19 56  1 22 25  1  1 30  1  0 32 34  4  1 42\n  11  1 10  0  0  0  1 63]]\nInput length: 32, Label lengths: [5, 2, 5]\nToken distribution (Batch 0): {1: 2, 34: 1, 32: 1, 7: 1, 19: 1, 35: 1, 12: 1}\nToken distribution (Batch 1): {1: 1, 32: 1, 6: 1, 56: 1, 17: 1, 27: 1, 10: 1, 34: 1}\nToken distribution (Batch 2): {11: 1, 6: 1, 20: 1, 30: 1, 35: 1, 34: 1}\nToken distribution (Batch 3): {42: 1, 43: 1, 22: 1, 1: 3}\nToken distribution (Batch 4): {34: 1, 43: 1}\nToken distribution (Batch 5): {56: 1, 32: 1, 19: 1, 1: 1, 10: 1, 6: 1}\nToken distribution (Batch 6): {1: 3, 34: 2, 47: 1}\nToken distribution (Batch 7): {34: 1, 1: 2, 10: 2, 19: 1}\nToken distribution (Batch 8): {10: 1, 47: 1}\nToken distribution (Batch 9): {43: 1, 4: 1, 27: 2, 22: 1, 1: 1}\nToken distribution (Batch 10): {1: 2, 24: 1, 56: 1, 21: 1, 12: 1, 4: 1, 25: 1}\nToken distribution (Batch 11): {12: 1, 32: 1, 22: 2, 1: 1, 6: 1}\nToken distribution (Batch 12): {25: 1, 43: 1}\nToken distribution (Batch 13): {44: 1, 30: 1, 1: 2, 9: 1, 47: 2, 43: 1, 20: 1, 56: 1}\nToken distribution (Batch 14): {1: 1, 32: 1, 19: 1, 35: 1, 34: 1, 11: 1}\nToken distribution (Batch 15): {11: 2, 43: 2, 34: 1, 1: 1, 6: 1, 7: 1}\nToken distribution (Batch 16): {1: 1, 6: 1}\nToken distribution (Batch 17): {24: 1, 34: 1, 12: 1, 42: 1, 11: 2, 22: 2, 6: 1, 1: 1}\nToken distribution (Batch 18): {25: 1, 50: 1, 22: 2}\nToken distribution (Batch 19): {1: 2, 12: 1, 34: 2, 0: 1, 6: 1, 4: 1, 50: 1, 47: 1}\nToken distribution (Batch 20): {1: 1, 56: 2, 19: 1, 11: 1, 32: 1, 35: 1, 23: 1}\nToken distribution (Batch 21): {6: 2, 47: 1, 42: 1, 41: 1, 34: 2, 11: 1, 9: 1, 1: 1}\nToken distribution (Batch 22): {12: 1, 44: 1, 1: 2, 20: 1, 10: 1}\nToken distribution (Batch 23): {10: 1, 34: 1, 6: 1, 50: 1}\nToken distribution (Batch 24): {34: 1, 1: 3, 32: 1, 45: 1, 42: 1, 12: 1}\nToken distribution (Batch 25): {47: 1, 43: 1}\nToken distribution (Batch 26): {19: 1, 1: 1, 56: 1, 13: 1, 32: 1, 47: 1, 12: 1, 48: 1, 4: 1, 63: 1}\nToken distribution (Batch 27): {42: 1, 10: 1, 30: 1, 47: 1}\nToken distribution (Batch 28): {11: 2, 48: 1, 1: 1}\nToken distribution (Batch 29): {33: 1, 32: 2, 50: 1, 22: 2, 5: 1, 1: 1, 63: 1, 4: 1}\nToken distribution (Batch 30): {17: 1, 1: 4, 47: 1, 32: 1, 6: 1, 4: 1, 22: 1}\nToken distribution (Batch 31): {63: 1, 34: 1, 24: 1, 19: 1}\nBatch 40, Gradient norm: 27892.5977\nEpoch 5, Batch 40/66, Loss: 56.0715\nAvg Blank Probability: 0.0223\nSample predictions: ['aHFgsIla', 'aFf3qAjH', 'kftDIH']\nGround Truth (first 3): ['ZXrt', '7SGL', 'Q5C']\nRaw outputs (first 3): [[ 1  1 11  0 34 56  1 34 10 43  1 12 25 44  1 11  1 24 25  0  1  6 12 10\n  34 47  0 42 11 33 17 63]\n [ 0 32  6  0 43 32 34  1 47  4  0 32 43 30 32  0  6 34  0  0 56 47 44 34\n   1 43  1 10 48 32  1 34]\n [32  0 20 22  6 19 47 10  0 27 56 22 34  1 19 34 22  0 22  0 19 42  1  6\n   1 35 56 30  1 50 47 24]]\nInput length: 32, Label lengths: [4, 4, 3]\nToken distribution (Batch 0): {36: 1, 35: 1, 47: 1, 8: 1}\nToken distribution (Batch 1): {57: 1, 10: 1}\nToken distribution (Batch 2): {10: 2, 34: 1, 1: 2, 42: 1}\nToken distribution (Batch 3): {19: 1, 10: 1, 1: 1, 4: 1, 5: 1, 34: 1}\nToken distribution (Batch 4): {34: 1, 12: 1, 47: 1, 1: 3, 11: 1, 61: 1, 22: 1, 43: 1}\nToken distribution (Batch 5): {32: 1, 29: 1}\nToken distribution (Batch 6): {1: 2, 6: 2}\nToken distribution (Batch 7): {1: 3, 57: 1, 42: 1, 12: 1, 43: 1, 10: 1, 5: 1, 19: 1}\nToken distribution (Batch 8): {10: 1, 35: 1, 7: 1, 19: 1, 22: 2, 42: 1, 12: 1}\nToken distribution (Batch 9): {63: 1, 1: 1, 14: 1, 56: 1}\nToken distribution (Batch 10): {18: 1, 1: 1, 35: 1, 19: 1}\nToken distribution (Batch 11): {56: 1, 11: 1, 1: 3, 43: 1, 55: 1, 32: 1}\nToken distribution (Batch 12): {10: 3, 56: 1, 32: 1, 34: 1, 11: 1, 1: 1, 47: 1, 61: 1}\nToken distribution (Batch 13): {12: 1, 63: 1, 20: 1, 44: 1, 56: 1, 58: 1}\nToken distribution (Batch 14): {42: 1, 32: 1, 35: 1, 56: 1}\nToken distribution (Batch 15): {1: 1, 7: 1, 32: 1, 27: 1, 47: 1, 12: 1, 10: 1, 26: 1}\nToken distribution (Batch 16): {1: 2, 11: 1, 19: 1}\nToken distribution (Batch 17): {1: 1, 5: 1, 11: 1, 4: 1}\nToken distribution (Batch 18): {48: 1, 12: 1}\nToken distribution (Batch 19): {4: 1, 63: 1, 51: 1, 19: 1, 27: 1, 33: 1, 1: 1, 34: 1}\nToken distribution (Batch 20): {24: 2, 43: 2, 1: 1, 22: 1, 19: 1, 16: 1, 38: 1, 5: 1}\nToken distribution (Batch 21): {56: 1, 1: 2, 5: 2, 50: 1, 12: 1, 32: 1}\nToken distribution (Batch 22): {47: 2, 56: 1, 5: 2, 1: 1, 34: 1, 55: 1}\nToken distribution (Batch 23): {10: 1, 47: 1, 43: 1, 4: 1, 12: 1, 22: 1}\nToken distribution (Batch 24): {43: 1, 11: 1, 55: 1, 35: 1, 10: 1, 34: 1, 19: 1, 42: 1}\nToken distribution (Batch 25): {13: 1, 4: 1, 46: 1, 1: 2, 7: 1, 47: 1, 20: 1, 27: 1, 32: 1}\nToken distribution (Batch 26): {42: 1, 47: 1, 10: 1, 1: 1}\nToken distribution (Batch 27): {36: 1, 47: 1, 57: 1, 55: 2, 11: 1, 35: 1, 34: 1}\nToken distribution (Batch 28): {34: 1, 35: 1, 1: 1, 48: 1, 19: 1, 12: 1, 32: 1, 42: 1}\nToken distribution (Batch 29): {11: 1, 27: 1, 1: 2, 42: 1, 63: 1, 56: 1, 47: 1}\nToken distribution (Batch 30): {42: 1, 6: 1, 11: 1, 1: 2, 12: 1}\nToken distribution (Batch 31): {22: 1, 1: 3, 35: 1, 12: 1, 5: 1, 11: 1}\nBatch 50, Gradient norm: 145.8826\nEpoch 5, Batch 50/66, Loss: 51.8817\nAvg Blank Probability: 0.0232\nSample predictions: ['JIUh', '4j', 'jHaP']\nGround Truth (first 3): ['36', 'p', 'L2B']\nRaw outputs (first 3): [[36 57 10 19 34 32  1  1 10 63 18  0  0 12 42  1  1  1 48  4 24 56 47  0\n  43  0 42 36 34  0  0 22]\n [35 10 10 10 12 29  6  0  0  0  1 11 10 63 32  0 11  0 12 63  0  1 56 47\n   0  0 47 47  0  0  6  0]\n [47  0 34  1  0 48  1  0  0 14 35  1 10 20  0 32 19 11  0  0  1  5  5 43\n   0  0 10 57  1  1  0 35]]\nInput length: 32, Label lengths: [2, 1, 3]\nToken distribution (Batch 0): {16: 1, 1: 1, 48: 1, 47: 1}\nToken distribution (Batch 1): {24: 1, 42: 1, 10: 1, 34: 1, 9: 1, 5: 1, 18: 1, 27: 1, 63: 1, 12: 1}\nToken distribution (Batch 2): {19: 1, 32: 1, 34: 3, 50: 1, 47: 1, 1: 1, 10: 1, 22: 1}\nToken distribution (Batch 3): {34: 1, 55: 1, 1: 1, 4: 1}\nToken distribution (Batch 4): {5: 1, 47: 1, 19: 1, 24: 1, 1: 2, 35: 1, 48: 1}\nToken distribution (Batch 5): {43: 1, 32: 1}\nToken distribution (Batch 6): {51: 1, 19: 1}\nToken distribution (Batch 7): {32: 2, 19: 1, 4: 1, 1: 3, 35: 1, 13: 1, 56: 1}\nToken distribution (Batch 8): {12: 1, 1: 3, 11: 1, 56: 1, 47: 1, 50: 1}\nToken distribution (Batch 9): {32: 1, 19: 1, 34: 1, 1: 4, 47: 1}\nToken distribution (Batch 10): {56: 1, 29: 1}\nToken distribution (Batch 11): {1: 1, 4: 1, 12: 3, 47: 1}\nToken distribution (Batch 12): {13: 1, 27: 2, 4: 1}\nToken distribution (Batch 13): {50: 1, 6: 1}\nToken distribution (Batch 14): {61: 1, 10: 2, 27: 1, 28: 1, 30: 1, 32: 1, 47: 1}\nToken distribution (Batch 15): {13: 1, 1: 1, 10: 2, 22: 1, 56: 1}\nToken distribution (Batch 16): {10: 1, 1: 2, 57: 1, 48: 1, 32: 2, 16: 1}\nToken distribution (Batch 17): {1: 2, 42: 2, 34: 1, 6: 1}\nToken distribution (Batch 18): {10: 1, 50: 1, 23: 1, 13: 1, 1: 3, 19: 1}\nToken distribution (Batch 19): {10: 2, 1: 1, 34: 1}\nToken distribution (Batch 20): {1: 2, 47: 1, 7: 1, 22: 1, 28: 1}\nToken distribution (Batch 21): {42: 1, 11: 1}\nToken distribution (Batch 22): {6: 2, 4: 1, 11: 1, 34: 1, 1: 1, 25: 1, 19: 1}\nToken distribution (Batch 23): {10: 1, 47: 1, 1: 1, 34: 1, 5: 1, 22: 1}\nToken distribution (Batch 24): {11: 1, 1: 1, 10: 1, 46: 1}\nToken distribution (Batch 25): {10: 1, 1: 3, 19: 1, 27: 1, 6: 1, 28: 1}\nToken distribution (Batch 26): {11: 2, 10: 1, 6: 1, 4: 1, 35: 1, 26: 1, 63: 1}\nToken distribution (Batch 27): {13: 1, 32: 1, 1: 1, 43: 1}\nToken distribution (Batch 28): {6: 2, 45: 1, 32: 3, 50: 1, 10: 1, 34: 1, 1: 1}\nToken distribution (Batch 29): {1: 2, 6: 1, 11: 1, 30: 1, 47: 1, 22: 1, 32: 1}\nToken distribution (Batch 30): {6: 1, 34: 1, 4: 1, 43: 1, 1: 2, 47: 1, 23: 1}\nToken distribution (Batch 31): {61: 1, 1: 2, 34: 1, 25: 1, 47: 2, 7: 1}\nBatch 60, Gradient norm: 127.2396\nEpoch 5, Batch 60/66, Loss: 55.4839\nAvg Blank Probability: 0.0240\nSample predictions: ['paVU', 'xPjHierA-l', 'sFHXUaHjv']\nGround Truth (first 3): ['4z', 'b4YLU', 'VjYzO']\nRaw outputs (first 3): [[16  0 19 34  5 43  0 32 12 32 56  1  0 50 61 13 10  1 10 10  1 42  6 10\n  11 10 11 13  0  1  6 61]\n [ 1 42  0 55 47  0  0 32  1  0 29  4  0  6 10  1  1  1 50  1 47  0  4 47\n   1  1 10  0  6  0 34  1]\n [ 0 10 34  1 19  1  0 19  0 34 32 12  4 35 27 10 57 42 23 10  1  1 11  1\n   0 19  6  1 45  6  4 34]]\nInput length: 32, Label lengths: [2, 5, 5]\nEpoch 5/20, Loss: 59.3490\nToken distribution (Batch 0): {1: 10}\nToken distribution (Batch 1): {1: 2}\nToken distribution (Batch 2): {1: 4}\nToken distribution (Batch 3): {1: 10}\nToken distribution (Batch 4): {1: 2}\nToken distribution (Batch 5): {1: 8}\nToken distribution (Batch 6): {1: 10}\nToken distribution (Batch 7): {1: 10}\nToken distribution (Batch 8): {1: 8}\nValidation Loss: 66.0503\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['rFyEG', '8', 'A5', 'F*y3i', 'K']\nCurrent Learning Rate: 1.6133825227853389e-06\nEpoch 6, Filtered data size: 2604, Sample labels: ['qV8i', 'i', 'PnB']\nTrain size: 2083, Val size: 521\nToken distribution (Batch 0): {6: 1, 1: 3}\nToken distribution (Batch 1): {10: 1, 34: 2, 23: 1, 48: 1, 1: 2, 12: 1, 7: 1, 35: 1}\nToken distribution (Batch 2): {1: 1, 48: 1, 47: 1, 55: 1}\nToken distribution (Batch 3): {1: 1, 22: 2, 32: 1, 19: 1, 9: 1, 47: 1, 20: 1}\nToken distribution (Batch 4): {25: 1, 1: 1}\nToken distribution (Batch 5): {32: 2, 10: 1, 1: 2, 34: 1, 23: 1, 27: 1}\nToken distribution (Batch 6): {63: 1, 43: 1, 1: 1, 35: 1}\nToken distribution (Batch 7): {48: 1, 10: 1, 19: 1, 32: 1, 27: 1, 43: 1, 23: 1, 5: 1, 20: 1, 1: 1}\nToken distribution (Batch 8): {34: 1, 11: 1}\nToken distribution (Batch 9): {10: 1, 6: 1, 1: 1, 55: 1}\nToken distribution (Batch 10): {4: 3, 27: 1, 1: 2, 10: 1, 12: 2, 25: 1}\nToken distribution (Batch 11): {12: 1, 48: 1, 4: 1, 43: 1}\nToken distribution (Batch 12): {11: 1, 32: 1, 22: 1, 10: 2, 12: 1, 34: 1, 1: 1}\nToken distribution (Batch 13): {1: 2, 34: 1, 25: 1}\nToken distribution (Batch 14): {1: 2, 61: 1, 34: 1, 47: 1, 28: 1}\nToken distribution (Batch 15): {19: 1, 22: 1, 32: 1, 56: 1, 34: 1, 57: 1, 38: 1, 5: 1}\nToken distribution (Batch 16): {34: 1, 5: 1, 22: 1, 1: 4, 53: 1}\nToken distribution (Batch 17): {11: 1, 47: 1}\nToken distribution (Batch 18): {50: 1, 7: 1, 10: 1, 47: 1, 1: 1, 4: 1}\nToken distribution (Batch 19): {11: 1, 1: 2, 43: 1, 32: 3, 35: 1}\nToken distribution (Batch 20): {12: 3, 57: 1, 32: 1, 50: 1, 22: 1, 63: 1}\nToken distribution (Batch 21): {10: 1, 1: 2, 22: 2, 34: 2, 11: 1, 50: 2}\nToken distribution (Batch 22): {42: 1, 56: 1}\nToken distribution (Batch 23): {34: 1, 47: 1}\nToken distribution (Batch 24): {4: 1, 10: 1, 8: 1, 1: 1}\nToken distribution (Batch 25): {24: 1, 61: 1}\nToken distribution (Batch 26): {42: 1, 4: 1, 31: 1, 43: 1, 1: 2}\nToken distribution (Batch 27): {34: 1, 12: 2, 50: 1, 11: 1, 63: 1, 4: 1, 6: 1}\nToken distribution (Batch 28): {10: 1, 35: 2, 19: 1, 32: 1, 1: 1}\nToken distribution (Batch 29): {63: 1, 1: 3}\nToken distribution (Batch 30): {32: 1, 11: 1}\nToken distribution (Batch 31): {6: 1, 11: 1, 1: 3, 47: 1}\nBatch 0, Gradient norm: 31528.1504\nEpoch 6, Batch 0/66, Loss: 62.7398\nAvg Blank Probability: 0.0242\nSample predictions: ['fa', 'jHwValgaIH', 'aVU2']\nGround Truth (first 3): ['4F', 'tKd4F', 'IC']\nRaw outputs (first 3): [[ 6 10  1  1 25 32 63 48 34 10  4 12 11  1  0 19 34 11  0  0 12  0 42 34\n   4 24 42 34  0 63 32  6]\n [ 0 34 48  0  1 10 43 10 11  6 27 48 32 34  1  0  5  0  7  1 57  1  0 47\n  10 61  0 12 35  1 11 11]\n [ 1 23  0 32  0  0  1 19  0  1  4  4  0 25  0 32  0  6  0  0  0 22  1  0\n   8  6  0  0  0  1 16  1]]\nInput length: 32, Label lengths: [2, 5, 2]\nToken distribution (Batch 0): {1: 1, 4: 1, 21: 1, 47: 1}\nToken distribution (Batch 1): {24: 1, 34: 1, 32: 3, 48: 1, 12: 1, 16: 1}\nToken distribution (Batch 2): {22: 1, 32: 1, 13: 1, 10: 1}\nToken distribution (Batch 3): {10: 2, 16: 1, 6: 1, 11: 1, 32: 1}\nToken distribution (Batch 4): {42: 1, 50: 1, 11: 1, 19: 1, 1: 2}\nToken distribution (Batch 5): {4: 1, 56: 1, 47: 1, 12: 1, 32: 1, 1: 1}\nToken distribution (Batch 6): {35: 1, 22: 1}\nToken distribution (Batch 7): {25: 1, 12: 1, 35: 1, 50: 1}\nToken distribution (Batch 8): {1: 2, 50: 1, 21: 2, 43: 1}\nToken distribution (Batch 9): {57: 1, 6: 1, 25: 1, 32: 3, 1: 2, 20: 1, 12: 1}\nToken distribution (Batch 10): {6: 1, 1: 3}\nToken distribution (Batch 11): {57: 1, 1: 2, 19: 1, 47: 2, 5: 1, 43: 1, 34: 1, 10: 1}\nToken distribution (Batch 12): {24: 1, 34: 1}\nToken distribution (Batch 13): {63: 1, 12: 2, 3: 1, 11: 1, 4: 1}\nToken distribution (Batch 14): {47: 1, 1: 3, 22: 1, 32: 1, 6: 1, 55: 1}\nToken distribution (Batch 15): {63: 1, 19: 1, 32: 1, 22: 1, 50: 1, 10: 1}\nToken distribution (Batch 16): {6: 2, 1: 2, 23: 1, 34: 1, 35: 2, 32: 1, 11: 1}\nToken distribution (Batch 17): {35: 1, 34: 1, 50: 1, 1: 1}\nToken distribution (Batch 18): {34: 1, 35: 1, 10: 1, 56: 1, 32: 1, 19: 1, 0: 1, 12: 1}\nToken distribution (Batch 19): {1: 1, 55: 1}\nToken distribution (Batch 20): {6: 3, 1: 3, 35: 1, 34: 1, 5: 1, 47: 1}\nToken distribution (Batch 21): {35: 1, 22: 1, 25: 1, 47: 1}\nToken distribution (Batch 22): {32: 1, 56: 1, 47: 1, 34: 1, 53: 1, 19: 1, 12: 1, 10: 1}\nToken distribution (Batch 23): {1: 2, 12: 1, 35: 1}\nToken distribution (Batch 24): {61: 1, 11: 1, 35: 1, 19: 1, 27: 2, 10: 2, 5: 1, 12: 1}\nToken distribution (Batch 25): {35: 1, 32: 2, 1: 1, 50: 1, 6: 1}\nToken distribution (Batch 26): {32: 1, 34: 3, 3: 1, 7: 1, 61: 1, 11: 1}\nToken distribution (Batch 27): {47: 2, 1: 3, 6: 1, 11: 1, 34: 2, 55: 1}\nToken distribution (Batch 28): {10: 1, 1: 2, 22: 1, 48: 1, 32: 1}\nToken distribution (Batch 29): {12: 2, 11: 1, 5: 2, 6: 1, 35: 1, 55: 1}\nToken distribution (Batch 30): {12: 1, 47: 1}\nToken distribution (Batch 31): {34: 1, 4: 1, 56: 1, 32: 1}\nBatch 10, Gradient norm: 376.7149\nEpoch 6, Batch 10/66, Loss: 54.6006\nAvg Blank Probability: 0.0251\nSample predictions: ['aduU', 'xHFVFlp', 'vFmj']\nGround Truth (first 3): ['p9', '7SGL', 'yN']\nRaw outputs (first 3): [[ 1 24 22 10  0  4 35 25  1 57  6 57  0 63  0 63  6 35 34  1  6 35 32  1\n   0 35 32 47 10 12  0 34]\n [ 4 34 32 16 50  0 22 12 50  6  1  1 34 12  0 19  1 34  0 55  0 22 56 12\n  11  0  0  1  0 11 47  4]\n [ 0  0 13 10 11  0 34 35  1  0  1 19 47  3 22 32 23  0 10 12  0 25  0  1\n  35  1 34  6  1 12 47  0]]\nInput length: 32, Label lengths: [2, 4, 2]\nToken distribution (Batch 0): {63: 1, 1: 1, 27: 1, 12: 1, 42: 1, 19: 1}\nToken distribution (Batch 1): {4: 1, 6: 1, 47: 1, 12: 1}\nToken distribution (Batch 2): {1: 2, 0: 1, 10: 1, 12: 1, 27: 1, 35: 1, 34: 1}\nToken distribution (Batch 3): {12: 2, 22: 1, 42: 1, 1: 1, 8: 1, 13: 1, 19: 1}\nToken distribution (Batch 4): {4: 1, 42: 1, 11: 1, 43: 1}\nToken distribution (Batch 5): {50: 1, 10: 2, 42: 1, 33: 1, 1: 1}\nToken distribution (Batch 6): {5: 1, 27: 1}\nToken distribution (Batch 7): {12: 2, 42: 1, 34: 1, 0: 1, 11: 1, 6: 1, 19: 1}\nToken distribution (Batch 8): {10: 1, 47: 1, 1: 2, 63: 1, 55: 2, 5: 1, 54: 1, 27: 1}\nToken distribution (Batch 9): {35: 1, 1: 3, 34: 1, 10: 1, 11: 1, 51: 1}\nToken distribution (Batch 10): {35: 2, 9: 1, 5: 1, 19: 1, 12: 2, 11: 1, 4: 1, 50: 1}\nToken distribution (Batch 11): {31: 1, 27: 2, 33: 1, 63: 1, 29: 1, 34: 1, 32: 1, 47: 2}\nToken distribution (Batch 12): {34: 1, 20: 1}\nToken distribution (Batch 13): {1: 2, 59: 1, 35: 1, 4: 1, 12: 1}\nToken distribution (Batch 14): {42: 1, 10: 1, 1: 2, 47: 2, 27: 1, 6: 1}\nToken distribution (Batch 15): {19: 2, 63: 1, 9: 1, 34: 1, 1: 1}\nToken distribution (Batch 16): {27: 1, 35: 2, 43: 1, 22: 1, 48: 1}\nToken distribution (Batch 17): {20: 1, 11: 1, 34: 1, 27: 2, 32: 2, 47: 1, 56: 1, 25: 1}\nToken distribution (Batch 18): {10: 1, 1: 1}\nToken distribution (Batch 19): {42: 1, 50: 1}\nToken distribution (Batch 20): {4: 1, 63: 1}\nToken distribution (Batch 21): {34: 2, 1: 2}\nToken distribution (Batch 22): {63: 1, 32: 2, 1: 3, 34: 2, 19: 1, 10: 1}\nToken distribution (Batch 23): {1: 3, 55: 1, 4: 1, 34: 2, 12: 1, 5: 1, 61: 1}\nToken distribution (Batch 24): {32: 1, 12: 1}\nToken distribution (Batch 25): {11: 2, 36: 1, 48: 1}\nToken distribution (Batch 26): {35: 1, 12: 1, 1: 2, 27: 1, 19: 1}\nToken distribution (Batch 27): {12: 1, 34: 1, 35: 1, 56: 1}\nToken distribution (Batch 28): {34: 1, 7: 1, 1: 1, 27: 1, 43: 1, 0: 1}\nToken distribution (Batch 29): {42: 1, 10: 2, 25: 1, 1: 2}\nToken distribution (Batch 30): {5: 1, 55: 1, 1: 1, 34: 2, 35: 1}\nToken distribution (Batch 31): {12: 1, 5: 1, 35: 1, 34: 3, 32: 1, 19: 1, 1: 2}\nBatch 20, Gradient norm: 1737.1079\nEpoch 6, Batch 20/66, Loss: 56.9227\nAvg Blank Probability: 0.0262\nSample predictions: ['-aAlPs', 'dfUl', 'ajlAaIH']\nGround Truth (first 3): ['Os7', 'RS', '7lDV']\nRaw outputs (first 3): [[63  4  1  0  4 50  0 12 10 35 35  0  0  1 42  0 27  0 10 42  4  0 63  0\n  32  0 35 12 34 42  5  0]\n [ 0  6  0  0 42  0 27 42 47  0  0  0 20  0 10 19 35 11  0 50  0  0  0  0\n   0  0 12  0  7 10 55  5]\n [ 0 47  0  0  0 42  0 34  1  0  5 33  0 35  1  0 35 34 10  0 12  1 32  1\n  25  0  1 35  1 25  1 35]]\nInput length: 32, Label lengths: [3, 2, 4]\nToken distribution (Batch 0): {34: 1, 1: 3}\nToken distribution (Batch 1): {34: 3, 27: 1, 1: 1, 55: 1}\nToken distribution (Batch 2): {10: 1, 11: 1, 1: 2, 32: 1, 25: 1, 4: 1, 22: 1}\nToken distribution (Batch 3): {63: 1, 34: 1, 12: 1, 22: 1, 1: 2}\nToken distribution (Batch 4): {56: 1, 1: 1}\nToken distribution (Batch 5): {6: 1, 12: 2, 32: 1, 56: 2, 1: 1, 42: 1}\nToken distribution (Batch 6): {1: 1, 34: 1, 6: 1, 12: 1, 50: 1, 20: 1}\nToken distribution (Batch 7): {34: 1, 1: 2, 12: 1, 42: 1, 11: 1}\nToken distribution (Batch 8): {20: 1, 12: 1, 1: 3, 34: 1}\nToken distribution (Batch 9): {4: 1, 1: 1, 11: 1, 42: 1, 19: 1, 47: 1}\nToken distribution (Batch 10): {14: 1, 10: 1, 36: 1, 9: 1, 1: 2}\nToken distribution (Batch 11): {11: 1, 34: 1, 1: 1, 42: 1, 32: 1, 13: 1, 47: 2}\nToken distribution (Batch 12): {50: 1, 11: 2, 1: 2, 5: 1, 6: 1, 16: 1, 32: 1, 47: 1}\nToken distribution (Batch 13): {42: 1, 5: 1, 47: 1, 36: 1, 34: 1, 1: 1, 32: 1, 35: 1}\nToken distribution (Batch 14): {34: 1, 12: 1}\nToken distribution (Batch 15): {34: 1, 1: 1}\nToken distribution (Batch 16): {17: 1, 47: 1}\nToken distribution (Batch 17): {12: 1, 56: 1, 4: 2, 47: 1, 9: 1}\nToken distribution (Batch 18): {12: 1, 1: 3, 56: 1, 32: 1, 34: 1, 22: 1}\nToken distribution (Batch 19): {12: 1, 22: 2, 11: 1, 42: 1, 21: 1}\nToken distribution (Batch 20): {6: 1, 10: 1, 4: 1, 35: 1, 47: 1, 32: 1}\nToken distribution (Batch 21): {22: 1, 1: 2, 10: 1, 34: 2, 27: 1, 50: 1}\nToken distribution (Batch 22): {10: 1, 43: 1, 12: 2}\nToken distribution (Batch 23): {34: 1, 35: 1}\nToken distribution (Batch 24): {1: 1, 6: 1, 32: 1, 47: 2, 48: 1, 12: 1, 61: 1}\nToken distribution (Batch 25): {56: 1, 34: 3, 63: 1, 4: 1, 1: 1, 19: 1}\nToken distribution (Batch 26): {10: 1, 9: 1}\nToken distribution (Batch 27): {10: 1, 29: 1}\nToken distribution (Batch 28): {56: 1, 12: 1, 19: 1, 48: 1, 34: 1, 22: 1, 13: 1, 1: 1, 4: 1, 63: 1}\nToken distribution (Batch 29): {34: 1, 12: 1, 40: 1, 1: 1}\nToken distribution (Batch 30): {34: 2, 19: 1, 25: 1, 56: 1, 10: 1, 24: 1, 50: 1}\nToken distribution (Batch 31): {34: 2, 5: 1, 27: 1, 21: 1, 4: 1, 47: 1, 55: 1}\nBatch 30, Gradient norm: 97.2881\nEpoch 6, Batch 30/66, Loss: 58.3661\nAvg Blank Probability: 0.0274\nSample predictions: ['Ha', 'HAHa2H', 'jkaFaydv']\nGround Truth (first 3): ['kX', 'uHJ', 'AaGy']\nRaw outputs (first 3): [[34 34 10 63 56  6  1  0 20  4  0 11  0  0 34  0 17 12 12 12  0  0 10 34\n   1 56  0 10 56  0 34 34]\n [ 0 27  0 34  0 12  0  0 12  0 10 34  0  5  0  1 47 56  0 22  0  1 43 35\n   0 34  0  0  0  0  0 34]\n [ 0 34  1 12  0  0  6  0  1 11  0  0  1 47  1 50  0  4  0 22  0  0  0 27\n  32  0 22  0  0 40  0  5]]\nInput length: 32, Label lengths: [2, 3, 4]\nToken distribution (Batch 0): {32: 1, 27: 1, 19: 1, 9: 1, 47: 1, 0: 1}\nToken distribution (Batch 1): {1: 2, 35: 1, 4: 1, 11: 1, 63: 1}\nToken distribution (Batch 2): {1: 2, 56: 1, 20: 1, 5: 2, 12: 1, 34: 1, 32: 1, 57: 1}\nToken distribution (Batch 3): {1: 2, 35: 2, 55: 1, 24: 1, 10: 1, 19: 1}\nToken distribution (Batch 4): {32: 1, 19: 1, 27: 1, 1: 1}\nToken distribution (Batch 5): {47: 1, 48: 1, 1: 4, 10: 1, 22: 1, 20: 1, 0: 1}\nToken distribution (Batch 6): {47: 1, 1: 2, 34: 1, 27: 1, 12: 1, 10: 1, 0: 1}\nToken distribution (Batch 7): {1: 2, 47: 1, 43: 1, 21: 1, 26: 1}\nToken distribution (Batch 8): {32: 2, 12: 2, 10: 1, 6: 1, 48: 1, 51: 1, 14: 1, 31: 1}\nToken distribution (Batch 9): {1: 1, 50: 1}\nToken distribution (Batch 10): {12: 2, 35: 1, 42: 1}\nToken distribution (Batch 11): {1: 5, 61: 1, 11: 2, 25: 1, 42: 1}\nToken distribution (Batch 12): {27: 1, 47: 2, 42: 1, 1: 1, 56: 1, 6: 1, 32: 1, 19: 1, 34: 1}\nToken distribution (Batch 13): {42: 1, 34: 1, 0: 1, 10: 1, 11: 2, 56: 1, 47: 2, 43: 1}\nToken distribution (Batch 14): {10: 1, 11: 1, 24: 1, 19: 1}\nToken distribution (Batch 15): {12: 1, 22: 1, 1: 2, 34: 1, 56: 1}\nToken distribution (Batch 16): {1: 4, 50: 1, 34: 1, 47: 1, 7: 1}\nToken distribution (Batch 17): {11: 2, 20: 1, 12: 1, 22: 1, 26: 1, 1: 1, 47: 1}\nToken distribution (Batch 18): {6: 1, 12: 1, 32: 1, 10: 1, 20: 1, 22: 1, 26: 1, 27: 1}\nToken distribution (Batch 19): {20: 1, 56: 1, 48: 1, 63: 1, 32: 1, 0: 1, 4: 1, 1: 1}\nToken distribution (Batch 20): {63: 1, 11: 2, 35: 1, 43: 1, 13: 1, 34: 3, 32: 1}\nToken distribution (Batch 21): {47: 2, 19: 1, 43: 1, 34: 1, 12: 1, 10: 1, 4: 1}\nToken distribution (Batch 22): {10: 1, 56: 1}\nToken distribution (Batch 23): {32: 1, 1: 2, 43: 1}\nToken distribution (Batch 24): {34: 1, 32: 1}\nToken distribution (Batch 25): {34: 1, 50: 1, 1: 2, 11: 1, 0: 1}\nToken distribution (Batch 26): {4: 1, 32: 1, 1: 1, 11: 1, 34: 1, 42: 1}\nToken distribution (Batch 27): {42: 1, 4: 1, 19: 1, 10: 1, 5: 1, 34: 1}\nToken distribution (Batch 28): {19: 1, 25: 1, 12: 1, 33: 1, 4: 1, 35: 1}\nToken distribution (Batch 29): {50: 1, 11: 1, 56: 1, 19: 2, 1: 1, 10: 1, 9: 1}\nToken distribution (Batch 30): {20: 1, 23: 1, 4: 2, 22: 1, 50: 1, 34: 1, 1: 1}\nToken distribution (Batch 31): {6: 1, 16: 1, 1: 1, 5: 1}\nBatch 40, Gradient norm: 152.5238\nEpoch 6, Batch 40/66, Loss: 47.7349\nAvg Blank Probability: 0.0286\nSample predictions: ['FAsiU', 'aIdak-', 'a3telHFa4']\nGround Truth (first 3): ['S65', '8NA', 'dX*TW']\nRaw outputs (first 3): [[32  1  0  1 32 47  0  0 32  1 12  1 27 42  0 12  1  0  6  0 63 47 10 32\n   0  0  4 42  0  0 20  6]\n [27 35 56  0  0 48  1  0 12  0  0  0  0  0  0  0 50  0  0  0 11 19  0  1\n  32  0 32  4  0  0  0  0]\n [19  4  0  0 27  0  0  1  0  0  0  1  0  0 24  0  1  0 32 48 11  0 47 43\n   1  0  1  0  0 56  0  0]]\nInput length: 32, Label lengths: [3, 3, 5]\nToken distribution (Batch 0): {1: 1, 56: 1, 42: 2, 34: 1, 4: 1}\nToken distribution (Batch 1): {50: 1, 34: 2, 22: 1, 35: 1, 4: 1, 16: 1, 32: 1, 13: 1, 47: 1}\nToken distribution (Batch 2): {1: 1, 11: 1}\nToken distribution (Batch 3): {34: 1, 1: 1}\nToken distribution (Batch 4): {56: 1, 48: 1, 63: 1, 1: 2, 35: 1}\nToken distribution (Batch 5): {32: 2, 47: 1, 63: 1, 12: 1, 21: 2, 43: 1, 10: 1, 24: 1}\nToken distribution (Batch 6): {22: 1, 32: 1}\nToken distribution (Batch 7): {4: 1, 1: 3, 50: 1, 14: 1, 0: 1, 32: 1, 10: 1, 22: 1}\nToken distribution (Batch 8): {12: 1, 42: 1}\nToken distribution (Batch 9): {34: 1, 10: 1, 19: 1, 63: 1}\nToken distribution (Batch 10): {34: 1, 6: 1}\nToken distribution (Batch 11): {11: 1, 4: 1, 23: 1, 18: 1}\nToken distribution (Batch 12): {1: 1, 10: 1, 43: 1, 56: 1, 9: 1, 4: 1}\nToken distribution (Batch 13): {42: 1, 50: 2, 14: 1, 8: 1, 1: 1}\nToken distribution (Batch 14): {35: 1, 56: 1, 57: 1, 28: 1, 38: 1, 50: 1}\nToken distribution (Batch 15): {50: 1, 32: 2, 22: 1, 56: 1, 55: 2, 47: 1, 6: 1, 27: 1}\nToken distribution (Batch 16): {22: 1, 10: 2, 4: 1, 5: 2, 34: 2, 48: 1, 6: 1}\nToken distribution (Batch 17): {29: 1, 42: 1, 1: 2}\nToken distribution (Batch 18): {11: 1, 19: 1}\nToken distribution (Batch 19): {11: 1, 7: 1, 12: 1, 1: 1}\nToken distribution (Batch 20): {43: 2, 50: 2, 21: 1, 10: 1, 0: 1, 55: 1}\nToken distribution (Batch 21): {4: 2, 34: 1, 56: 1, 1: 2, 12: 1, 48: 1}\nToken distribution (Batch 22): {32: 1, 9: 1}\nToken distribution (Batch 23): {19: 1, 4: 1}\nToken distribution (Batch 24): {1: 2, 12: 1, 34: 1, 50: 1, 0: 1}\nToken distribution (Batch 25): {1: 2, 42: 1, 35: 1, 57: 1, 32: 1, 43: 2, 56: 1, 27: 1}\nToken distribution (Batch 26): {1: 1, 32: 1}\nToken distribution (Batch 27): {63: 1, 1: 1, 19: 1, 26: 1, 0: 2, 12: 1, 47: 1}\nToken distribution (Batch 28): {1: 1, 5: 1}\nToken distribution (Batch 29): {50: 1, 34: 1, 11: 2, 57: 1, 35: 1}\nToken distribution (Batch 30): {12: 2, 32: 1, 56: 2, 34: 2, 35: 1}\nToken distribution (Batch 31): {32: 3, 1: 1, 24: 1, 19: 1}\nBatch 50, Gradient norm: 101.4294\nEpoch 6, Batch 50/66, Loss: 64.9262\nAvg Blank Probability: 0.0298\nSample predictions: ['a3PHdP', 'XHvIdpFHmU', 'ak']\nGround Truth (first 3): ['sLL', 'c6jM0', 'I']\nRaw outputs (first 3): [[ 0  0  1 34  0  0 22  4  0 34  0 11  0 42  0  0  0 29 11  0  0  4  0  0\n   0  1  1 63  0  0  0 32]\n [ 0  0 11  1  0  0  0  1  0 10  0  4 10 50  0 32 10  0  0  7  0  0  9  4\n  12  0  0  1  0 34  0  1]\n [ 0  0 56 35  0  0  0  0  0  0  0  0 43  0  0  0  4  0  0  0  0  0 42  0\n  34  1  0 19 11 11 56  0]]\nInput length: 32, Label lengths: [3, 5, 1]\nToken distribution (Batch 0): {63: 1, 20: 1}\nToken distribution (Batch 1): {63: 1, 19: 1, 32: 1, 10: 1, 27: 1, 6: 1}\nToken distribution (Batch 2): {32: 1, 35: 1, 19: 1, 4: 1}\nToken distribution (Batch 3): {1: 1, 11: 1, 35: 1, 0: 1}\nToken distribution (Batch 4): {19: 1, 1: 1, 43: 1, 0: 1}\nToken distribution (Batch 5): {1: 2, 10: 1, 34: 1, 47: 1, 0: 1}\nToken distribution (Batch 6): {47: 1, 1: 2, 0: 1, 32: 1, 27: 1}\nToken distribution (Batch 7): {32: 1, 12: 2, 25: 1, 1: 1, 28: 1, 50: 1, 0: 1, 56: 1, 34: 1}\nToken distribution (Batch 8): {32: 1, 6: 1, 1: 1, 12: 2, 11: 1, 10: 1, 26: 1}\nToken distribution (Batch 9): {13: 1, 25: 1, 56: 1, 22: 2, 47: 2, 35: 1}\nToken distribution (Batch 10): {47: 1, 32: 1, 5: 2}\nToken distribution (Batch 11): {63: 1, 35: 2, 32: 2, 0: 1, 5: 2, 1: 1, 11: 1}\nToken distribution (Batch 12): {34: 1, 19: 1}\nToken distribution (Batch 13): {35: 1, 23: 2, 11: 1, 55: 1, 1: 2, 0: 2, 12: 1}\nToken distribution (Batch 14): {10: 1, 34: 2, 32: 1, 46: 1, 5: 2, 47: 1}\nToken distribution (Batch 15): {1: 1, 11: 1, 27: 1, 55: 1}\nToken distribution (Batch 16): {56: 1, 4: 1, 47: 1, 13: 1}\nToken distribution (Batch 17): {47: 1, 36: 1}\nToken distribution (Batch 18): {50: 1, 1: 1, 4: 2}\nToken distribution (Batch 19): {22: 1, 27: 1}\nToken distribution (Batch 20): {10: 2, 1: 3, 27: 1, 22: 1, 32: 1, 6: 1, 25: 1}\nToken distribution (Batch 21): {34: 3, 10: 1, 15: 1, 36: 1}\nToken distribution (Batch 22): {10: 1, 1: 1, 19: 2}\nToken distribution (Batch 23): {1: 2, 27: 1, 47: 1, 56: 1, 9: 1}\nToken distribution (Batch 24): {32: 1, 1: 1, 47: 1, 11: 1}\nToken distribution (Batch 25): {27: 1, 34: 1, 47: 1, 56: 1, 10: 1, 1: 1}\nToken distribution (Batch 26): {50: 1, 1: 3, 0: 1, 34: 1, 41: 1, 42: 1, 27: 1, 10: 1}\nToken distribution (Batch 27): {42: 1, 15: 1}\nToken distribution (Batch 28): {10: 1, 1: 1}\nToken distribution (Batch 29): {57: 1, 12: 1, 27: 1, 47: 1, 19: 1, 36: 1, 32: 1, 59: 1, 10: 1, 34: 1}\nToken distribution (Batch 30): {50: 1, 1: 1, 34: 2, 42: 2, 48: 1, 22: 1, 35: 1, 6: 1}\nToken distribution (Batch 31): {1: 2, 0: 2, 34: 1, 47: 1}\nBatch 60, Gradient norm: 92.3325\nEpoch 6, Batch 60/66, Loss: 57.6926\nAvg Blank Probability: 0.0313\nSample predictions: ['-t', '-sFjAf', 'FIsd']\nGround Truth (first 3): ['R', 'GHo', 'YT']\nRaw outputs (first 3): [[ 0  0 32  1  0  1 47  0 32  0  0  0 34 35  0  1 56 47  0  0  0 34  0  1\n  32 27  0 42  0 57  0  0]\n [ 0  0 35 11  0 10  0 12  0 25 32  0 19  0 34  0  0  0  0  0  1 10  1 27\n   0  0  1 15  0 12  1  1]\n [63  0  0  0  0 34  0  0  0 56  0  0  0 11 34  0 47  0  0  0 27  0  0 47\n  47  0  0  0 12  0  0  0]]\nInput length: 32, Label lengths: [1, 3, 2]\nEpoch 6/20, Loss: 57.1745\nToken distribution (Batch 0): {1: 6}\nToken distribution (Batch 1): {1: 8}\nToken distribution (Batch 2): {1: 4}\nToken distribution (Batch 3): {1: 10}\nToken distribution (Batch 4): {1: 8}\nToken distribution (Batch 5): {1: 6}\nToken distribution (Batch 6): {1: 2}\nToken distribution (Batch 7): {1: 8}\nToken distribution (Batch 8): {1: 6}\nValidation Loss: 60.5964\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['CFS', 'efa8', 'Z*', 'JkOF3', '55Gq']\nCurrent Learning Rate: 1.911501005323806e-06\nEpoch 7, Filtered data size: 2604, Sample labels: ['qV8i', 'i', 'PnB']\nTrain size: 2083, Val size: 521\nToken distribution (Batch 0): {1: 2, 32: 1, 27: 1, 11: 1, 19: 1}\nToken distribution (Batch 1): {1: 2, 48: 1, 28: 1, 12: 2, 5: 1, 47: 1, 25: 1, 19: 1}\nToken distribution (Batch 2): {44: 1, 5: 1, 34: 1, 4: 1}\nToken distribution (Batch 3): {22: 2, 11: 1, 1: 3, 56: 1, 4: 1, 12: 1, 35: 1}\nToken distribution (Batch 4): {10: 1, 5: 1}\nToken distribution (Batch 5): {1: 1, 43: 1, 34: 1, 4: 1, 26: 1, 40: 1, 0: 1, 36: 1}\nToken distribution (Batch 6): {47: 3, 1: 2, 50: 1, 43: 1, 27: 1}\nToken distribution (Batch 7): {27: 1, 47: 1, 0: 1, 1: 1}\nToken distribution (Batch 8): {34: 1, 29: 1, 50: 1, 11: 1, 5: 1, 26: 1, 0: 1, 1: 1}\nToken distribution (Batch 9): {6: 1, 4: 1, 47: 1, 10: 1, 60: 1, 1: 1}\nToken distribution (Batch 10): {1: 2}\nToken distribution (Batch 11): {1: 1, 50: 1, 3: 1, 19: 2, 9: 1}\nToken distribution (Batch 12): {10: 1, 42: 1}\nToken distribution (Batch 13): {10: 1, 11: 1}\nToken distribution (Batch 14): {10: 1, 27: 1, 12: 1, 1: 1}\nToken distribution (Batch 15): {46: 1, 8: 1, 7: 1, 12: 1, 1: 2, 50: 1, 9: 1}\nToken distribution (Batch 16): {43: 1, 1: 2, 19: 1, 34: 1, 27: 1, 56: 1, 4: 1}\nToken distribution (Batch 17): {11: 2, 50: 1, 0: 2, 19: 2, 27: 1, 22: 1, 48: 1}\nToken distribution (Batch 18): {10: 1, 1: 2, 36: 1}\nToken distribution (Batch 19): {47: 2, 35: 1, 41: 1}\nToken distribution (Batch 20): {32: 1, 4: 1, 48: 1, 11: 1}\nToken distribution (Batch 21): {14: 1, 56: 1, 6: 1, 48: 1}\nToken distribution (Batch 22): {34: 2}\nToken distribution (Batch 23): {13: 1, 9: 1}\nToken distribution (Batch 24): {1: 3, 27: 2, 4: 1, 35: 1, 50: 1, 34: 1, 0: 1}\nToken distribution (Batch 25): {46: 1, 25: 1, 61: 1, 7: 1, 50: 1, 32: 1, 10: 1, 11: 1}\nToken distribution (Batch 26): {9: 1, 32: 1}\nToken distribution (Batch 27): {6: 1, 34: 1, 25: 1, 42: 1}\nToken distribution (Batch 28): {1: 1, 34: 2, 12: 1, 56: 1, 48: 1, 22: 1, 10: 2, 11: 1}\nToken distribution (Batch 29): {1: 3, 47: 1, 11: 1, 22: 1, 50: 1, 6: 1}\nToken distribution (Batch 30): {25: 1, 19: 2, 11: 1, 10: 2, 0: 2, 1: 1, 33: 1}\nToken distribution (Batch 31): {1: 1, 22: 1}\nBatch 0, Gradient norm: 97.5272\nEpoch 7, Batch 0/66, Loss: 60.7187\nAvg Blank Probability: 0.0324\nSample predictions: ['aFAkas', 'aVBlealUys', 'ReHd']\nGround Truth (first 3): ['jNX', '68hng', 'Pv']\nRaw outputs (first 3): [[ 1  1  0 22  0  0  0 27 34  0  0  0 10 10 10 46  0  0 10 47 32 14  0  0\n   1  0  9  0  1  0 25  1]\n [ 0  0  0 11  0  0  0  0  0  0  1 50  0 11  0  0  0  0  0  0  0  0  0  9\n   0  0  0 34  0  1  0  0]\n [ 0  0  0 22  0  0  0  0  0  0 34  0  0  0 12  0 19  0  0  0  0  6  0  0\n   0  0 47  0  0  0  0  0]]\nInput length: 32, Label lengths: [3, 5, 2]\nToken distribution (Batch 0): {24: 1, 9: 1, 6: 2, 10: 1, 1: 3, 25: 1, 48: 1}\nToken distribution (Batch 1): {1: 3, 25: 1, 0: 1, 4: 1, 19: 1, 56: 1}\nToken distribution (Batch 2): {63: 1, 42: 1, 6: 1, 12: 1}\nToken distribution (Batch 3): {34: 1, 32: 1, 1: 1, 19: 1, 56: 2, 27: 1, 4: 1}\nToken distribution (Batch 4): {20: 1, 6: 2, 53: 1, 44: 1, 27: 3, 0: 1, 1: 1}\nToken distribution (Batch 5): {27: 1, 34: 1, 10: 1, 5: 1, 0: 1, 1: 1, 9: 1, 14: 1}\nToken distribution (Batch 6): {20: 1, 50: 1, 27: 1, 56: 1, 32: 1, 1: 1, 47: 1, 0: 1}\nToken distribution (Batch 7): {19: 1, 47: 1, 1: 1, 32: 1}\nToken distribution (Batch 8): {1: 1, 34: 1}\nToken distribution (Batch 9): {20: 1, 34: 1, 1: 3, 47: 1, 45: 1, 27: 2, 50: 1}\nToken distribution (Batch 10): {47: 1, 5: 2, 11: 3, 1: 3, 0: 1}\nToken distribution (Batch 11): {34: 1, 4: 1}\nToken distribution (Batch 12): {1: 2, 46: 1, 50: 1}\nToken distribution (Batch 13): {32: 2, 27: 1, 4: 1, 6: 1, 0: 1, 10: 2, 56: 1, 19: 1}\nToken distribution (Batch 14): {12: 2}\nToken distribution (Batch 15): {1: 2, 11: 1, 47: 2, 35: 1, 32: 1, 5: 1}\nToken distribution (Batch 16): {12: 1, 34: 1, 1: 2, 50: 1, 10: 1, 9: 1, 56: 1}\nToken distribution (Batch 17): {12: 1, 42: 1, 34: 1, 1: 2, 50: 1}\nToken distribution (Batch 18): {19: 1, 11: 1}\nToken distribution (Batch 19): {35: 1, 1: 3, 22: 1, 4: 1, 56: 1, 12: 1}\nToken distribution (Batch 20): {11: 2, 57: 1, 1: 1, 4: 1, 34: 1}\nToken distribution (Batch 21): {19: 1, 43: 2, 24: 1, 21: 1, 1: 1}\nToken distribution (Batch 22): {11: 1, 36: 1, 47: 2, 0: 1, 5: 1, 55: 1, 19: 1}\nToken distribution (Batch 23): {57: 1, 34: 1}\nToken distribution (Batch 24): {34: 1, 4: 2, 47: 1}\nToken distribution (Batch 25): {56: 1, 0: 1}\nToken distribution (Batch 26): {10: 1, 18: 1}\nToken distribution (Batch 27): {1: 2}\nToken distribution (Batch 28): {10: 1, 12: 1}\nToken distribution (Batch 29): {34: 1, 1: 2, 32: 1, 9: 1, 12: 1, 0: 1, 10: 1, 21: 1, 47: 1}\nToken distribution (Batch 30): {10: 2}\nToken distribution (Batch 31): {4: 2, 1: 1, 0: 3, 22: 1, 10: 1, 32: 1, 25: 1}\nBatch 10, Gradient norm: 102.6830\nEpoch 7, Batch 10/66, Loss: 61.3426\nAvg Blank Probability: 0.0340\nSample predictions: ['xifjayVf', 'aydsa3', '-Pfl']\nGround Truth (first 3): ['E0V2k', 'VudB', 'Q*']\nRaw outputs (first 3): [[ 0  0 63 34  0  0 20  0  1  0 47  0  1 32  0  1  0 12  0 35  0 19 11  0\n  34  0  0  0  0  0  0  4]\n [ 0  0  0  0  0 34  0  0 34 34  0  4  0  0 12 11  0 42  0  0  0  0 36 34\n   4  0  0  1  0  0  0  0]\n [ 0  0  6  0  0  0  0  1  0  0  0  0  1  4  0  0  0 34  0  0  1  0  0  0\n   4  0  0  0  1  0  0  0]]\nInput length: 32, Label lengths: [5, 4, 2]\nToken distribution (Batch 0): {47: 2, 0: 1, 42: 1, 32: 1, 19: 1}\nToken distribution (Batch 1): {1: 1, 19: 1}\nToken distribution (Batch 2): {13: 1, 25: 1, 12: 1, 4: 1, 22: 1, 34: 1}\nToken distribution (Batch 3): {32: 1, 1: 2, 0: 3, 56: 1, 34: 1, 5: 1, 10: 1}\nToken distribution (Batch 4): {11: 1, 1: 2, 5: 2, 47: 1, 43: 1, 19: 1, 34: 1, 27: 1}\nToken distribution (Batch 5): {43: 1, 34: 1, 4: 1, 47: 2, 10: 1, 0: 3, 27: 1}\nToken distribution (Batch 6): {50: 1, 47: 2, 32: 1, 22: 2, 5: 1, 36: 1, 0: 1, 35: 1}\nToken distribution (Batch 7): {6: 2, 22: 1, 15: 1, 0: 3, 1: 1}\nToken distribution (Batch 8): {1: 2, 11: 1, 63: 1, 0: 5, 27: 1}\nToken distribution (Batch 9): {63: 1, 22: 1}\nToken distribution (Batch 10): {43: 1, 21: 1, 50: 1, 11: 1, 55: 2, 22: 1, 0: 1, 56: 1, 47: 1}\nToken distribution (Batch 11): {58: 1, 48: 1, 1: 1, 6: 1}\nToken distribution (Batch 12): {5: 1, 22: 1, 1: 1, 27: 1}\nToken distribution (Batch 13): {17: 1, 32: 4, 4: 1, 47: 1, 0: 1, 6: 1, 1: 1}\nToken distribution (Batch 14): {50: 1, 1: 1, 34: 1, 0: 1, 22: 1, 27: 1}\nToken distribution (Batch 15): {61: 1, 12: 1, 44: 1, 34: 1, 0: 1, 25: 1, 36: 1, 22: 1}\nToken distribution (Batch 16): {23: 1, 12: 2, 1: 1, 19: 1, 32: 1, 35: 1, 21: 1}\nToken distribution (Batch 17): {1: 3, 20: 1, 34: 1, 50: 2, 36: 1, 0: 1, 4: 1}\nToken distribution (Batch 18): {10: 1, 50: 1, 35: 1, 1: 2, 0: 1}\nToken distribution (Batch 19): {12: 1, 0: 2, 34: 1, 61: 1, 22: 1, 1: 1, 27: 1}\nToken distribution (Batch 20): {1: 1, 22: 1, 42: 1, 32: 2, 47: 1}\nToken distribution (Batch 21): {10: 1, 0: 3, 19: 1, 4: 1, 1: 1, 25: 1}\nToken distribution (Batch 22): {4: 2, 22: 1, 0: 1, 1: 1, 10: 1, 12: 1, 21: 1}\nToken distribution (Batch 23): {55: 1, 34: 1}\nToken distribution (Batch 24): {53: 1, 22: 1, 47: 2, 0: 2, 27: 1, 19: 1}\nToken distribution (Batch 25): {35: 1, 1: 1, 22: 1, 30: 1}\nToken distribution (Batch 26): {50: 1, 35: 1, 32: 1, 0: 3, 47: 1, 19: 1, 11: 1, 26: 1}\nToken distribution (Batch 27): {23: 1, 48: 1, 34: 1, 19: 1, 1: 2, 35: 1, 12: 2, 53: 1}\nToken distribution (Batch 28): {34: 1, 1: 2, 32: 1}\nToken distribution (Batch 29): {1: 2, 56: 1, 48: 1}\nToken distribution (Batch 30): {34: 1, 21: 1, 1: 4, 47: 1, 0: 2, 63: 1}\nToken distribution (Batch 31): {19: 1, 10: 2, 17: 1, 35: 1, 34: 1, 0: 1, 6: 1}\nBatch 20, Gradient norm: 1905.9993\nEpoch 7, Batch 20/66, Loss: 43.9237\nAvg Blank Probability: 0.0361\nSample predictions: ['UPFs', 'as', 'myldvH']\nGround Truth (first 3): ['Ufo', 'a', 'KV2']\nRaw outputs (first 3): [[ 0  1  0 32  0  0  0  6  0 63 43  0  5  0 50  0  0  0 10  0  1 10  0 55\n   0  0  0  0 34  0  0 19]\n [47  0  0  0  0  0  0  0  1  0  0  0  0 32  0  0 12  0  0  0 22  0  0  0\n  22  1  0  0  1 56 21  0]\n [ 0  0  0  0  0  0  0 15  0  0  0  0  1  0 34  0  0  0  0 34  0  0  0  0\n   0  0  0 34  1 48  0  0]]\nInput length: 32, Label lengths: [3, 1, 3]\nToken distribution (Batch 0): {44: 1, 0: 2, 23: 2, 32: 1, 12: 1, 27: 1, 55: 1, 4: 1}\nToken distribution (Batch 1): {1: 4, 10: 1, 35: 1, 11: 2}\nToken distribution (Batch 2): {26: 1, 1: 2, 0: 1, 32: 1, 56: 1, 22: 1, 5: 1}\nToken distribution (Batch 3): {27: 1, 36: 1}\nToken distribution (Batch 4): {47: 1, 43: 1}\nToken distribution (Batch 5): {10: 1, 1: 1, 51: 1, 32: 1, 47: 1, 25: 1}\nToken distribution (Batch 6): {5: 3, 12: 1, 11: 1, 27: 1}\nToken distribution (Batch 7): {34: 1, 22: 1, 1: 2, 3: 1, 7: 1}\nToken distribution (Batch 8): {47: 1, 34: 2, 50: 1, 27: 1, 10: 1, 0: 3, 63: 1}\nToken distribution (Batch 9): {27: 2, 19: 1, 34: 1, 5: 1, 0: 1}\nToken distribution (Batch 10): {34: 2, 42: 1, 1: 1, 56: 1, 0: 1}\nToken distribution (Batch 11): {12: 1, 24: 2, 11: 1, 0: 2, 1: 1, 47: 1, 9: 1, 48: 1}\nToken distribution (Batch 12): {10: 1, 1: 2, 47: 1, 0: 2, 22: 1, 34: 1}\nToken distribution (Batch 13): {17: 1, 1: 2, 36: 1, 57: 1, 27: 1}\nToken distribution (Batch 14): {12: 4, 11: 1, 0: 1, 5: 1, 10: 1, 7: 1, 28: 1}\nToken distribution (Batch 15): {1: 1, 56: 1, 48: 1, 27: 1, 7: 1, 51: 1, 47: 1, 32: 1}\nToken distribution (Batch 16): {1: 2, 10: 1, 35: 1, 16: 1, 20: 1, 0: 2}\nToken distribution (Batch 17): {10: 1, 11: 1, 0: 3, 32: 1, 5: 1, 1: 2, 35: 1}\nToken distribution (Batch 18): {10: 1, 30: 1, 22: 1, 0: 1}\nToken distribution (Batch 19): {12: 1, 5: 1, 42: 1, 1: 1, 0: 2, 30: 1, 32: 1, 4: 1, 22: 1}\nToken distribution (Batch 20): {10: 1, 56: 3, 4: 1, 47: 1, 1: 1, 40: 1}\nToken distribution (Batch 21): {47: 1, 63: 1, 43: 1, 14: 1}\nToken distribution (Batch 22): {13: 1, 1: 1, 47: 1, 32: 1}\nToken distribution (Batch 23): {12: 1, 20: 1, 1: 1, 19: 1, 34: 1, 0: 1}\nToken distribution (Batch 24): {4: 1, 50: 1, 34: 2, 1: 1, 25: 1, 0: 1, 6: 1}\nToken distribution (Batch 25): {12: 1, 19: 1}\nToken distribution (Batch 26): {1: 2, 26: 1, 0: 2, 47: 1}\nToken distribution (Batch 27): {12: 1, 0: 1}\nToken distribution (Batch 28): {3: 1, 34: 1}\nToken distribution (Batch 29): {56: 1, 16: 1}\nToken distribution (Batch 30): {22: 1, 6: 1, 56: 1, 47: 1, 31: 1, 1: 2, 30: 1}\nToken distribution (Batch 31): {19: 1, 32: 2, 22: 1, 34: 1, 4: 1}\nBatch 30, Gradient norm: 499.2334\nEpoch 7, Batch 30/66, Loss: 50.8493\nAvg Blank Probability: 0.0383\nSample predictions: ['RwFlA2dw', 'ajIakak', 'zaF3ve']\nGround Truth (first 3): ['U7Dtf', 'ZxKU', 'dWi3']\nRaw outputs (first 3): [[ 0  1  0 27  0  0  0 34 47  0 34 12  0 17 12  0  0 10 10  0  0  0  0  0\n   0  0  1  0  3  0  0 19]\n [ 0  0  0 36  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0 32]\n [ 0  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0 42  0  0  0  0\n  34  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [5, 4, 4]\nToken distribution (Batch 0): {1: 3, 11: 1, 0: 3, 55: 1, 19: 1, 44: 1}\nToken distribution (Batch 1): {34: 1, 10: 1, 12: 1, 0: 5, 22: 1, 4: 1}\nToken distribution (Batch 2): {14: 1, 1: 2, 19: 1, 32: 1, 0: 3, 5: 1, 47: 1}\nToken distribution (Batch 3): {11: 1, 56: 1}\nToken distribution (Batch 4): {19: 1, 1: 2, 4: 1, 0: 2, 10: 1, 14: 1, 48: 1, 34: 1}\nToken distribution (Batch 5): {35: 1, 25: 1, 0: 3, 1: 1, 53: 1, 47: 1}\nToken distribution (Batch 6): {4: 2, 35: 2, 19: 1, 1: 1}\nToken distribution (Batch 7): {1: 1, 34: 3, 0: 5, 53: 1}\nToken distribution (Batch 8): {34: 2, 5: 1, 1: 1, 43: 1, 52: 1, 0: 2, 6: 1, 42: 1}\nToken distribution (Batch 9): {35: 1, 4: 1, 34: 1, 0: 2, 12: 1, 21: 1, 10: 1}\nToken distribution (Batch 10): {11: 1, 12: 1, 50: 1, 22: 1, 44: 1, 4: 1, 0: 2}\nToken distribution (Batch 11): {5: 1, 19: 1, 27: 1, 21: 1, 0: 2}\nToken distribution (Batch 12): {4: 1, 51: 1}\nToken distribution (Batch 13): {10: 1, 17: 1, 1: 2, 0: 2, 12: 1, 50: 1, 11: 1, 43: 1}\nToken distribution (Batch 14): {47: 2, 1: 2}\nToken distribution (Batch 15): {10: 1, 4: 1, 32: 1, 36: 1, 55: 1, 12: 1, 19: 1, 34: 1, 35: 1, 0: 1}\nToken distribution (Batch 16): {6: 1, 34: 1, 35: 1, 1: 2, 32: 1, 0: 2}\nToken distribution (Batch 17): {56: 1, 47: 1, 1: 3, 0: 2, 27: 1}\nToken distribution (Batch 18): {10: 2, 33: 1, 0: 1}\nToken distribution (Batch 19): {47: 2, 1: 1, 0: 2, 32: 1}\nToken distribution (Batch 20): {10: 1, 34: 1, 30: 1, 1: 1}\nToken distribution (Batch 21): {47: 1, 55: 1, 32: 1, 0: 3, 34: 1, 1: 1}\nToken distribution (Batch 22): {1: 1, 12: 1, 50: 1, 0: 1}\nToken distribution (Batch 23): {11: 1, 4: 1}\nToken distribution (Batch 24): {1: 2, 43: 1, 34: 1, 12: 2}\nToken distribution (Batch 25): {1: 1, 0: 1}\nToken distribution (Batch 26): {34: 1, 32: 1, 1: 1, 27: 2, 6: 1}\nToken distribution (Batch 27): {34: 1, 20: 1, 19: 2, 0: 4, 5: 1, 10: 1}\nToken distribution (Batch 28): {4: 1, 6: 1, 32: 1, 1: 2, 0: 2, 56: 1, 10: 2}\nToken distribution (Batch 29): {34: 1, 28: 1, 47: 1, 0: 3, 53: 1, 19: 1}\nToken distribution (Batch 30): {12: 1, 44: 1, 0: 3, 34: 1, 47: 1, 1: 1}\nToken distribution (Batch 31): {4: 1, 0: 1}\nBatch 40, Gradient norm: 15402.8506\nEpoch 7, Batch 40/66, Loss: 47.0445\nAvg Blank Probability: 0.0406\nSample predictions: ['ak2saR', 'Hjlvd', 'nasFeU']\nGround Truth (first 3): ['uJJX-', 'wSqWA', 'xl1Yx']\nRaw outputs (first 3): [[ 1  0  0 11  0  0  0  1  0  0  0  5  0  0 47  0  0 56 10 47 10 47  1  0\n   0  0 34  0  4 34  0  0]\n [ 0  0  0 56  0  0  0  0  0  0  0  0  0  0  0  0 34  0  0 47  0  0  0  4\n   0  0  0  0  6  0  0  0]\n [ 0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0 22]]\nInput length: 32, Label lengths: [5, 5, 5]\nToken distribution (Batch 0): {34: 1, 1: 4, 42: 1, 7: 1, 0: 1, 20: 1, 5: 1}\nToken distribution (Batch 1): {34: 1, 12: 1, 5: 1, 1: 1, 32: 1, 0: 1}\nToken distribution (Batch 2): {0: 6, 63: 1, 25: 1}\nToken distribution (Batch 3): {47: 1, 34: 1, 0: 4}\nToken distribution (Batch 4): {48: 1, 12: 1, 34: 1, 0: 2, 1: 1}\nToken distribution (Batch 5): {20: 1, 0: 2, 12: 1, 14: 1, 32: 1}\nToken distribution (Batch 6): {11: 1, 46: 1, 0: 4, 34: 2}\nToken distribution (Batch 7): {1: 2, 47: 1, 0: 3}\nToken distribution (Batch 8): {1: 1, 32: 1, 25: 1, 61: 1, 0: 3, 5: 1, 47: 1, 35: 1}\nToken distribution (Batch 9): {47: 1, 0: 1}\nToken distribution (Batch 10): {1: 1, 0: 4, 47: 1}\nToken distribution (Batch 11): {11: 1, 42: 1, 12: 1, 34: 1, 9: 1, 0: 2, 27: 1}\nToken distribution (Batch 12): {11: 1, 1: 2, 27: 2, 10: 1, 0: 1, 47: 1}\nToken distribution (Batch 13): {11: 1, 35: 1, 1: 1, 48: 1, 0: 1, 12: 1, 56: 1, 51: 1}\nToken distribution (Batch 14): {63: 1, 10: 1, 4: 1, 0: 1}\nToken distribution (Batch 15): {0: 3, 56: 1, 38: 1, 32: 1}\nToken distribution (Batch 16): {59: 1, 12: 1, 5: 1, 0: 3, 6: 1, 19: 1}\nToken distribution (Batch 17): {10: 1, 0: 1}\nToken distribution (Batch 18): {10: 1, 1: 1, 0: 2, 55: 1, 47: 1}\nToken distribution (Batch 19): {47: 1, 27: 1}\nToken distribution (Batch 20): {56: 2, 48: 2, 10: 1, 35: 1}\nToken distribution (Batch 21): {34: 1, 47: 1}\nToken distribution (Batch 22): {11: 1, 34: 1, 0: 4, 19: 2}\nToken distribution (Batch 23): {33: 1, 4: 2, 1: 1, 22: 1, 0: 2, 58: 1}\nToken distribution (Batch 24): {19: 1, 47: 2, 34: 1}\nToken distribution (Batch 25): {5: 1, 0: 3, 1: 1, 47: 2, 12: 2, 32: 1}\nToken distribution (Batch 26): {56: 1, 4: 1}\nToken distribution (Batch 27): {1: 2, 0: 1, 35: 1, 11: 1, 10: 1}\nToken distribution (Batch 28): {48: 1, 6: 1}\nToken distribution (Batch 29): {32: 1, 34: 1, 42: 1, 27: 1, 63: 1, 56: 1, 0: 2}\nToken distribution (Batch 30): {25: 1, 22: 1, 0: 2}\nToken distribution (Batch 31): {56: 1, 11: 1}\nBatch 50, Gradient norm: 100.7538\nEpoch 7, Batch 50/66, Loss: 52.6100\nAvg Blank Probability: 0.0436\nSample predictions: ['HaPgate', 'HleaF', '-y']\nGround Truth (first 3): ['nQD9O', 'L2B', 'bFM5']\nRaw outputs (first 3): [[34  0  0  0  0  0  0  1  0  0  0  0  0 11  0  0 59  0  0  0 56  0 11 33\n   0  0  0  0 48  0 25  0]\n [ 0 12 63  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 34  0\n   0  0  0  1  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0 27  1  0  0  0  1  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [5, 3, 4]\nToken distribution (Batch 0): {35: 1, 62: 1, 43: 1, 0: 1, 1: 1, 22: 1}\nToken distribution (Batch 1): {11: 1, 32: 1, 12: 1, 35: 1}\nToken distribution (Batch 2): {9: 1, 5: 1, 27: 3, 0: 3, 34: 1, 56: 1}\nToken distribution (Batch 3): {12: 1, 47: 1, 0: 4, 29: 1, 32: 1, 22: 1, 11: 1}\nToken distribution (Batch 4): {34: 1, 10: 1, 41: 1, 28: 1, 1: 1, 0: 2, 6: 1, 5: 1, 4: 1}\nToken distribution (Batch 5): {11: 1, 0: 4, 21: 1, 1: 1, 9: 1}\nToken distribution (Batch 6): {48: 1, 22: 1, 0: 2}\nToken distribution (Batch 7): {1: 1, 47: 1, 0: 3, 21: 1, 32: 1, 12: 1}\nToken distribution (Batch 8): {22: 1, 50: 1, 35: 1, 0: 1}\nToken distribution (Batch 9): {10: 1, 4: 1}\nToken distribution (Batch 10): {42: 1, 1: 1}\nToken distribution (Batch 11): {0: 7, 30: 1}\nToken distribution (Batch 12): {24: 1, 47: 1}\nToken distribution (Batch 13): {6: 1, 12: 1, 0: 5, 34: 2, 9: 1}\nToken distribution (Batch 14): {6: 1, 0: 2, 47: 1, 5: 1, 36: 1}\nToken distribution (Batch 15): {62: 1, 22: 1, 0: 2}\nToken distribution (Batch 16): {27: 1, 5: 1}\nToken distribution (Batch 17): {11: 1, 10: 1}\nToken distribution (Batch 18): {0: 5, 1: 1, 10: 1, 22: 1, 34: 1, 9: 1}\nToken distribution (Batch 19): {20: 1, 0: 5, 11: 2, 1: 1, 36: 1}\nToken distribution (Batch 20): {34: 2, 47: 1, 1: 2, 0: 5}\nToken distribution (Batch 21): {63: 1, 56: 1}\nToken distribution (Batch 22): {32: 1, 6: 1, 0: 1, 27: 1, 50: 1, 22: 1}\nToken distribution (Batch 23): {56: 1, 1: 2, 27: 2, 0: 4, 4: 1}\nToken distribution (Batch 24): {11: 1, 0: 3, 27: 3, 1: 1, 12: 1, 34: 1}\nToken distribution (Batch 25): {27: 2, 21: 1, 1: 1, 10: 1, 0: 4, 22: 1}\nToken distribution (Batch 26): {32: 2, 34: 1, 1: 1, 43: 1, 27: 1}\nToken distribution (Batch 27): {5: 1, 34: 1}\nToken distribution (Batch 28): {32: 2, 1: 1, 0: 1}\nToken distribution (Batch 29): {1: 1, 35: 1, 50: 2, 26: 1, 0: 1}\nToken distribution (Batch 30): {18: 1, 0: 6, 47: 1, 1: 1, 35: 1}\nToken distribution (Batch 31): {38: 1, 47: 1, 32: 1, 0: 3, 1: 1, 5: 1}\nBatch 60, Gradient norm: 107.9888\nEpoch 7, Batch 60/66, Loss: 50.9461\nAvg Blank Probability: 0.0463\nSample predictions: ['I9Qav', 'kFlI', 'ieAAH3A']\nGround Truth (first 3): ['RD6', 'iF', 'n3jVe']\nRaw outputs (first 3): [[ 0  0  0  0 34  0  0  0  0 10  0  0  0  0  6  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  1  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0 34  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  1  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [3, 2, 5]\nEpoch 7/20, Loss: 52.4891\nToken distribution (Batch 0): {1: 6}\nToken distribution (Batch 1): {1: 10}\nToken distribution (Batch 2): {1: 2}\nToken distribution (Batch 3): {1: 6}\nToken distribution (Batch 4): {1: 8}\nToken distribution (Batch 5): {1: 8}\nToken distribution (Batch 6): {1: 8}\nToken distribution (Batch 7): {1: 10}\nToken distribution (Batch 8): {1: 4}\nValidation Loss: 61.7385\nValidation Predictions: ['a', 'a', 'a', 'a', 'a']\nGround Truth: ['Yvw', 'j4CGh', 'L', 'PuL', 'Ocf0']\nCurrent Learning Rate: 2.199612826677113e-06\nEpoch 8, Filtered data size: 2604, Sample labels: ['qV8i', 'i', 'PnB']\nTrain size: 2083, Val size: 521\nToken distribution (Batch 0): {61: 1, 27: 1, 35: 1, 1: 1, 0: 3, 4: 1, 25: 1, 51: 1}\nToken distribution (Batch 1): {56: 1, 19: 1, 42: 2}\nToken distribution (Batch 2): {38: 1, 26: 1, 27: 1, 11: 1, 22: 1, 21: 1}\nToken distribution (Batch 3): {22: 1, 0: 2, 43: 1}\nToken distribution (Batch 4): {34: 1, 32: 1}\nToken distribution (Batch 5): {35: 1, 47: 2, 50: 2, 0: 2, 26: 1, 41: 1, 1: 1}\nToken distribution (Batch 6): {12: 1, 34: 1, 0: 3, 1: 1}\nToken distribution (Batch 7): {12: 2, 10: 1, 0: 4, 5: 1}\nToken distribution (Batch 8): {32: 1, 1: 1, 7: 1, 9: 1, 10: 1, 56: 1, 0: 2}\nToken distribution (Batch 9): {48: 1, 34: 3, 4: 1, 1: 1, 11: 1, 0: 3}\nToken distribution (Batch 10): {56: 1, 1: 1, 0: 7, 34: 1}\nToken distribution (Batch 11): {22: 1, 0: 3, 55: 1, 34: 1, 32: 1, 48: 1}\nToken distribution (Batch 12): {10: 1, 36: 1, 47: 1, 34: 1, 0: 5, 1: 1}\nToken distribution (Batch 13): {27: 1, 10: 1, 1: 1, 35: 1, 55: 1, 0: 1}\nToken distribution (Batch 14): {12: 1, 55: 1}\nToken distribution (Batch 15): {34: 1, 12: 2, 0: 5, 55: 1, 1: 1}\nToken distribution (Batch 16): {44: 1, 0: 4, 34: 1, 25: 1, 36: 1}\nToken distribution (Batch 17): {1: 2, 0: 2}\nToken distribution (Batch 18): {1: 1, 10: 1, 32: 2, 12: 1, 0: 4, 2: 1}\nToken distribution (Batch 19): {1: 2, 50: 1, 43: 1, 0: 2}\nToken distribution (Batch 20): {34: 1, 1: 2, 0: 5}\nToken distribution (Batch 21): {12: 1, 50: 1, 0: 5, 5: 1, 13: 1, 1: 1}\nToken distribution (Batch 22): {0: 3, 11: 1, 1: 1, 5: 1}\nToken distribution (Batch 23): {36: 1, 10: 1, 6: 1, 47: 1, 0: 4}\nToken distribution (Batch 24): {0: 1, 22: 1}\nToken distribution (Batch 25): {35: 1, 56: 1, 6: 1, 27: 1, 0: 2}\nToken distribution (Batch 26): {56: 2, 34: 1, 1: 2, 0: 1}\nToken distribution (Batch 27): {43: 1, 12: 2, 27: 1, 0: 2, 7: 1, 34: 1}\nToken distribution (Batch 28): {42: 1, 1: 1}\nToken distribution (Batch 29): {19: 1, 13: 1, 10: 1, 0: 1}\nToken distribution (Batch 30): {34: 1, 1: 2, 9: 1, 0: 4}\nToken distribution (Batch 31): {1: 1, 34: 1, 56: 1, 0: 1}\nBatch 0, Gradient norm: 152.3809\nEpoch 8, Batch 0/66, Loss: 44.3393\nAvg Blank Probability: 0.0485\nSample predictions: ['8AIadyY', '3sP', 'LzAkvu']\nGround Truth (first 3): ['MJOdz', '05', '5I8']\nRaw outputs (first 3): [[ 0  0 38  0  0 35  0  0  0  0 56 22  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0 56  0 42  0 34  0]\n [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0 12  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [5, 2, 3]\nToken distribution (Batch 0): {0: 8, 12: 1, 34: 1}\nToken distribution (Batch 1): {34: 2, 47: 1, 27: 1}\nToken distribution (Batch 2): {63: 1, 1: 1, 10: 2, 19: 1, 0: 2, 56: 1}\nToken distribution (Batch 3): {1: 1, 9: 1, 0: 1, 25: 1}\nToken distribution (Batch 4): {47: 1, 19: 1, 11: 1, 0: 5, 4: 1, 5: 1}\nToken distribution (Batch 5): {46: 1, 12: 1, 19: 1, 10: 1, 0: 3, 1: 1, 5: 1, 34: 1}\nToken distribution (Batch 6): {10: 1, 0: 7, 1: 2}\nToken distribution (Batch 7): {27: 1, 6: 1, 5: 1, 32: 1}\nToken distribution (Batch 8): {0: 1, 12: 1, 27: 1, 1: 1}\nToken distribution (Batch 9): {0: 1, 34: 1}\nToken distribution (Batch 10): {50: 1, 12: 1, 0: 2}\nToken distribution (Batch 11): {22: 2, 35: 1, 4: 1, 27: 1, 0: 1}\nToken distribution (Batch 12): {0: 4, 10: 1, 19: 1}\nToken distribution (Batch 13): {1: 3, 56: 1, 11: 1, 0: 5}\nToken distribution (Batch 14): {11: 1, 35: 1, 1: 1, 0: 1}\nToken distribution (Batch 15): {35: 1, 25: 1, 32: 1, 34: 1}\nToken distribution (Batch 16): {11: 1, 0: 4, 55: 1, 27: 1, 34: 1}\nToken distribution (Batch 17): {12: 1, 32: 1, 41: 1, 34: 1}\nToken distribution (Batch 18): {1: 1, 0: 6, 42: 1}\nToken distribution (Batch 19): {6: 2, 19: 1, 32: 2, 0: 5}\nToken distribution (Batch 20): {0: 1, 27: 1}\nToken distribution (Batch 21): {1: 1, 34: 1, 0: 2}\nToken distribution (Batch 22): {10: 1, 13: 1, 56: 1, 22: 1, 6: 1, 0: 5}\nToken distribution (Batch 23): {32: 1, 50: 2, 0: 1}\nToken distribution (Batch 24): {3: 1, 34: 1, 19: 1, 24: 1}\nToken distribution (Batch 25): {1: 1, 19: 1, 11: 1, 0: 4, 5: 1}\nToken distribution (Batch 26): {47: 1, 36: 1, 0: 2}\nToken distribution (Batch 27): {12: 1, 47: 1, 0: 1, 1: 1}\nToken distribution (Batch 28): {6: 2, 34: 1, 4: 1, 0: 2, 22: 1, 43: 1}\nToken distribution (Batch 29): {34: 1, 11: 1, 0: 3, 22: 1}\nToken distribution (Batch 30): {12: 1, 11: 2, 32: 1}\nToken distribution (Batch 31): {32: 1, 1: 1}\nBatch 10, Gradient norm: 157.2472\nEpoch 8, Batch 10/66, Loss: 47.0269\nAvg Blank Probability: 0.0528\nSample predictions: ['lH', 'HUA', '-ajs3']\nGround Truth (first 3): ['t0K4D', 'TK', 'ALMJ']\nRaw outputs (first 3): [[ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n   0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 32  0 19  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [5, 2, 4]\nToken distribution (Batch 0): {4: 1, 47: 1, 0: 4}\nToken distribution (Batch 1): {47: 1, 11: 2, 19: 1}\nToken distribution (Batch 2): {41: 1, 5: 1, 0: 1, 22: 1}\nToken distribution (Batch 3): {27: 1, 34: 1, 0: 1, 1: 1}\nToken distribution (Batch 4): {12: 1, 28: 1}\nToken distribution (Batch 5): {12: 1, 20: 1}\nToken distribution (Batch 6): {34: 1, 0: 7}\nToken distribution (Batch 7): {0: 6, 34: 2, 11: 1, 1: 1}\nToken distribution (Batch 8): {1: 1, 0: 6, 12: 1, 47: 1, 34: 1}\nToken distribution (Batch 9): {63: 1, 1: 1}\nToken distribution (Batch 10): {0: 7, 9: 1}\nToken distribution (Batch 11): {34: 1, 32: 1, 0: 6}\nToken distribution (Batch 12): {47: 1, 0: 4, 50: 1}\nToken distribution (Batch 13): {10: 1, 0: 1, 47: 1, 19: 1}\nToken distribution (Batch 14): {1: 1, 6: 1, 5: 1, 0: 1}\nToken distribution (Batch 15): {25: 1, 42: 1, 0: 7, 6: 1}\nToken distribution (Batch 16): {19: 1, 1: 1}\nToken distribution (Batch 17): {10: 1, 0: 5, 27: 1, 1: 1}\nToken distribution (Batch 18): {47: 1, 0: 4, 32: 1}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {11: 1, 27: 1, 0: 2}\nToken distribution (Batch 21): {33: 1, 0: 3}\nToken distribution (Batch 22): {0: 6, 1: 1, 47: 1}\nToken distribution (Batch 23): {27: 1, 0: 4, 7: 1}\nToken distribution (Batch 24): {10: 1, 0: 2, 11: 1}\nToken distribution (Batch 25): {4: 1, 1: 1, 0: 2}\nToken distribution (Batch 26): {48: 1, 0: 5, 12: 1, 1: 1}\nToken distribution (Batch 27): {48: 2, 0: 4}\nToken distribution (Batch 28): {50: 1, 34: 1, 0: 3, 47: 1}\nToken distribution (Batch 29): {0: 7, 26: 1}\nToken distribution (Batch 30): {19: 1, 0: 6, 20: 1}\nToken distribution (Batch 31): {11: 1, 0: 5, 32: 1, 1: 1}\nBatch 20, Gradient norm: 396.2370\nEpoch 8, Batch 20/66, Loss: 46.3609\nAvg Blank Probability: 0.0566\nSample predictions: ['dU', 'Uks', 'Oev']\nGround Truth (first 3): ['T-F', '9W', 'IG']\nRaw outputs (first 3): [[ 0  0  0  0  0  0  0  0  0 63  0  0  0  0  0 25  0  0  0  0 11  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  1  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [3, 2, 2]\nToken distribution (Batch 0): {0: 5, 1: 2, 10: 1}\nToken distribution (Batch 1): {26: 1, 1: 1, 0: 4}\nToken distribution (Batch 2): {61: 1, 10: 1, 50: 1, 0: 2, 32: 1}\nToken distribution (Batch 3): {10: 1, 34: 1, 0: 4}\nToken distribution (Batch 4): {0: 7, 47: 1, 12: 1, 1: 1}\nToken distribution (Batch 5): {47: 1, 0: 1}\nToken distribution (Batch 6): {10: 1, 12: 1}\nToken distribution (Batch 7): {1: 1, 0: 8, 7: 1}\nToken distribution (Batch 8): {0: 7, 47: 1, 26: 1, 35: 1}\nToken distribution (Batch 9): {56: 1, 32: 1, 0: 6}\nToken distribution (Batch 10): {0: 5, 1: 1, 34: 2}\nToken distribution (Batch 11): {0: 3, 11: 1, 27: 1, 23: 1}\nToken distribution (Batch 12): {50: 1, 10: 1, 0: 3, 4: 1}\nToken distribution (Batch 13): {42: 1, 0: 5, 19: 1, 34: 1}\nToken distribution (Batch 14): {22: 1, 12: 1, 19: 1, 0: 1}\nToken distribution (Batch 15): {35: 1, 0: 1}\nToken distribution (Batch 16): {10: 1, 47: 1, 0: 6}\nToken distribution (Batch 17): {6: 1, 0: 5}\nToken distribution (Batch 18): {35: 1, 47: 1}\nToken distribution (Batch 19): {11: 1, 63: 1, 1: 2, 0: 4}\nToken distribution (Batch 20): {12: 2}\nToken distribution (Batch 21): {27: 1, 12: 1, 0: 5, 47: 1}\nToken distribution (Batch 22): {0: 6, 1: 2}\nToken distribution (Batch 23): {47: 1, 11: 1, 0: 7, 14: 1}\nToken distribution (Batch 24): {12: 1, 0: 2, 36: 1}\nToken distribution (Batch 25): {6: 1, 19: 1, 32: 1, 50: 1, 0: 2}\nToken distribution (Batch 26): {1: 1, 12: 1, 0: 6}\nToken distribution (Batch 27): {10: 1, 0: 7}\nToken distribution (Batch 28): {1: 1, 12: 1}\nToken distribution (Batch 29): {13: 1, 0: 6, 34: 1}\nToken distribution (Batch 30): {10: 1, 0: 4, 48: 1}\nToken distribution (Batch 31): {0: 8}\nBatch 30, Gradient norm: 1598.8181\nEpoch 8, Batch 30/66, Loss: 44.6786\nAvg Blank Probability: 0.0622\nSample predictions: ['aja', 'za', '8jXF']\nGround Truth (first 3): ['KqsU', 'F6Y', '8MW']\nRaw outputs (first 3): [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0 12  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [4, 3, 3]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {63: 1, 10: 1, 55: 1, 4: 1, 0: 3, 14: 1}\nToken distribution (Batch 2): {12: 1, 0: 9}\nToken distribution (Batch 3): {1: 1, 34: 1, 0: 6}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 8, 22: 1, 9: 1}\nToken distribution (Batch 6): {47: 1, 0: 9}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {47: 1, 43: 1}\nToken distribution (Batch 9): {48: 1, 12: 1}\nToken distribution (Batch 10): {34: 1, 19: 1, 0: 2}\nToken distribution (Batch 11): {47: 1, 0: 3}\nToken distribution (Batch 12): {6: 1, 53: 1, 8: 1, 0: 1}\nToken distribution (Batch 13): {6: 1, 0: 1}\nToken distribution (Batch 14): {12: 1, 0: 3}\nToken distribution (Batch 15): {32: 1, 34: 1, 0: 2}\nToken distribution (Batch 16): {17: 1, 0: 7}\nToken distribution (Batch 17): {0: 3, 34: 1}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 9, 63: 1}\nToken distribution (Batch 20): {1: 1, 42: 1, 0: 5, 6: 1}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {47: 1, 12: 1, 0: 4}\nToken distribution (Batch 23): {12: 1, 1: 1, 0: 4}\nToken distribution (Batch 24): {0: 4, 1: 1, 5: 1}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {59: 1, 22: 1, 1: 2, 56: 1, 32: 1, 0: 2}\nToken distribution (Batch 27): {5: 1, 0: 1}\nToken distribution (Batch 28): {32: 1, 22: 1}\nToken distribution (Batch 29): {42: 1, 0: 6, 34: 1}\nToken distribution (Batch 30): {1: 1, 32: 1, 0: 2}\nToken distribution (Batch 31): {50: 1, 34: 1}\nBatch 40, Gradient norm: 124.7005\nEpoch 8, Batch 40/66, Loss: 49.5846\nAvg Blank Probability: 0.0658\nSample predictions: ['<empty>', '-j2dn', 'l']\nGround Truth (first 3): ['lAV', '5Lpu', 'L-eej']\nRaw outputs (first 3): [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0 42  0  0]\n [ 0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [3, 4, 5]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {7: 1, 0: 3}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 7, 1: 1}\nToken distribution (Batch 5): {24: 1, 4: 1, 0: 7, 43: 1}\nToken distribution (Batch 6): {12: 1, 0: 7}\nToken distribution (Batch 7): {12: 1, 0: 2, 1: 1}\nToken distribution (Batch 8): {7: 1, 51: 1, 0: 6}\nToken distribution (Batch 9): {11: 1, 0: 5}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {34: 1, 0: 4, 10: 1}\nToken distribution (Batch 12): {4: 1, 10: 1, 34: 1, 0: 3}\nToken distribution (Batch 13): {19: 1, 0: 9}\nToken distribution (Batch 14): {47: 1, 0: 4, 28: 1}\nToken distribution (Batch 15): {50: 1, 0: 6, 1: 1}\nToken distribution (Batch 16): {0: 3, 19: 1}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 3, 7: 1}\nToken distribution (Batch 19): {12: 1, 0: 9}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 6, 26: 2}\nToken distribution (Batch 22): {6: 1, 1: 1, 0: 5, 47: 1}\nToken distribution (Batch 23): {51: 1, 11: 1, 0: 2}\nToken distribution (Batch 24): {34: 1, 0: 7}\nToken distribution (Batch 25): {19: 1, 0: 5}\nToken distribution (Batch 26): {1: 1, 0: 5}\nToken distribution (Batch 27): {50: 1, 0: 3}\nToken distribution (Batch 28): {34: 1, 1: 1, 0: 2}\nToken distribution (Batch 29): {25: 1, 7: 1}\nToken distribution (Batch 30): {20: 1, 0: 2, 35: 1}\nToken distribution (Batch 31): {0: 10}\nBatch 50, Gradient norm: 1627.2430\nEpoch 8, Batch 50/66, Loss: 44.8620\nAvg Blank Probability: 0.0732\nSample predictions: ['<empty>', 'g', '<empty>']\nGround Truth (first 3): ['G', 'Q*', 'K']\nRaw outputs (first 3): [[ 0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [1, 2, 1]\nToken distribution (Batch 0): {10: 1, 0: 1}\nToken distribution (Batch 1): {30: 1, 0: 9}\nToken distribution (Batch 2): {47: 1, 0: 1}\nToken distribution (Batch 3): {50: 1, 11: 1}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 5, 34: 1}\nToken distribution (Batch 6): {34: 1, 0: 7}\nToken distribution (Batch 7): {32: 1, 0: 5}\nToken distribution (Batch 8): {6: 1, 0: 3}\nToken distribution (Batch 9): {0: 1, 1: 1}\nToken distribution (Batch 10): {1: 1, 0: 7}\nToken distribution (Batch 11): {34: 1, 12: 1, 0: 2}\nToken distribution (Batch 12): {1: 1, 0: 1}\nToken distribution (Batch 13): {35: 1, 0: 1}\nToken distribution (Batch 14): {0: 1, 13: 1}\nToken distribution (Batch 15): {10: 1, 0: 1}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {27: 1, 1: 1, 0: 4}\nToken distribution (Batch 18): {12: 1, 0: 5}\nToken distribution (Batch 19): {0: 4, 48: 1, 1: 1}\nToken distribution (Batch 20): {0: 7, 24: 1}\nToken distribution (Batch 21): {10: 1, 0: 3}\nToken distribution (Batch 22): {1: 1, 0: 7}\nToken distribution (Batch 23): {50: 1, 32: 1}\nToken distribution (Batch 24): {0: 5, 36: 1}\nToken distribution (Batch 25): {10: 1, 0: 7, 1: 1, 35: 1}\nToken distribution (Batch 26): {0: 2}\nToken distribution (Batch 27): {12: 1, 0: 1}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 5, 19: 1}\nToken distribution (Batch 30): {10: 1, 0: 9}\nToken distribution (Batch 31): {0: 8, 35: 1, 12: 1}\nBatch 60, Gradient norm: 157.1934\nEpoch 8, Batch 60/66, Loss: 52.8871\nAvg Blank Probability: 0.0829\nSample predictions: ['j', 'D', 'U']\nGround Truth (first 3): ['l', 'ZrSBq', 'c']\nRaw outputs (first 3): [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0 10  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]]\nInput length: 32, Label lengths: [1, 5, 1]\nEpoch 8/20, Loss: 47.4002\nToken distribution (Batch 0): {47: 1, 1: 6, 0: 3}\nToken distribution (Batch 1): {12: 1, 1: 3}\nToken distribution (Batch 2): {12: 1, 1: 5}\nToken distribution (Batch 3): {12: 1, 1: 8, 0: 1}\nToken distribution (Batch 4): {12: 1, 1: 1}\nToken distribution (Batch 5): {47: 1, 1: 1}\nToken distribution (Batch 6): {47: 1, 1: 5, 0: 2}\nToken distribution (Batch 7): {12: 1, 1: 3}\nToken distribution (Batch 8): {12: 1, 1: 7}\nValidation Loss: 59.2612\nValidation Predictions: ['Ua', 'la', 'la', 'la', 'la']\nGround Truth: ['r*dU-', 'On', '2hT', 'iE3z5', 'q']\nCurrent Learning Rate: 2.4764799846762878e-06\nEpoch 9, Filtered data size: 2604, Sample labels: ['qV8i', 'i', 'PnB']\nTrain size: 2083, Val size: 521\nToken distribution (Batch 0): {47: 1, 20: 1, 0: 2}\nToken distribution (Batch 1): {13: 1, 19: 1, 0: 4}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {19: 1, 0: 1}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {63: 1, 0: 9}\nToken distribution (Batch 7): {33: 1, 0: 5}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 1, 27: 1}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 4, 5: 1, 29: 1}\nToken distribution (Batch 12): {22: 1, 0: 6, 1: 1}\nToken distribution (Batch 13): {34: 1, 51: 1, 0: 2}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {9: 1, 0: 5}\nToken distribution (Batch 17): {10: 1, 0: 1}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {25: 1, 53: 1, 0: 2}\nToken distribution (Batch 20): {11: 1, 0: 9}\nToken distribution (Batch 21): {22: 1, 0: 7}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {11: 1, 0: 5}\nToken distribution (Batch 25): {50: 1, 0: 1}\nToken distribution (Batch 26): {0: 6, 5: 1, 4: 1}\nToken distribution (Batch 27): {10: 1, 0: 9}\nToken distribution (Batch 28): {0: 9, 12: 1}\nToken distribution (Batch 29): {56: 1, 0: 8, 25: 1}\nToken distribution (Batch 30): {55: 1, 34: 1, 32: 1, 0: 3}\nToken distribution (Batch 31): {17: 1, 0: 5}\nBatch 0, Gradient norm: 196.7987\nEpoch 9, Batch 0/66, Loss: 44.1361\nAvg Blank Probability: 0.0851\nSample predictions: ['Ut', 'ms', '<empty>']\nGround Truth (first 3): ['HT', 'Hlb', 'L7Vk']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 3, 4]\nToken distribution (Batch 0): {20: 1, 0: 3}\nToken distribution (Batch 1): {12: 1, 0: 7}\nToken distribution (Batch 2): {57: 1, 0: 9}\nToken distribution (Batch 3): {4: 1, 0: 1}\nToken distribution (Batch 4): {50: 1, 0: 3}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {27: 1, 0: 9}\nToken distribution (Batch 10): {1: 1, 0: 1}\nToken distribution (Batch 11): {0: 8, 4: 1, 43: 1}\nToken distribution (Batch 12): {35: 1, 0: 3}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 5, 1: 1}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {4: 1, 0: 5}\nToken distribution (Batch 18): {63: 1, 0: 7}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {11: 1, 0: 5}\nToken distribution (Batch 23): {60: 1, 0: 1}\nToken distribution (Batch 24): {11: 1, 0: 9}\nToken distribution (Batch 25): {0: 4}\nToken distribution (Batch 26): {0: 3, 19: 1}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {6: 1, 11: 1, 0: 2}\nToken distribution (Batch 30): {12: 1, 40: 1, 0: 4}\nToken distribution (Batch 31): {0: 10}\nBatch 10, Gradient norm: 14334.0400\nEpoch 9, Batch 10/66, Loss: 43.1790\nAvg Blank Probability: 0.0981\nSample predictions: ['t', 'l', '4']\nGround Truth (first 3): ['ad', 'mQxk', 'YhVSr']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 4, 5]\nToken distribution (Batch 0): {34: 1, 0: 5}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {6: 1, 0: 7}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {24: 1, 0: 5}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 9, 47: 1}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {26: 1, 0: 9}\nToken distribution (Batch 15): {14: 1, 0: 9}\nToken distribution (Batch 16): {14: 1, 0: 5}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 9, 34: 1}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 7, 12: 1}\nToken distribution (Batch 26): {0: 2}\nToken distribution (Batch 27): {11: 1, 0: 9}\nToken distribution (Batch 28): {63: 1, 0: 2, 12: 1}\nToken distribution (Batch 29): {0: 3, 6: 1}\nToken distribution (Batch 30): {50: 1, 0: 9}\nToken distribution (Batch 31): {0: 2}\nBatch 20, Gradient norm: 2438.1838\nEpoch 9, Batch 20/66, Loss: 36.1300\nAvg Blank Probability: 0.1157\nSample predictions: ['H', '<empty>', '<empty>']\nGround Truth (first 3): ['0*T', '-zp9', 'DwBi']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 4, 4]\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {11: 1, 0: 3}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 9, 1: 1}\nToken distribution (Batch 4): {34: 2, 0: 8}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {34: 1, 0: 3}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {12: 1, 0: 7}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 1, 11: 1}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 9, 53: 1}\nToken distribution (Batch 25): {0: 7, 12: 1}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 8}\nBatch 30, Gradient norm: 2080.2063\nEpoch 9, Batch 30/66, Loss: 34.1320\nAvg Blank Probability: 0.1276\nSample predictions: ['<empty>', 'k', '<empty>']\nGround Truth (first 3): ['dm', 's0', 'K2s']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 2, 3]\nToken distribution (Batch 0): {42: 1, 0: 1}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {34: 1, 0: 7}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 1, 27: 1}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {47: 1, 0: 7}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {36: 1, 0: 9}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {27: 1, 0: 1}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {22: 1, 0: 1}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 2}\nBatch 40, Gradient norm: 138.1682\nEpoch 9, Batch 40/66, Loss: 36.5889\nAvg Blank Probability: 0.1438\nSample predictions: ['P', '<empty>', '<empty>']\nGround Truth (first 3): ['z', 'g8X', 'Z']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 3, 1]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {4: 1, 0: 5}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 3, 32: 1}\nToken distribution (Batch 16): {5: 1, 0: 1}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 9, 34: 1}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 2}\nBatch 50, Gradient norm: 4497.8794\nEpoch 9, Batch 50/66, Loss: 36.6883\nAvg Blank Probability: 0.1622\nSample predictions: ['<empty>', '<empty>', 'd']\nGround Truth (first 3): ['KbANC', 'xj3G', 'T5Y']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 3]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 5, 1: 1}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {48: 1, 0: 7}\nToken distribution (Batch 23): {0: 4}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {35: 1, 0: 9}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 8}\nBatch 60, Gradient norm: 132.1000\nEpoch 9, Batch 60/66, Loss: 30.3869\nAvg Blank Probability: 0.1826\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['8', 'w1', 'Mk']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 2, 2]\nEpoch 9/20, Loss: 37.9718\nToken distribution (Batch 0): {12: 1, 0: 3}\nToken distribution (Batch 1): {12: 1, 0: 3}\nToken distribution (Batch 2): {12: 1, 0: 5}\nToken distribution (Batch 3): {12: 1, 0: 7}\nToken distribution (Batch 4): {12: 1, 0: 3}\nToken distribution (Batch 5): {12: 1, 0: 5}\nToken distribution (Batch 6): {12: 1, 0: 5}\nToken distribution (Batch 7): {12: 1, 0: 3}\nToken distribution (Batch 8): {12: 1, 0: 1}\nValidation Loss: 53.7581\nValidation Predictions: ['l', 'l', 'l', 'l', 'l']\nGround Truth: ['*B', 'EG', 'L2B', 'XSpH', 'DK']\nCurrent Learning Rate: 2.7403715199412507e-06\nEpoch 10, Filtered data size: 2604, Sample labels: ['qV8i', 'i', 'PnB']\nTrain size: 2083, Val size: 521\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 8}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {35: 1, 0: 9}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {43: 1, 0: 9}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 8}\nBatch 0, Gradient norm: 143.9670\nEpoch 10, Batch 0/66, Loss: 26.5335\nAvg Blank Probability: 0.1982\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['3U6', 'Nsc6', 'p']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 4, 1]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 5, 1: 1}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {47: 1, 0: 7}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 4}\nBatch 10, Gradient norm: 732.5681\nEpoch 10, Batch 10/66, Loss: 28.6973\nAvg Blank Probability: 0.2319\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['ahDQZ', 'A', 'f']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 1, 1]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 6}\nBatch 20, Gradient norm: 6649.5840\nEpoch 10, Batch 20/66, Loss: 26.8359\nAvg Blank Probability: 0.2624\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['o68L', 'bneT', 'O5al']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 4, 4]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {10: 1, 0: 1}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 4}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {36: 1, 0: 5}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 4}\nBatch 30, Gradient norm: 194.4573\nEpoch 10, Batch 30/66, Loss: 27.1570\nAvg Blank Probability: 0.3142\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['j-D*x', 'c8sos', 'FLP']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 5, 3]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 7, 56: 1}\nToken distribution (Batch 31): {0: 6}\nBatch 40, Gradient norm: 5190730.0000\nEpoch 10, Batch 40/66, Loss: 23.5846\nAvg Blank Probability: 0.3558\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['hACa', '5Za', 'oiUh']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 3, 4]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 2}\nBatch 50, Gradient norm: 10342.5010\nEpoch 10, Batch 50/66, Loss: 22.8132\nAvg Blank Probability: 0.4034\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['jY*7c', '5', 's0']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 1, 2]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 4}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 2}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 2}\nBatch 60, Gradient norm: 1727.9681\nEpoch 10, Batch 60/66, Loss: 19.9382\nAvg Blank Probability: 0.4472\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['pRXvu', 'oZROy', 'n4LAz']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 5, 5]\nEpoch 10/20, Loss: 25.4773\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 2}\nValidation Loss: 48.6300\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['Ck', 'R', 'En', 'jAh4', 'Du38J']\nCurrent Learning Rate: 2.988871889874007e-06\nEpoch 11, Filtered data size: 3620, Sample labels: ['qV8i', 'i', 'mer*BH']\nTrain size: 2896, Val size: 724\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 12}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 4}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 8}\nBatch 0, Gradient norm: 104.2361\nEpoch 11, Batch 0/91, Loss: 16.6430\nAvg Blank Probability: 0.4848\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['uCYo', 'P', 'wx-je']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 1, 5]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 12}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 4}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 4}\nBatch 10, Gradient norm: 315.3303\nEpoch 11, Batch 10/91, Loss: 15.2804\nAvg Blank Probability: 0.5726\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['RqAgCd', 'biTha', '93']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 5, 2]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 8}\nBatch 20, Gradient norm: 22738.8047\nEpoch 11, Batch 20/91, Loss: 10.8605\nAvg Blank Probability: 0.6531\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['gSGYn-', 'T9vty', '4haKGSo']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 5, 7]\nToken distribution (Batch 0): {0: 14}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 14}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 14}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 12}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 2}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 10}\nBatch 30, Gradient norm: 68.7348\nEpoch 11, Batch 30/91, Loss: 10.5207\nAvg Blank Probability: 0.7448\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Oqb511L', 'P', 'SF']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 1, 2]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 14}\nBatch 40, Gradient norm: 255.4759\nEpoch 11, Batch 40/91, Loss: 8.3574\nAvg Blank Probability: 0.8042\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['aIZk9O', '30Ez6', 'K8vUvMN']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 5, 7]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 8}\nBatch 50, Gradient norm: 133119.3750\nEpoch 11, Batch 50/91, Loss: 7.8976\nAvg Blank Probability: 0.8535\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['7ZZ', 'uWA78n0', 'dNm']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 7, 3]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 6}\nBatch 60, Gradient norm: 863.6535\nEpoch 11, Batch 60/91, Loss: 7.2261\nAvg Blank Probability: 0.8859\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['I*0', 'l', '3uam']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 1, 4]\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 4}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 2}\nBatch 70, Gradient norm: 17279506.0000\nEpoch 11, Batch 70/91, Loss: 6.8347\nAvg Blank Probability: 0.9285\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['hQ', 'iikgJBB', 'ZyI9z8r']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 7, 7]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 12}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 10}\nBatch 80, Gradient norm: 5.1632\nEpoch 11, Batch 80/91, Loss: 6.8104\nAvg Blank Probability: 0.9457\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['3', 'd40XO', 'UeXm8Z']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 5, 6]\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 12}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 14}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 12}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 10}\nBatch 90, Gradient norm: 878509924810752.0000\nEpoch 11, Batch 90/91, Loss: 6.6333\nAvg Blank Probability: 0.9600\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['HL', '9vQ54v5', 'TBnHM5*']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 7, 7]\nEpoch 11/20, Loss: 9.6555\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 12}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 12}\nValidation Loss: 23.6787\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['bLBIm9', 'h65z19', 'Noe', 'U-xr4', 'dh8hotR']\nCurrent Learning Rate: 4.374262139839078e-06\nEpoch 12, Filtered data size: 3620, Sample labels: ['qV8i', 'i', 'mer*BH']\nTrain size: 2896, Val size: 724\nToken distribution (Batch 0): {0: 14}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 12}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 2}\nBatch 0, Gradient norm: 23937265664.0000\nEpoch 12, Batch 0/91, Loss: 6.7943\nAvg Blank Probability: 0.9624\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['h7epS8M', 'O-ir', 'c']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 4, 1]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 12}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 14}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 14}\nBatch 10, Gradient norm: 6.2387\nEpoch 12, Batch 10/91, Loss: 6.8202\nAvg Blank Probability: 0.9668\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['QGLe0', 'er', 'lm']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 2, 2]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 14}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 14}\nBatch 20, Gradient norm: 126562616.0000\nEpoch 12, Batch 20/91, Loss: 6.9773\nAvg Blank Probability: 0.9718\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['kHfqx', 'wn4a7rf', '1N']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 7, 2]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 12}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 14}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 10}\nBatch 30, Gradient norm: 11.1530\nEpoch 12, Batch 30/91, Loss: 7.3507\nAvg Blank Probability: 0.9744\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Yvw', 'rT', 'yGW']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 2, 3]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 4}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 8}\nBatch 40, Gradient norm: 11.0237\nEpoch 12, Batch 40/91, Loss: 7.0696\nAvg Blank Probability: 0.9771\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['q6gTu', 'Ocf0', '02']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 2]\nToken distribution (Batch 0): {0: 14}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 6}\nBatch 50, Gradient norm: 11.4107\nEpoch 12, Batch 50/91, Loss: 7.2612\nAvg Blank Probability: 0.9763\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['yeZBt6x', 'WJ3im', 'rj2Y']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 5, 4]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 12}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 2}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 10}\nBatch 60, Gradient norm: 375.6573\nEpoch 12, Batch 60/91, Loss: 7.0381\nAvg Blank Probability: 0.9743\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['2JT', 'nQoAFh', 'q']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 6, 1]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 4}\nBatch 70, Gradient norm: 521.9654\nEpoch 12, Batch 70/91, Loss: 7.0311\nAvg Blank Probability: 0.9755\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['k', 'T1', 'XaGhKvP']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 2, 7]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 12}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 6}\nBatch 80, Gradient norm: 10.2884\nEpoch 12, Batch 80/91, Loss: 7.1774\nAvg Blank Probability: 0.9720\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['YWmFk6', 'AjNIAzj', 'xe5']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 7, 3]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 14}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 14}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 12}\nBatch 90, Gradient norm: 4632.3887\nEpoch 12, Batch 90/91, Loss: 6.8147\nAvg Blank Probability: 0.9764\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['3', 'BQ', 'j39MjIY']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 2, 7]\nEpoch 12/20, Loss: 6.9405\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 4}\nValidation Loss: 21.5559\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['uduag5', 'v', 'kXK', '4oAo', 'Xgg*r']\nCurrent Learning Rate: 3.763932022500211e-06\nEpoch 13, Filtered data size: 3620, Sample labels: ['qV8i', 'i', 'mer*BH']\nTrain size: 2896, Val size: 724\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 2}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 14}\nBatch 0, Gradient norm: 117871832.0000\nEpoch 13, Batch 0/91, Loss: 6.6828\nAvg Blank Probability: 0.9765\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['DMYT*', '2m09p04', 'rsJczXz']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 7, 7]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 12}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 10}\nBatch 10, Gradient norm: 10.3886\nEpoch 13, Batch 10/91, Loss: 7.1477\nAvg Blank Probability: 0.9746\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Zax', 'GblpNE', 'AO5O8Xz']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 6, 7]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 2}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 8}\nBatch 20, Gradient norm: 6022703.0000\nEpoch 13, Batch 20/91, Loss: 6.5688\nAvg Blank Probability: 0.9717\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['c', 'DZ', 'yE']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 2, 2]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 12}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 8}\nBatch 30, Gradient norm: 9.3055\nEpoch 13, Batch 30/91, Loss: 6.9445\nAvg Blank Probability: 0.9731\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Gpz4l', 'gHBHK', 'wL7']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 5, 3]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 14}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 14}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 12}\nBatch 40, Gradient norm: 7.3569\nEpoch 13, Batch 40/91, Loss: 6.9650\nAvg Blank Probability: 0.9677\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['xM7tm', '-', '7BJ']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 1, 3]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 12}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 2}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 10}\nBatch 50, Gradient norm: 7.5618\nEpoch 13, Batch 50/91, Loss: 6.9943\nAvg Blank Probability: 0.9680\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['3cT-U7', 'R', '2XR']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 1, 3]\nToken distribution (Batch 0): {0: 14}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 8}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 6}\nBatch 60, Gradient norm: 9.1651\nEpoch 13, Batch 60/91, Loss: 7.0499\nAvg Blank Probability: 0.9705\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['vYIU0WD', 'v8Nha', 'G4R']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 5, 3]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 4}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 14}\nBatch 70, Gradient norm: 379.1902\nEpoch 13, Batch 70/91, Loss: 6.7595\nAvg Blank Probability: 0.9700\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['RdZ1x', 'WOr3O', 'BVzm05v']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 5, 7]\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 4}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 2}\nBatch 80, Gradient norm: 28.5828\nEpoch 13, Batch 80/91, Loss: 6.9628\nAvg Blank Probability: 0.9731\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['oe', 'eKSRDzi', 'bxDZ']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 7, 4]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 12}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 2}\nBatch 90, Gradient norm: 1376265699328.0000\nEpoch 13, Batch 90/91, Loss: 6.3138\nAvg Blank Probability: 0.9710\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['FqXD', 't5ttxVy', 'LkN']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 7, 3]\nEpoch 13/20, Loss: 6.8882\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 14}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 14}\nValidation Loss: 22.1490\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['oiUh', 'PfCU3-m', 'n', 'o*N', 'C']\nCurrent Learning Rate: 3.1840380010418134e-06\nEpoch 14, Filtered data size: 3620, Sample labels: ['qV8i', 'i', 'mer*BH']\nTrain size: 2896, Val size: 724\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 8}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 14}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 14}\nBatch 0, Gradient norm: 3314.3240\nEpoch 14, Batch 0/91, Loss: 6.9549\nAvg Blank Probability: 0.9722\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['e', 'E6RxizT', 'QjW7gxl']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 7, 7]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 14}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 12}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 14}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 10}\nBatch 10, Gradient norm: 26168.1230\nEpoch 14, Batch 10/91, Loss: 7.0354\nAvg Blank Probability: 0.9747\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['7N7d', '0D', 'ixDqn']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 2, 5]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 12}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 12}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 14}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 2}\nBatch 20, Gradient norm: 1684520960.0000\nEpoch 14, Batch 20/91, Loss: 6.8795\nAvg Blank Probability: 0.9733\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['tKd4F', 'tkL', 'V*czz']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 3, 5]\nToken distribution (Batch 0): {0: 14}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 12}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 6}\nBatch 30, Gradient norm: 964720459776.0000\nEpoch 14, Batch 30/91, Loss: 6.8568\nAvg Blank Probability: 0.9746\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['XaGhKvP', 'VQ', 'PVS8nuq']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 2, 7]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 12}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 14}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 10}\nBatch 40, Gradient norm: 5.3030\nEpoch 14, Batch 40/91, Loss: 7.1708\nAvg Blank Probability: 0.9728\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['DQsMZ9', 'TAi', 'Q8dF']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 3, 4]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 12}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 12}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 14}\nBatch 50, Gradient norm: 11020993.0000\nEpoch 14, Batch 50/91, Loss: 7.1099\nAvg Blank Probability: 0.9776\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['*tIE', 'bVwNB', '19VXY']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 5, 5]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 12}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 12}\nBatch 60, Gradient norm: 4795005.5000\nEpoch 14, Batch 60/91, Loss: 6.8139\nAvg Blank Probability: 0.9772\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['u', '37', '6']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 2, 1]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 14}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 14}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 12}\nBatch 70, Gradient norm: 1120056066965504.0000\nEpoch 14, Batch 70/91, Loss: 6.9841\nAvg Blank Probability: 0.9765\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['h', 'UVE', 'j']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 3, 1]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 12}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 2}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 8}\nBatch 80, Gradient norm: 157673568.0000\nEpoch 14, Batch 80/91, Loss: 6.7606\nAvg Blank Probability: 0.9794\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['6JT6A', '*ukzOg', 'ks']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 6, 2]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 12}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 6}\nBatch 90, Gradient norm: 543.0860\nEpoch 14, Batch 90/91, Loss: 6.8164\nAvg Blank Probability: 0.9767\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['b3OhG', 'yn*', 'rOq']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 3, 3]\nEpoch 14/20, Loss: 6.9527\nToken distribution (Batch 0): {0: 14}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 14}\nValidation Loss: 21.5920\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['exRVivK', '1', 'zLD', 'ALMJ', '32']\nCurrent Learning Rate: 2.6488589908301082e-06\nEpoch 15, Filtered data size: 3620, Sample labels: ['qV8i', 'i', 'mer*BH']\nTrain size: 2896, Val size: 724\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 12}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 14}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 12}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 10}\nBatch 0, Gradient norm: 10.5408\nEpoch 15, Batch 0/91, Loss: 7.1067\nAvg Blank Probability: 0.9766\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Ub', '7VLzPb', '-k']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 6, 2]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 14}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 8}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 2}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 8}\nBatch 10, Gradient norm: 10.7068\nEpoch 15, Batch 10/91, Loss: 6.9659\nAvg Blank Probability: 0.9762\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['HlfFN', '5hb', 'Hl']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 3, 2]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 4}\nToken distribution (Batch 26): {0: 2}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 14}\nBatch 20, Gradient norm: 1462.4600\nEpoch 15, Batch 20/91, Loss: 7.0181\nAvg Blank Probability: 0.9743\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['7Ifh9Q', 'UuGt', 'W4WB']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 4, 4]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 2}\nBatch 30, Gradient norm: 906340992.0000\nEpoch 15, Batch 30/91, Loss: 6.7910\nAvg Blank Probability: 0.9744\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['TA9pm', 'EItm', 'C']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 1]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 12}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 12}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 4}\nBatch 40, Gradient norm: 20.5562\nEpoch 15, Batch 40/91, Loss: 6.9244\nAvg Blank Probability: 0.9741\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['ViIigG', '9k0', 'PCa']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 3, 3]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 2}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 10}\nBatch 50, Gradient norm: 419.0182\nEpoch 15, Batch 50/91, Loss: 6.9702\nAvg Blank Probability: 0.9737\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['v', 'u', 'cCNy0u1']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 1, 7]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 12}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 2}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 14}\nBatch 60, Gradient norm: 9.9068\nEpoch 15, Batch 60/91, Loss: 7.0022\nAvg Blank Probability: 0.9740\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['n3jVe', 'Sbl', 'HEu9kyT']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 3, 7]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 4}\nBatch 70, Gradient norm: 2249463485693952.0000\nEpoch 15, Batch 70/91, Loss: 6.9005\nAvg Blank Probability: 0.9759\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['uwYu', 'ZEvd7', 'N94']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 5, 3]\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 8}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 10}\nBatch 80, Gradient norm: 74892.0859\nEpoch 15, Batch 80/91, Loss: 6.7316\nAvg Blank Probability: 0.9738\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Zg', 'Uw0', 'fA']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 3, 2]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 2}\nBatch 90, Gradient norm: 10.0391\nEpoch 15, Batch 90/91, Loss: 6.7191\nAvg Blank Probability: 0.9737\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['6Ki', 'uVaQJ', 'cqS']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 5, 3]\nEpoch 15/20, Loss: 6.9412\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 12}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 8}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 14}\nValidation Loss: 23.0893\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['G', 'Wa131', 'q', 'SdH', 'mSQ']\nCurrent Learning Rate: 2.1715728752538105e-06\nEpoch 16, Filtered data size: 5120, Sample labels: ['qV8i', 'i', 'mer*BH']\nTrain size: 4096, Val size: 1024\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 12}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 18}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 18}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 20}\nToken distribution (Batch 10): {0: 16}\nToken distribution (Batch 11): {0: 18}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 14}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 14}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 20}\nToken distribution (Batch 31): {0: 8}\nBatch 0, Gradient norm: 2672.9939\nEpoch 16, Batch 0/128, Loss: 7.0169\nAvg Blank Probability: 0.9738\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['v', 'xpYZA5', 'Ub']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 6, 2]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 16}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 14}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 20}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 20}\nToken distribution (Batch 15): {0: 18}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 20}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 18}\nToken distribution (Batch 21): {0: 18}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 16}\nToken distribution (Batch 25): {0: 4}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 16}\nToken distribution (Batch 28): {0: 20}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 10}\nBatch 10, Gradient norm: inf\nEpoch 16, Batch 10/128, Loss: 7.0961\nAvg Blank Probability: 0.9743\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['ubsoqa', 'blCQ', 'pX']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 4, 2]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 12}\nToken distribution (Batch 3): {0: 18}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 16}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 16}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 18}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 8}\nToken distribution (Batch 16): {0: 18}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 18}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 16}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 18}\nToken distribution (Batch 30): {0: 20}\nToken distribution (Batch 31): {0: 6}\nBatch 20, Gradient norm: 13374314.0000\nEpoch 16, Batch 20/128, Loss: 7.2422\nAvg Blank Probability: 0.9750\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['2JJ', 'ZGrO', 'xFfxJd']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 4, 6]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 12}\nToken distribution (Batch 3): {0: 18}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 20}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 16}\nToken distribution (Batch 11): {0: 20}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 18}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 18}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 20}\nToken distribution (Batch 29): {0: 20}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 10}\nBatch 30, Gradient norm: inf\nEpoch 16, Batch 30/128, Loss: 7.0457\nAvg Blank Probability: 0.9697\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['xXf5uZ', 'lw', 'I0kNxv']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 2, 6]\nToken distribution (Batch 0): {0: 18}\nToken distribution (Batch 1): {0: 18}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 12}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 16}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 18}\nToken distribution (Batch 10): {0: 18}\nToken distribution (Batch 11): {0: 16}\nToken distribution (Batch 12): {0: 18}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 18}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 18}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 20}\nToken distribution (Batch 25): {0: 20}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 20}\nToken distribution (Batch 29): {0: 20}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 18}\nBatch 40, Gradient norm: 12.5694\nEpoch 16, Batch 40/128, Loss: 7.3205\nAvg Blank Probability: 0.9742\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Y1amuTnSp', 'w2kXVHIec', 'BahV']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [9, 9, 4]\nToken distribution (Batch 0): {0: 16}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 12}\nToken distribution (Batch 3): {0: 20}\nToken distribution (Batch 4): {0: 16}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 16}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 20}\nToken distribution (Batch 14): {0: 20}\nToken distribution (Batch 15): {0: 20}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 16}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 2}\nBatch 50, Gradient norm: 965.8952\nEpoch 16, Batch 50/128, Loss: 7.1493\nAvg Blank Probability: 0.9734\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['V90V682C', 'L', 'QUXKSN']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [8, 1, 6]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 20}\nToken distribution (Batch 2): {0: 12}\nToken distribution (Batch 3): {0: 20}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 20}\nToken distribution (Batch 8): {0: 18}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 20}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 20}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 16}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 18}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 16}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 18}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 20}\nToken distribution (Batch 29): {0: 16}\nToken distribution (Batch 30): {0: 18}\nToken distribution (Batch 31): {0: 14}\nBatch 60, Gradient norm: 20027336704.0000\nEpoch 16, Batch 60/128, Loss: 7.3364\nAvg Blank Probability: 0.9713\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['oPE*cz', 'RlxA*QNSli', 'fWbbc6']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 10, 6]\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 16}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 12}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 18}\nToken distribution (Batch 14): {0: 20}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 20}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 20}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 20}\nToken distribution (Batch 31): {0: 12}\nBatch 70, Gradient norm: inf\nEpoch 16, Batch 70/128, Loss: 7.0898\nAvg Blank Probability: 0.9780\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['56', 'v*LHo0TY', 'lSPYJWf']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 8, 7]\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 18}\nToken distribution (Batch 3): {0: 20}\nToken distribution (Batch 4): {0: 20}\nToken distribution (Batch 5): {0: 16}\nToken distribution (Batch 6): {0: 12}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 18}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 20}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 20}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 18}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 8}\nBatch 80, Gradient norm: 74476.1797\nEpoch 16, Batch 80/128, Loss: 7.3144\nAvg Blank Probability: 0.9753\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['iA', 'Yp5', '5rxkXtyka']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 3, 9]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 16}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 18}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 16}\nToken distribution (Batch 14): {0: 16}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 16}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 20}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 20}\nToken distribution (Batch 26): {0: 20}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 18}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 12}\nBatch 90, Gradient norm: 74.5472\nEpoch 16, Batch 90/128, Loss: 7.3772\nAvg Blank Probability: 0.9777\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['tkg', 'Ocf0', 'Qi']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 4, 2]\nToken distribution (Batch 0): {0: 16}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 20}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 18}\nToken distribution (Batch 5): {0: 16}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 20}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 8}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 16}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 16}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 20}\nToken distribution (Batch 27): {0: 16}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 8}\nBatch 100, Gradient norm: 239.0452\nEpoch 16, Batch 100/128, Loss: 7.2999\nAvg Blank Probability: 0.9777\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['EqtUXBUD', 'RMii', 'rQVG7a8bAn']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [8, 4, 10]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 20}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 16}\nToken distribution (Batch 6): {0: 20}\nToken distribution (Batch 7): {0: 18}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 20}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 12}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 18}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 20}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 16}\nToken distribution (Batch 22): {0: 16}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 20}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 18}\nToken distribution (Batch 31): {0: 2}\nBatch 110, Gradient norm: 11.6965\nEpoch 16, Batch 110/128, Loss: 7.3912\nAvg Blank Probability: 0.9764\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['eMBEWR', 'xd', 'YiJn5tka6z']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 2, 10]\nToken distribution (Batch 0): {0: 18}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 16}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 20}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 16}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 4}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 14}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 10}\nBatch 120, Gradient norm: 363.3525\nEpoch 16, Batch 120/128, Loss: 7.0797\nAvg Blank Probability: 0.9766\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['UKi-iSUyo', 'Q', 'ooEkgjnB']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [9, 1, 8]\nEpoch 16/20, Loss: 7.2393\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 20}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 18}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 18}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 20}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 8}\nToken distribution (Batch 16): {0: 18}\nToken distribution (Batch 17): {0: 16}\nToken distribution (Batch 18): {0: 20}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 16}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 18}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 18}\nToken distribution (Batch 31): {0: 4}\nValidation Loss: 18.9265\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['5N0ju', 'MDVk', 'PbuH2h-L38', 'rdC-', '0bL-p5o']\nCurrent Learning Rate: 1.7639320225002107e-06\nEpoch 17, Filtered data size: 5120, Sample labels: ['qV8i', 'i', 'mer*BH']\nTrain size: 4096, Val size: 1024\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 16}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 16}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 18}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 20}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 18}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 18}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 14}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 20}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 16}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 16}\nBatch 0, Gradient norm: 449.9576\nEpoch 17, Batch 0/128, Loss: 7.2265\nAvg Blank Probability: 0.9772\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['xc', 'RkYgh', 'b33h*xc-']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 5, 8]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 16}\nToken distribution (Batch 2): {0: 20}\nToken distribution (Batch 3): {0: 16}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 18}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 16}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 18}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 18}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 20}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 18}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 4}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 16}\nToken distribution (Batch 29): {0: 20}\nToken distribution (Batch 30): {0: 16}\nToken distribution (Batch 31): {0: 2}\nBatch 10, Gradient norm: inf\nEpoch 17, Batch 10/128, Loss: 6.9589\nAvg Blank Probability: 0.9791\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['S', 'NJlcyoqK', 'nkX7OoRR9E']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 8, 10]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 16}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 18}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 18}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 16}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 20}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 16}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 18}\nToken distribution (Batch 17): {0: 18}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 18}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 20}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 20}\nBatch 20, Gradient norm: 1806766976.0000\nEpoch 17, Batch 20/128, Loss: 7.5772\nAvg Blank Probability: 0.9790\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['-Bs', 'LJI8DCME', 'yuDw3']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 8, 5]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 18}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 16}\nToken distribution (Batch 7): {0: 18}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 18}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 16}\nToken distribution (Batch 20): {0: 18}\nToken distribution (Batch 21): {0: 16}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 18}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 2}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 6}\nBatch 30, Gradient norm: 33932356.0000\nEpoch 17, Batch 30/128, Loss: 7.2689\nAvg Blank Probability: 0.9782\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['K7ebq', 'dP', '7FqjaQiPG']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 2, 9]\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 16}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 14}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 20}\nToken distribution (Batch 12): {0: 20}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 16}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 18}\nToken distribution (Batch 19): {0: 18}\nToken distribution (Batch 20): {0: 20}\nToken distribution (Batch 21): {0: 16}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 20}\nToken distribution (Batch 24): {0: 16}\nToken distribution (Batch 25): {0: 18}\nToken distribution (Batch 26): {0: 20}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 20}\nToken distribution (Batch 30): {0: 10}\nToken distribution (Batch 31): {0: 10}\nBatch 40, Gradient norm: 14.7588\nEpoch 17, Batch 40/128, Loss: 7.7820\nAvg Blank Probability: 0.9776\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['gZ9rU', 'jAh4', 'kXK']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [5, 4, 3]\nToken distribution (Batch 0): {0: 20}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 16}\nToken distribution (Batch 7): {0: 20}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 16}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 20}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 18}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 16}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 18}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 16}\nToken distribution (Batch 31): {0: 18}\nBatch 50, Gradient norm: 12.4640\nEpoch 17, Batch 50/128, Loss: 7.3246\nAvg Blank Probability: 0.9796\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['HkIUAN8CPh', '6', 'WOr3O']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [10, 1, 5]\nToken distribution (Batch 0): {0: 18}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 20}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 16}\nToken distribution (Batch 7): {0: 18}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 18}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 18}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 20}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 2}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 6}\nBatch 60, Gradient norm: 12.4734\nEpoch 17, Batch 60/128, Loss: 7.4611\nAvg Blank Probability: 0.9798\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['wCG6wX7fc', '76rX', 'w']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [9, 4, 1]\nToken distribution (Batch 0): {0: 20}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 16}\nToken distribution (Batch 4): {0: 18}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 20}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 18}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 16}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 8}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 16}\nToken distribution (Batch 20): {0: 18}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 16}\nToken distribution (Batch 29): {0: 16}\nToken distribution (Batch 30): {0: 16}\nToken distribution (Batch 31): {0: 16}\nBatch 70, Gradient norm: 241879471882240.0000\nEpoch 17, Batch 70/128, Loss: 7.4279\nAvg Blank Probability: 0.9767\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['p1TWXUAoWG', 'VNbV', '*dgl1']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [10, 4, 5]\nToken distribution (Batch 0): {0: 20}\nToken distribution (Batch 1): {0: 20}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 16}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 20}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 18}\nToken distribution (Batch 26): {0: 20}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 20}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 18}\nBatch 80, Gradient norm: 7.9390\nEpoch 17, Batch 80/128, Loss: 7.3773\nAvg Blank Probability: 0.9795\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['9KxJ3uEnv4', '7ONq2*yVPG', 'LQY']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [10, 10, 3]\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 20}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 18}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 16}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 12}\nToken distribution (Batch 12): {0: 20}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 18}\nToken distribution (Batch 18): {0: 16}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 20}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 16}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 20}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 18}\nToken distribution (Batch 31): {0: 12}\nBatch 90, Gradient norm: 85697396736.0000\nEpoch 17, Batch 90/128, Loss: 7.4755\nAvg Blank Probability: 0.9759\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['p9', 'Aofdh5RsDS', 'q']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 10, 1]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 12}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 16}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 20}\nToken distribution (Batch 20): {0: 20}\nToken distribution (Batch 21): {0: 20}\nToken distribution (Batch 22): {0: 20}\nToken distribution (Batch 23): {0: 4}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 16}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 18}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 4}\nBatch 100, Gradient norm: 59699368.0000\nEpoch 17, Batch 100/128, Loss: 7.3518\nAvg Blank Probability: 0.9764\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['NNv8U8', '60ko', '97Vu2R']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 4, 6]\nToken distribution (Batch 0): {0: 20}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 20}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 16}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 20}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 16}\nToken distribution (Batch 16): {0: 20}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 18}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 20}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 2}\nBatch 110, Gradient norm: 62.6179\nEpoch 17, Batch 110/128, Loss: 7.1707\nAvg Blank Probability: 0.9785\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Z5bX87Rpgi', 'bVwNB', '607MvwXKzK']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [10, 5, 10]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 18}\nToken distribution (Batch 10): {0: 20}\nToken distribution (Batch 11): {0: 16}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 16}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 20}\nToken distribution (Batch 20): {0: 16}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 18}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 18}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 14}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 14}\nBatch 120, Gradient norm: inf\nEpoch 17, Batch 120/128, Loss: 7.3805\nAvg Blank Probability: 0.9778\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Q', 'io', 'Y5jwp']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 2, 5]\nEpoch 17/20, Loss: 7.3569\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 12}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 20}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 10}\nToken distribution (Batch 9): {0: 20}\nToken distribution (Batch 10): {0: 18}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 16}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 18}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 18}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 18}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 20}\nToken distribution (Batch 31): {0: 20}\nValidation Loss: 18.8149\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['rn3tWG', 'Cg0k4r', 'Yp5', 'kPzmE48bf0', '8AIjG']\nCurrent Learning Rate: 1.435973903246529e-06\nEpoch 18, Filtered data size: 5120, Sample labels: ['qV8i', 'i', 'mer*BH']\nTrain size: 4096, Val size: 1024\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 20}\nToken distribution (Batch 2): {0: 18}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 18}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 20}\nToken distribution (Batch 11): {0: 20}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 18}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 18}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 2}\nBatch 0, Gradient norm: 1968886382592.0000\nEpoch 18, Batch 0/128, Loss: 7.3171\nAvg Blank Probability: 0.9791\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['yzB', 'MFC1jnHEBt', '7FqjaQiPG']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 10, 9]\nToken distribution (Batch 0): {0: 16}\nToken distribution (Batch 1): {0: 12}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 18}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 18}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 20}\nToken distribution (Batch 8): {0: 18}\nToken distribution (Batch 9): {0: 18}\nToken distribution (Batch 10): {0: 16}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 16}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 20}\nToken distribution (Batch 18): {0: 20}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 14}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 2}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 18}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 10}\nBatch 10, Gradient norm: 216997.2500\nEpoch 18, Batch 10/128, Loss: 7.4946\nAvg Blank Probability: 0.9794\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['8SCvKN0S', 'JwDvNN', '4oAo']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [8, 6, 4]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 20}\nToken distribution (Batch 6): {0: 18}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 20}\nToken distribution (Batch 11): {0: 16}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 20}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 18}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 18}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 20}\nToken distribution (Batch 30): {0: 16}\nToken distribution (Batch 31): {0: 10}\nBatch 20, Gradient norm: inf\nEpoch 18, Batch 20/128, Loss: 7.3456\nAvg Blank Probability: 0.9781\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['KtQhCC', 'k1Z', 'rf2o']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 3, 4]\nToken distribution (Batch 0): {0: 14}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 20}\nToken distribution (Batch 4): {0: 12}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 18}\nToken distribution (Batch 7): {0: 18}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 18}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 16}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 18}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 18}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 4}\nBatch 30, Gradient norm: 280542904320.0000\nEpoch 18, Batch 30/128, Loss: 7.3052\nAvg Blank Probability: 0.9781\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['PwC8ai3', 'zBv', 'K0b']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 3, 3]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 18}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 16}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 16}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 16}\nToken distribution (Batch 20): {0: 14}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 18}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 18}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 2}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 12}\nBatch 40, Gradient norm: inf\nEpoch 18, Batch 40/128, Loss: 7.1985\nAvg Blank Probability: 0.9778\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['TOI', 'bVwNB', 'IsX3zLd']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 5, 7]\nToken distribution (Batch 0): {0: 16}\nToken distribution (Batch 1): {0: 18}\nToken distribution (Batch 2): {0: 16}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 16}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 20}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 18}\nToken distribution (Batch 22): {0: 18}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 20}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 14}\nBatch 50, Gradient norm: 52911.3203\nEpoch 18, Batch 50/128, Loss: 7.3789\nAvg Blank Probability: 0.9800\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['WpNtD7cz', 'E6HnJZsVV', 'eQ8pMyQp']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [8, 9, 8]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 20}\nToken distribution (Batch 4): {0: 20}\nToken distribution (Batch 5): {0: 20}\nToken distribution (Batch 6): {0: 18}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 20}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 16}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 16}\nToken distribution (Batch 23): {0: 20}\nToken distribution (Batch 24): {0: 18}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 20}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 20}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 6}\nBatch 60, Gradient norm: 349695.8125\nEpoch 18, Batch 60/128, Loss: 7.4776\nAvg Blank Probability: 0.9791\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['5yccyJ', 'Bq', '0x']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 2, 2]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 18}\nToken distribution (Batch 2): {0: 16}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 20}\nToken distribution (Batch 8): {0: 16}\nToken distribution (Batch 9): {0: 14}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 20}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 20}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 16}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 20}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 14}\nBatch 70, Gradient norm: 11.8392\nEpoch 18, Batch 70/128, Loss: 7.2582\nAvg Blank Probability: 0.9794\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['JfCY', 'CZfOCe9FV', '4yILIwOF']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 9, 8]\nToken distribution (Batch 0): {0: 18}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 20}\nToken distribution (Batch 3): {0: 18}\nToken distribution (Batch 4): {0: 16}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 18}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 18}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 16}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 18}\nToken distribution (Batch 22): {0: 16}\nToken distribution (Batch 23): {0: 20}\nToken distribution (Batch 24): {0: 20}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 4}\nBatch 80, Gradient norm: 13.8250\nEpoch 18, Batch 80/128, Loss: 7.6198\nAvg Blank Probability: 0.9793\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['kAWIPaZw4', '7lDV', 'zninSN1y7G']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [9, 4, 10]\nToken distribution (Batch 0): {0: 18}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 16}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 18}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 18}\nToken distribution (Batch 17): {0: 18}\nToken distribution (Batch 18): {0: 16}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 2}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 20}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 16}\nToken distribution (Batch 28): {0: 20}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 10}\nBatch 90, Gradient norm: 89439648874496.0000\nEpoch 18, Batch 90/128, Loss: 7.2705\nAvg Blank Probability: 0.9796\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['3JuRA61mc', '2f', 'P7QbB']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [9, 2, 5]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 18}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 20}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 20}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 18}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 16}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 16}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 4}\nBatch 100, Gradient norm: inf\nEpoch 18, Batch 100/128, Loss: 7.3242\nAvg Blank Probability: 0.9807\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['2UiKEQ', '5JsDVGajf', '56']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 9, 2]\nToken distribution (Batch 0): {0: 16}\nToken distribution (Batch 1): {0: 16}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 18}\nToken distribution (Batch 7): {0: 6}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 4}\nToken distribution (Batch 11): {0: 20}\nToken distribution (Batch 12): {0: 16}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 16}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 20}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 14}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 8}\nBatch 110, Gradient norm: 3593769216.0000\nEpoch 18, Batch 110/128, Loss: 7.1059\nAvg Blank Probability: 0.9762\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['b005xtkb', 'X9rqrpSp', 'zOxS']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [8, 8, 4]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 16}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 16}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 18}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 2}\nToken distribution (Batch 10): {0: 20}\nToken distribution (Batch 11): {0: 18}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 8}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 20}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 16}\nToken distribution (Batch 24): {0: 20}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 20}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 16}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 18}\nBatch 120, Gradient norm: 35638829121536.0000\nEpoch 18, Batch 120/128, Loss: 7.5696\nAvg Blank Probability: 0.9796\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['r5y8', 'JHJlmh0i', 'NS7KJL6']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 8, 7]\nEpoch 18/20, Loss: 7.3750\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 18}\nToken distribution (Batch 3): {0: 16}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 12}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 18}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 16}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 20}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 18}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 16}\nToken distribution (Batch 27): {0: 20}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 16}\nValidation Loss: 18.5931\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['nq', 'fxHGS', '9IIwuwWAr', 'nwEPlz6V', '4rz']\nCurrent Learning Rate: 1.195773934819386e-06\nEpoch 19, Filtered data size: 5120, Sample labels: ['qV8i', 'i', 'mer*BH']\nTrain size: 4096, Val size: 1024\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 16}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 18}\nToken distribution (Batch 11): {0: 18}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 20}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 20}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 18}\nToken distribution (Batch 23): {0: 16}\nToken distribution (Batch 24): {0: 16}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 18}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 16}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 16}\nToken distribution (Batch 31): {0: 10}\nBatch 0, Gradient norm: 12.5181\nEpoch 19, Batch 0/128, Loss: 7.4641\nAvg Blank Probability: 0.9772\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Y', 'vpIY', 'AfNS6']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 4, 5]\nToken distribution (Batch 0): {0: 20}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 16}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 18}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 12}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 16}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 16}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 20}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 20}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 18}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 14}\nToken distribution (Batch 29): {0: 2}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 2}\nBatch 10, Gradient norm: 17089.7520\nEpoch 19, Batch 10/128, Loss: 7.4311\nAvg Blank Probability: 0.9780\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['rMdg2h0uii', 'M*li4jM', 'KFJJ7Egd']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [10, 7, 8]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 16}\nToken distribution (Batch 2): {0: 20}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 18}\nToken distribution (Batch 6): {0: 18}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 18}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 18}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 20}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 16}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 16}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 16}\nToken distribution (Batch 31): {0: 20}\nBatch 20, Gradient norm: 11.9045\nEpoch 19, Batch 20/128, Loss: 7.3245\nAvg Blank Probability: 0.9772\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['CYefHF', '1BaHucs7', '45t061hgf9']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 8, 10]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 16}\nToken distribution (Batch 2): {0: 16}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 14}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 16}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 14}\nBatch 30, Gradient norm: 59386420527104.0000\nEpoch 19, Batch 30/128, Loss: 7.0869\nAvg Blank Probability: 0.9784\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['uduag5', 'fr1jAmXu', 'RMaR5*7p']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 8, 8]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 20}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 20}\nToken distribution (Batch 7): {0: 14}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 20}\nToken distribution (Batch 10): {0: 20}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 16}\nToken distribution (Batch 14): {0: 16}\nToken distribution (Batch 15): {0: 20}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 20}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 20}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 16}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 16}\nToken distribution (Batch 31): {0: 2}\nBatch 40, Gradient norm: inf\nEpoch 19, Batch 40/128, Loss: 7.2630\nAvg Blank Probability: 0.9776\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['7zV', '-i6LwZrz47', 'lj1']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 10, 3]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 12}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 8}\nToken distribution (Batch 7): {0: 2}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 20}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 20}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 16}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 20}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 20}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 16}\nBatch 50, Gradient norm: 65125531648.0000\nEpoch 19, Batch 50/128, Loss: 6.9335\nAvg Blank Probability: 0.9769\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['1', '0pQUIWd', '4RpW']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 7, 4]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 20}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 12}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 16}\nToken distribution (Batch 10): {0: 14}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 4}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 20}\nToken distribution (Batch 21): {0: 4}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 12}\nToken distribution (Batch 25): {0: 20}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 14}\nBatch 60, Gradient norm: 823180263424.0000\nEpoch 19, Batch 60/128, Loss: 7.2404\nAvg Blank Probability: 0.9761\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['NkE9', 'GRgo5', 'MRrU']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 5, 4]\nToken distribution (Batch 0): {0: 6}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 16}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 12}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 16}\nToken distribution (Batch 9): {0: 16}\nToken distribution (Batch 10): {0: 16}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 14}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 12}\nToken distribution (Batch 20): {0: 20}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 16}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 14}\nBatch 70, Gradient norm: 14883243870388224.0000\nEpoch 19, Batch 70/128, Loss: 7.3199\nAvg Blank Probability: 0.9820\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['waV', 'Mx', 'yn*']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [3, 2, 3]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 14}\nToken distribution (Batch 6): {0: 16}\nToken distribution (Batch 7): {0: 18}\nToken distribution (Batch 8): {0: 8}\nToken distribution (Batch 9): {0: 16}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 20}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 20}\nToken distribution (Batch 18): {0: 18}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 20}\nToken distribution (Batch 22): {0: 18}\nToken distribution (Batch 23): {0: 6}\nToken distribution (Batch 24): {0: 2}\nToken distribution (Batch 25): {0: 14}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 16}\nToken distribution (Batch 28): {0: 16}\nToken distribution (Batch 29): {0: 18}\nToken distribution (Batch 30): {0: 18}\nToken distribution (Batch 31): {0: 12}\nBatch 80, Gradient norm: 410815135744.0000\nEpoch 19, Batch 80/128, Loss: 7.4558\nAvg Blank Probability: 0.9799\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['O', 'C5Wdy4c', 'WLe']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 7, 3]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 20}\nToken distribution (Batch 2): {0: 18}\nToken distribution (Batch 3): {0: 12}\nToken distribution (Batch 4): {0: 20}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 12}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 20}\nToken distribution (Batch 10): {0: 20}\nToken distribution (Batch 11): {0: 12}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 16}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 12}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 18}\nToken distribution (Batch 26): {0: 16}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 2}\nBatch 90, Gradient norm: 12.6055\nEpoch 19, Batch 90/128, Loss: 7.3981\nAvg Blank Probability: 0.9787\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['lZ2TUT', 'q9zklSaiuC', 'AXtKCJ05k']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 10, 9]\nToken distribution (Batch 0): {0: 14}\nToken distribution (Batch 1): {0: 16}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 12}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 4}\nToken distribution (Batch 6): {0: 20}\nToken distribution (Batch 7): {0: 18}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 12}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 14}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 8}\nToken distribution (Batch 17): {0: 20}\nToken distribution (Batch 18): {0: 8}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 8}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 10}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 20}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 20}\nToken distribution (Batch 30): {0: 18}\nToken distribution (Batch 31): {0: 20}\nBatch 100, Gradient norm: 32642453504.0000\nEpoch 19, Batch 100/128, Loss: 7.1377\nAvg Blank Probability: 0.9807\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['P5-Gz9K', 'zYARCOeb', '2j']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 8, 2]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 20}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 16}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 18}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 18}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 12}\nToken distribution (Batch 14): {0: 20}\nToken distribution (Batch 15): {0: 16}\nToken distribution (Batch 16): {0: 18}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 18}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 14}\nToken distribution (Batch 24): {0: 18}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 18}\nToken distribution (Batch 28): {0: 2}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 16}\nBatch 110, Gradient norm: 13.9360\nEpoch 19, Batch 110/128, Loss: 7.7158\nAvg Blank Probability: 0.9787\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['hACa', 'g', 'kOnkX']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 1, 5]\nToken distribution (Batch 0): {0: 14}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 16}\nToken distribution (Batch 3): {0: 18}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 14}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 10}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 2}\nToken distribution (Batch 12): {0: 20}\nToken distribution (Batch 13): {0: 18}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 20}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 10}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 20}\nToken distribution (Batch 21): {0: 20}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 18}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 10}\nToken distribution (Batch 29): {0: 4}\nToken distribution (Batch 30): {0: 4}\nToken distribution (Batch 31): {0: 8}\nBatch 120, Gradient norm: 14.1887\nEpoch 19, Batch 120/128, Loss: 7.4803\nAvg Blank Probability: 0.9790\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['**GKC5p', 'MI', 'Ql1pa--3']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 2, 8]\nEpoch 19/20, Loss: 7.3917\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 20}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 18}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 10}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 18}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 10}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 16}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 20}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 18}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 16}\nToken distribution (Batch 31): {0: 14}\nValidation Loss: 18.3718\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['Tn', 'zypb', 'U6LgEHC8oW', 'c', 'NXr']\nCurrent Learning Rate: 1.0492466376194493e-06\nEpoch 20, Filtered data size: 5120, Sample labels: ['qV8i', 'i', 'mer*BH']\nTrain size: 4096, Val size: 1024\nToken distribution (Batch 0): {0: 18}\nToken distribution (Batch 1): {0: 16}\nToken distribution (Batch 2): {0: 12}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 2}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 16}\nToken distribution (Batch 16): {0: 18}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 4}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 14}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 16}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 14}\nBatch 0, Gradient norm: 12.0243\nEpoch 20, Batch 0/128, Loss: 7.2896\nAvg Blank Probability: 0.9780\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['PDeSdwlOT', 'kPAyIFQu', 'fForlf']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [9, 8, 6]\nToken distribution (Batch 0): {0: 20}\nToken distribution (Batch 1): {0: 10}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 12}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 20}\nToken distribution (Batch 6): {0: 16}\nToken distribution (Batch 7): {0: 8}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 18}\nToken distribution (Batch 11): {0: 14}\nToken distribution (Batch 12): {0: 12}\nToken distribution (Batch 13): {0: 16}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 18}\nToken distribution (Batch 21): {0: 12}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 8}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 4}\nToken distribution (Batch 29): {0: 16}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 18}\nBatch 10, Gradient norm: 2399247794176.0000\nEpoch 20, Batch 10/128, Loss: 7.5362\nAvg Blank Probability: 0.9775\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['pOTlc0P5pw', 'u6XUi', 'n']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [10, 5, 1]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 16}\nToken distribution (Batch 4): {0: 16}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 14}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 16}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 20}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 12}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 16}\nToken distribution (Batch 19): {0: 10}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 20}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 10}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 20}\nToken distribution (Batch 26): {0: 16}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 16}\nToken distribution (Batch 30): {0: 16}\nToken distribution (Batch 31): {0: 4}\nBatch 20, Gradient norm: 10.2646\nEpoch 20, Batch 20/128, Loss: 7.6480\nAvg Blank Probability: 0.9804\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['6', '7N7d', 'g']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 4, 1]\nToken distribution (Batch 0): {0: 4}\nToken distribution (Batch 1): {0: 4}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 16}\nToken distribution (Batch 4): {0: 6}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 12}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 18}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 18}\nToken distribution (Batch 16): {0: 20}\nToken distribution (Batch 17): {0: 6}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 8}\nToken distribution (Batch 20): {0: 20}\nToken distribution (Batch 21): {0: 12}\nToken distribution (Batch 22): {0: 2}\nToken distribution (Batch 23): {0: 2}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 4}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 20}\nToken distribution (Batch 28): {0: 18}\nToken distribution (Batch 29): {0: 20}\nToken distribution (Batch 30): {0: 12}\nToken distribution (Batch 31): {0: 16}\nBatch 30, Gradient norm: 103839711232.0000\nEpoch 20, Batch 30/128, Loss: 6.8897\nAvg Blank Probability: 0.9767\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['T6', 'AE', 'q']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [2, 2, 1]\nToken distribution (Batch 0): {0: 16}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 4}\nToken distribution (Batch 3): {0: 18}\nToken distribution (Batch 4): {0: 14}\nToken distribution (Batch 5): {0: 16}\nToken distribution (Batch 6): {0: 2}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 16}\nToken distribution (Batch 10): {0: 18}\nToken distribution (Batch 11): {0: 8}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 20}\nToken distribution (Batch 14): {0: 20}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 14}\nToken distribution (Batch 17): {0: 10}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 14}\nToken distribution (Batch 21): {0: 16}\nToken distribution (Batch 22): {0: 20}\nToken distribution (Batch 23): {0: 20}\nToken distribution (Batch 24): {0: 16}\nToken distribution (Batch 25): {0: 6}\nToken distribution (Batch 26): {0: 4}\nToken distribution (Batch 27): {0: 14}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 18}\nToken distribution (Batch 31): {0: 8}\nBatch 40, Gradient norm: 51.2025\nEpoch 20, Batch 40/128, Loss: 7.7733\nAvg Blank Probability: 0.9784\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['MmvXqd05', '55kxoPw', 'K9']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [8, 7, 2]\nToken distribution (Batch 0): {0: 20}\nToken distribution (Batch 1): {0: 12}\nToken distribution (Batch 2): {0: 2}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 16}\nToken distribution (Batch 8): {0: 4}\nToken distribution (Batch 9): {0: 4}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 16}\nToken distribution (Batch 12): {0: 4}\nToken distribution (Batch 13): {0: 4}\nToken distribution (Batch 14): {0: 12}\nToken distribution (Batch 15): {0: 10}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 14}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 16}\nToken distribution (Batch 23): {0: 20}\nToken distribution (Batch 24): {0: 14}\nToken distribution (Batch 25): {0: 12}\nToken distribution (Batch 26): {0: 10}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 14}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 8}\nBatch 50, Gradient norm: inf\nEpoch 20, Batch 50/128, Loss: 6.9447\nAvg Blank Probability: 0.9783\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['9KaMuuhtgy', 'RQz0Sj', '8']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [10, 6, 1]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 20}\nToken distribution (Batch 2): {0: 8}\nToken distribution (Batch 3): {0: 14}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 8}\nToken distribution (Batch 6): {0: 20}\nToken distribution (Batch 7): {0: 14}\nToken distribution (Batch 8): {0: 20}\nToken distribution (Batch 9): {0: 20}\nToken distribution (Batch 10): {0: 16}\nToken distribution (Batch 11): {0: 20}\nToken distribution (Batch 12): {0: 6}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 10}\nToken distribution (Batch 15): {0: 16}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 20}\nToken distribution (Batch 18): {0: 20}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 10}\nToken distribution (Batch 21): {0: 2}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 20}\nToken distribution (Batch 25): {0: 20}\nToken distribution (Batch 26): {0: 14}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 14}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 20}\nBatch 60, Gradient norm: 37033594585088.0000\nEpoch 20, Batch 60/128, Loss: 7.5212\nAvg Blank Probability: 0.9766\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['Q', '1qwdH4orZZ', 'b0sr']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 10, 4]\nToken distribution (Batch 0): {0: 18}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 14}\nToken distribution (Batch 3): {0: 2}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 16}\nToken distribution (Batch 7): {0: 20}\nToken distribution (Batch 8): {0: 12}\nToken distribution (Batch 9): {0: 8}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 10}\nToken distribution (Batch 13): {0: 8}\nToken distribution (Batch 14): {0: 2}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 4}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 18}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 8}\nToken distribution (Batch 22): {0: 4}\nToken distribution (Batch 23): {0: 18}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 16}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 10}\nToken distribution (Batch 28): {0: 12}\nToken distribution (Batch 29): {0: 12}\nToken distribution (Batch 30): {0: 6}\nToken distribution (Batch 31): {0: 16}\nBatch 70, Gradient norm: 13.7383\nEpoch 20, Batch 70/128, Loss: 7.4280\nAvg Blank Probability: 0.9795\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['zapD0Ga3q', 'hjpH', 'H9Ih-81']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [9, 4, 7]\nToken distribution (Batch 0): {0: 8}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 20}\nToken distribution (Batch 3): {0: 8}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 18}\nToken distribution (Batch 6): {0: 12}\nToken distribution (Batch 7): {0: 20}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 18}\nToken distribution (Batch 10): {0: 6}\nToken distribution (Batch 11): {0: 12}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 6}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 14}\nToken distribution (Batch 18): {0: 20}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 18}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 12}\nToken distribution (Batch 23): {0: 12}\nToken distribution (Batch 24): {0: 4}\nToken distribution (Batch 25): {0: 10}\nToken distribution (Batch 26): {0: 2}\nToken distribution (Batch 27): {0: 2}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 18}\nBatch 80, Gradient norm: inf\nEpoch 20, Batch 80/128, Loss: 7.4164\nAvg Blank Probability: 0.9785\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['bUNq', '*m4W', 'BGcfsHl0j6']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [4, 4, 10]\nToken distribution (Batch 0): {0: 2}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 12}\nToken distribution (Batch 3): {0: 10}\nToken distribution (Batch 4): {0: 4}\nToken distribution (Batch 5): {0: 12}\nToken distribution (Batch 6): {0: 18}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 14}\nToken distribution (Batch 9): {0: 18}\nToken distribution (Batch 10): {0: 10}\nToken distribution (Batch 11): {0: 12}\nToken distribution (Batch 12): {0: 18}\nToken distribution (Batch 13): {0: 6}\nToken distribution (Batch 14): {0: 18}\nToken distribution (Batch 15): {0: 20}\nToken distribution (Batch 16): {0: 4}\nToken distribution (Batch 17): {0: 20}\nToken distribution (Batch 18): {0: 20}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 20}\nToken distribution (Batch 21): {0: 10}\nToken distribution (Batch 22): {0: 20}\nToken distribution (Batch 23): {0: 4}\nToken distribution (Batch 24): {0: 18}\nToken distribution (Batch 25): {0: 18}\nToken distribution (Batch 26): {0: 6}\nToken distribution (Batch 27): {0: 8}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 16}\nToken distribution (Batch 30): {0: 16}\nToken distribution (Batch 31): {0: 2}\nBatch 90, Gradient norm: 14.6432\nEpoch 20, Batch 90/128, Loss: 7.7108\nAvg Blank Probability: 0.9802\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['1', 'OWs', '2UiKEQ']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [1, 3, 6]\nToken distribution (Batch 0): {0: 14}\nToken distribution (Batch 1): {0: 6}\nToken distribution (Batch 2): {0: 12}\nToken distribution (Batch 3): {0: 12}\nToken distribution (Batch 4): {0: 2}\nToken distribution (Batch 5): {0: 14}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 10}\nToken distribution (Batch 8): {0: 6}\nToken distribution (Batch 9): {0: 20}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 16}\nToken distribution (Batch 12): {0: 18}\nToken distribution (Batch 13): {0: 12}\nToken distribution (Batch 14): {0: 20}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 12}\nToken distribution (Batch 17): {0: 2}\nToken distribution (Batch 18): {0: 4}\nToken distribution (Batch 19): {0: 2}\nToken distribution (Batch 20): {0: 6}\nToken distribution (Batch 21): {0: 14}\nToken distribution (Batch 22): {0: 20}\nToken distribution (Batch 23): {0: 18}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 8}\nToken distribution (Batch 26): {0: 16}\nToken distribution (Batch 27): {0: 6}\nToken distribution (Batch 28): {0: 20}\nToken distribution (Batch 29): {0: 6}\nToken distribution (Batch 30): {0: 18}\nToken distribution (Batch 31): {0: 16}\nBatch 100, Gradient norm: 2510693020584640512.0000\nEpoch 20, Batch 100/128, Loss: 7.3409\nAvg Blank Probability: 0.9810\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['09Vsfm3', 'k1Z', 'A8Be2f']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [7, 3, 6]\nToken distribution (Batch 0): {0: 12}\nToken distribution (Batch 1): {0: 2}\nToken distribution (Batch 2): {0: 16}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 16}\nToken distribution (Batch 5): {0: 6}\nToken distribution (Batch 6): {0: 10}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 20}\nToken distribution (Batch 10): {0: 18}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 14}\nToken distribution (Batch 14): {0: 4}\nToken distribution (Batch 15): {0: 16}\nToken distribution (Batch 16): {0: 6}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 6}\nToken distribution (Batch 19): {0: 6}\nToken distribution (Batch 20): {0: 4}\nToken distribution (Batch 21): {0: 12}\nToken distribution (Batch 22): {0: 6}\nToken distribution (Batch 23): {0: 8}\nToken distribution (Batch 24): {0: 6}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 8}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 6}\nToken distribution (Batch 29): {0: 18}\nToken distribution (Batch 30): {0: 2}\nToken distribution (Batch 31): {0: 2}\nBatch 110, Gradient norm: 78886.2344\nEpoch 20, Batch 110/128, Loss: 7.1009\nAvg Blank Probability: 0.9788\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['uBJcLx', 'l', '*mfyDLwx']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [6, 1, 8]\nToken distribution (Batch 0): {0: 16}\nToken distribution (Batch 1): {0: 8}\nToken distribution (Batch 2): {0: 10}\nToken distribution (Batch 3): {0: 6}\nToken distribution (Batch 4): {0: 10}\nToken distribution (Batch 5): {0: 2}\nToken distribution (Batch 6): {0: 6}\nToken distribution (Batch 7): {0: 12}\nToken distribution (Batch 8): {0: 2}\nToken distribution (Batch 9): {0: 6}\nToken distribution (Batch 10): {0: 18}\nToken distribution (Batch 11): {0: 4}\nToken distribution (Batch 12): {0: 2}\nToken distribution (Batch 13): {0: 10}\nToken distribution (Batch 14): {0: 6}\nToken distribution (Batch 15): {0: 2}\nToken distribution (Batch 16): {0: 18}\nToken distribution (Batch 17): {0: 8}\nToken distribution (Batch 18): {0: 2}\nToken distribution (Batch 19): {0: 18}\nToken distribution (Batch 20): {0: 18}\nToken distribution (Batch 21): {0: 18}\nToken distribution (Batch 22): {0: 20}\nToken distribution (Batch 23): {0: 20}\nToken distribution (Batch 24): {0: 10}\nToken distribution (Batch 25): {0: 18}\nToken distribution (Batch 26): {0: 20}\nToken distribution (Batch 27): {0: 12}\nToken distribution (Batch 28): {0: 8}\nToken distribution (Batch 29): {0: 8}\nToken distribution (Batch 30): {0: 8}\nToken distribution (Batch 31): {0: 4}\nBatch 120, Gradient norm: 13.2562\nEpoch 20, Batch 120/128, Loss: 7.5038\nAvg Blank Probability: 0.9794\nSample predictions: ['<empty>', '<empty>', '<empty>']\nGround Truth (first 3): ['UtXibJFR', 'GMTm', '65GcR']\nRaw outputs (first 3): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nInput length: 32, Label lengths: [8, 4, 5]\nEpoch 20/20, Loss: 7.4038\nToken distribution (Batch 0): {0: 10}\nToken distribution (Batch 1): {0: 14}\nToken distribution (Batch 2): {0: 6}\nToken distribution (Batch 3): {0: 4}\nToken distribution (Batch 4): {0: 8}\nToken distribution (Batch 5): {0: 10}\nToken distribution (Batch 6): {0: 4}\nToken distribution (Batch 7): {0: 4}\nToken distribution (Batch 8): {0: 18}\nToken distribution (Batch 9): {0: 18}\nToken distribution (Batch 10): {0: 8}\nToken distribution (Batch 11): {0: 6}\nToken distribution (Batch 12): {0: 8}\nToken distribution (Batch 13): {0: 2}\nToken distribution (Batch 14): {0: 16}\nToken distribution (Batch 15): {0: 14}\nToken distribution (Batch 16): {0: 2}\nToken distribution (Batch 17): {0: 12}\nToken distribution (Batch 18): {0: 12}\nToken distribution (Batch 19): {0: 14}\nToken distribution (Batch 20): {0: 12}\nToken distribution (Batch 21): {0: 6}\nToken distribution (Batch 22): {0: 8}\nToken distribution (Batch 23): {0: 16}\nToken distribution (Batch 24): {0: 16}\nToken distribution (Batch 25): {0: 2}\nToken distribution (Batch 26): {0: 12}\nToken distribution (Batch 27): {0: 4}\nToken distribution (Batch 28): {0: 20}\nToken distribution (Batch 29): {0: 10}\nToken distribution (Batch 30): {0: 14}\nToken distribution (Batch 31): {0: 16}\nValidation Loss: 18.9177\nValidation Predictions: ['<empty>', '<empty>', '<empty>', '<empty>', '<empty>']\nGround Truth: ['K7ebq', 'GXX3PhE', 'kuG', 'Bw', 'DV14']\nCurrent Learning Rate: 1e-06\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def zip_folder_with_shutil(source_folder, output_path):\n    '''Function for zip dir data'''\n    shutil.make_archive(output_path, 'zip', source_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:41:55.779636Z","iopub.execute_input":"2025-03-10T12:41:55.780006Z","iopub.status.idle":"2025-03-10T12:41:55.784908Z","shell.execute_reply.started":"2025-03-10T12:41:55.779914Z","shell.execute_reply":"2025-03-10T12:41:55.783848Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"zip_folder_with_shutil('/kaggle/working/backgrounds', '/kaggle/working/backgrounds')\nzip_folder_with_shutil('/kaggle/working/synthetic_data', '/kaggle/working/synthetic_data')\nzip_folder_with_shutil('/kaggle/working/model_dir', '/kaggle/working/model_dir')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:41:55.785835Z","iopub.execute_input":"2025-03-10T12:41:55.786147Z","iopub.status.idle":"2025-03-10T12:41:59.092522Z","shell.execute_reply.started":"2025-03-10T12:41:55.786120Z","shell.execute_reply":"2025-03-10T12:41:59.091355Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"zip_folder_with_shutil('/kaggle/working/runs', '/kaggle/working/runs')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:41:59.093515Z","iopub.execute_input":"2025-03-10T12:41:59.093839Z","iopub.status.idle":"2025-03-10T12:41:59.122226Z","shell.execute_reply.started":"2025-03-10T12:41:59.093812Z","shell.execute_reply":"2025-03-10T12:41:59.121176Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"#!tensorboard --logdir=/kaggle/working/runs --port 6006","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:41:59.123391Z","iopub.execute_input":"2025-03-10T12:41:59.123724Z","iopub.status.idle":"2025-03-10T12:41:59.127699Z","shell.execute_reply.started":"2025-03-10T12:41:59.123657Z","shell.execute_reply":"2025-03-10T12:41:59.126636Z"}},"outputs":[],"execution_count":28}]}