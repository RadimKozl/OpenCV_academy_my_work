{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10951507,"sourceType":"datasetVersion","datasetId":6783030}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---------------\n\n# **<font style=\"color:Black\">Create OCR by PyTorch</font>**\n-------------------\n-----------------","metadata":{}},{"cell_type":"code","source":"!pip install python-Levenshtein\n!pip install optuna\n!pip install optuna-dashboard --quiet\n!pip install --upgrade structlog","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:28:23.242779Z","iopub.execute_input":"2025-04-03T15:28:23.242999Z","iopub.status.idle":"2025-04-03T15:28:41.947950Z","shell.execute_reply.started":"2025-04-03T15:28:23.242978Z","shell.execute_reply":"2025-04-03T15:28:41.947083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install tensorboard\n!tensorboard --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:28:41.948932Z","iopub.execute_input":"2025-04-03T15:28:41.949243Z","iopub.status.idle":"2025-04-03T15:28:58.525528Z","shell.execute_reply.started":"2025-04-03T15:28:41.949214Z","shell.execute_reply":"2025-04-03T15:28:58.524698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport shutil\nimport random\nimport time\nimport datetime\nimport re\nimport string\nimport getpass\nimport threading\nfrom PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter\nimport numpy as np\nimport cv2\n\nimport logging\nimport structlog\nimport json\n\nfrom typing import Iterator, Tuple\n\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nfrom Levenshtein import distance as levenshtein_distance\n\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F  # Add this import\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport torch.nn.utils as nn_utils\n\nimport optuna\nfrom optuna.trial import Trial\nfrom optuna import visualization ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:28:58.526457Z","iopub.execute_input":"2025-04-03T15:28:58.526668Z","iopub.status.idle":"2025-04-03T15:29:08.166832Z","shell.execute_reply.started":"2025-04-03T15:28:58.526649Z","shell.execute_reply":"2025-04-03T15:29:08.165943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Logger settings</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"LOGS_DIR = os.path.join(\"/kaggle\",\"working\",\"logs\")\nos.makedirs(LOGS_DIR, exist_ok=True)\nlog_file_path = os.path.join(LOGS_DIR, \"train_logs.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.167743Z","iopub.execute_input":"2025-04-03T15:29:08.168275Z","iopub.status.idle":"2025-04-03T15:29:08.171964Z","shell.execute_reply.started":"2025-04-03T15:29:08.168248Z","shell.execute_reply":"2025-04-03T15:29:08.171200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration of the main logger\ndef configure_main_logger():\n    structlog.configure(\n        processors=[\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.add_logger_name,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(sort_keys=True),\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n    )\n\nconfigure_main_logger()  # Function call for configuring the main logger\n\n# Configuration Optuna logger\ndef configure_optuna_logger():\n    structlog.configure(\n        processors=[\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.add_logger_name,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(sort_keys=True),\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n    )\n\nconfigure_optuna_logger()  # Function call for Optuna logger configuration\n\n# Configuration Function logger\ndef configure_function_logger():\n    structlog.configure(\n        processors=[\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.add_logger_name,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(sort_keys=True),\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n    )\n\nconfigure_function_logger()  # Function call for Optuna logger configuration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.172928Z","iopub.execute_input":"2025-04-03T15:29:08.173216Z","iopub.status.idle":"2025-04-03T15:29:08.223365Z","shell.execute_reply.started":"2025-04-03T15:29:08.173190Z","shell.execute_reply":"2025-04-03T15:29:08.222520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom handler for writing logs to JSON file\nclass RotatingJSONFileHandler(logging.Handler):\n    def __init__(self, directory, max_bytes=10*1024*1024, backup_count=5):\n        super().__init__()\n        self.directory = directory\n        self.max_bytes = max_bytes\n        self.backup_count = backup_count\n        self.filename = self.get_filename()\n        self.file = open(self.filename, \"a\", encoding=\"utf-8\")\n\n    def get_filename(self):\n        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n        return os.path.join(self.directory, f\"train_{timestamp}.json\")\n\n    def should_rotate(self):\n        self.file.flush()\n        return os.path.getsize(self.filename) > self.max_bytes\n\n    def do_rotate(self):\n        self.file.close()\n        for i in range(self.backup_count - 1, 0, -1):\n            old_file = f\"{self.filename}.{i}\"\n            new_file = f\"{self.filename}.{i+1}\"\n            if os.path.exists(old_file):\n                os.rename(old_file, new_file)\n        if os.path.exists(self.filename):\n            os.rename(self.filename, f\"{self.filename}.1\")\n        self.filename = self.get_filename()\n        self.file = open(self.filename, \"a\", encoding=\"utf-8\")\n\n    def emit(self, record):\n        try:\n            if self.should_rotate():\n                self.do_rotate()\n\n            log_entry = record.getMessage()\n            try:\n                json.loads(log_entry)\n            except json.JSONDecodeError:\n                log_entry = json.dumps({\n                    \"timestamp\": record.created,\n                    \"level\": record.levelname,\n                    \"logger\": record.name,\n                    \"message\": record.msg,\n                    \"args\": record.args,\n                    \"exc_info\": record.exc_info if record.exc_info else None\n                })\n\n            self.file.write(log_entry + \"\\n\")\n            self.file.flush()\n        except Exception as e:\n            print(f\"Error in RotatingJSONFileHandler: {e}\")\n\n    def close(self):\n        self.file.close()\n        super().close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.225366Z","iopub.execute_input":"2025-04-03T15:29:08.225586Z","iopub.status.idle":"2025-04-03T15:29:08.243840Z","shell.execute_reply.started":"2025-04-03T15:29:08.225568Z","shell.execute_reply":"2025-04-03T15:29:08.243186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the main logger\nmain_logger = structlog.get_logger(\"mainlog\")\n\n# Creating an Optuna logger\noptuna_logger = structlog.get_logger(\"optunalog\")\n\n# Creating an Function logger\nfunction_logger = structlog.get_logger(\"functionlog\")\n\n# Set levels for each logger\nlogging.getLogger(\"mainlog\").setLevel(logging.DEBUG) # main_logger\nlogging.getLogger(\"optunalog\").setLevel(logging.DEBUG) # optuna_logger\nlogging.getLogger(\"functionlog\").setLevel(logging.DEBUG) # function_logger\n\n# Creating a RotatingJSONFileHandler\nrotating_json_file_handler = RotatingJSONFileHandler(LOGS_DIR)\n\n# Adding RotatingJSONFileHandler to the root logger\nlogging.getLogger().addHandler(rotating_json_file_handler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.245234Z","iopub.execute_input":"2025-04-03T15:29:08.245471Z","iopub.status.idle":"2025-04-03T15:29:08.260436Z","shell.execute_reply.started":"2025-04-03T15:29:08.245452Z","shell.execute_reply":"2025-04-03T15:29:08.259641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main_logger.info('Star process of notebook.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.261289Z","iopub.execute_input":"2025-04-03T15:29:08.261533Z","iopub.status.idle":"2025-04-03T15:29:08.267159Z","shell.execute_reply.started":"2025-04-03T15:29:08.261514Z","shell.execute_reply":"2025-04-03T15:29:08.266410Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Hyperparameters</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"seed = 3\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.268049Z","iopub.execute_input":"2025-04-03T15:29:08.268293Z","iopub.status.idle":"2025-04-03T15:29:08.286090Z","shell.execute_reply.started":"2025-04-03T15:29:08.268274Z","shell.execute_reply":"2025-04-03T15:29:08.285497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUT_DIR = os.path.join('/kaggle','working','synthetic_data','images')\nTENSORBOARD_DIR = os.path.join('/kaggle','working','runs')\nOPTUNA_DIR = os.path.join('/kaggle','working','optuna')\nMODEL_DIR = os.path.join('/kaggle','working','model_dir')\nLABELS_FILE = os.path.join('/kaggle','working','synthetic_data','labels.txt')\nNUM_SAMPLES = 20480  # Number of images generated\nIMG_WIDTH = 128\nIMG_HEIGHT = 32\nBATCH_SIZE = 32\nEPOCHS = 30\nLEARNING_RATE = 1.67196984476829e-08\nWEIGHT_DECAY =  0.0942890342924316\nWARMUP_STEPS = 514\nTEMPERATURE = 1.2389791109815292e-06\nCTC_ENTROPY_WEIGHT = 0.4723418968547364\nCTC_LABEL_SMOOTHING = 0.16862297966320244\nCTC_BLANK_PENALTY_WEIGHT = 0.13337416135751246\nBSD_BEAM_WIDTH = 12\nBSD_BLANK_PENALTY =  -0.10149835756207472\nBSD_LENGTH_PENALTY = -0.19953431904018504\nGRADIENT_CLIPPING_VALUE = 0.21583555718276623\nMAX_SEQ_LENGTH = None\nCHARSET = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789- \"  # Characters used in license plates\nFONT_DIR = os.path.join('/kaggle','input','google-fonts','GoogleFontScripts') # Folder with TrueType fonts (.ttf)\nBACKGROUND_DIR = os.path.join('/kaggle','working','backgrounds')  # New folder for background (optional)\nNUMBER_BACKGROUND_IMAGE = 4000\nNUMBER_OF_OPTUNA_TRIALS = 10\nGRADIENT_NORM_TRESHOLD = 100 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.286857Z","iopub.execute_input":"2025-04-03T15:29:08.287164Z","iopub.status.idle":"2025-04-03T15:29:08.293087Z","shell.execute_reply.started":"2025-04-03T15:29:08.287135Z","shell.execute_reply":"2025-04-03T15:29:08.292303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main_logger.info('Set Base hyperparameters.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.293943Z","iopub.execute_input":"2025-04-03T15:29:08.294262Z","iopub.status.idle":"2025-04-03T15:29:08.308145Z","shell.execute_reply.started":"2025-04-03T15:29:08.294229Z","shell.execute_reply":"2025-04-03T15:29:08.307481Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Utils support functions</font>**\n-------------------","metadata":{}},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Create output folders</font>**","metadata":{}},{"cell_type":"code","source":"os.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(BACKGROUND_DIR, exist_ok=True)\nos.makedirs(TENSORBOARD_DIR, exist_ok=True)\nos.makedirs(OPTUNA_DIR, exist_ok=True)\n\n# Creates a new file\nwith open(LABELS_FILE, 'w') as fp:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.308989Z","iopub.execute_input":"2025-04-03T15:29:08.309291Z","iopub.status.idle":"2025-04-03T15:29:08.320693Z","shell.execute_reply.started":"2025-04-03T15:29:08.309264Z","shell.execute_reply":"2025-04-03T15:29:08.319942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main_logger.info('Create dirs structure.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.321659Z","iopub.execute_input":"2025-04-03T15:29:08.321872Z","iopub.status.idle":"2025-04-03T15:29:08.330788Z","shell.execute_reply.started":"2025-04-03T15:29:08.321855Z","shell.execute_reply":"2025-04-03T15:29:08.330187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Load the font list (add the paths to the .ttf files to the \"fonts\" folder)</font>**","metadata":{}},{"cell_type":"code","source":"font_files = [\n    os.path.join(FONT_DIR, f) for f in os.listdir(FONT_DIR) \n    if f.endswith('.ttf') and os.path.isfile(os.path.join(FONT_DIR, f))\n]\nif not font_files:\n    raise FileNotFoundError(\"No fonts found in 'fonts' folder. Add .ttf files!\")\n    main_logger.error(\"No fonts found in 'fonts' folder. Add .ttf files!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:08.331757Z","iopub.execute_input":"2025-04-03T15:29:08.332043Z","iopub.status.idle":"2025-04-03T15:29:10.021372Z","shell.execute_reply.started":"2025-04-03T15:29:08.332012Z","shell.execute_reply":"2025-04-03T15:29:10.020685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main_logger.info('Load google-fonts files from dataset.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:10.022169Z","iopub.execute_input":"2025-04-03T15:29:10.022463Z","iopub.status.idle":"2025-04-03T15:29:10.026447Z","shell.execute_reply.started":"2025-04-03T15:29:10.022433Z","shell.execute_reply":"2025-04-03T15:29:10.025633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Generating a simple gradient background</font>**","metadata":{}},{"cell_type":"code","source":"def generate_gradient_background(filename, size=(128, 32)):\n    \"\"\"\n    Generates a gradient background image and saves it to a specified file.\n\n    This function creates a grayscale image with a soft gradient effect, where the color transitions\n    from a lighter gray at the top to a slightly darker gray at the bottom. The image is then blurred\n    to create a smooth background effect.\n\n    Args:\n        filename (str): The name of the file where the generated image will be saved.\n        size (tuple, optional): The dimensions (width, height) of the generated image. Defaults to (128, 32).\n\n    The function uses the Python Imaging Library (PIL) to create and manipulate the image.\n    \"\"\"\n    img = Image.new('L', size, color=230)  # Lighter gray as a base\n    draw = ImageDraw.Draw(img)\n    for y in range(size[1]):\n        # Soft gradient with low contrast\n        color = int(230 - 20 * (y / size[1]))  # From light gray to slightly darker\n        draw.line([(0, y), (size[0], y)], fill=color)\n    # Background blur\n    img = img.filter(ImageFilter.GaussianBlur(radius=2))\n    img.save(os.path.join(BACKGROUND_DIR, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:10.027168Z","iopub.execute_input":"2025-04-03T15:29:10.027413Z","iopub.status.idle":"2025-04-03T15:29:10.039853Z","shell.execute_reply.started":"2025-04-03T15:29:10.027391Z","shell.execute_reply":"2025-04-03T15:29:10.039157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Generate a background with noise (paper texture)</font>**","metadata":{}},{"cell_type":"code","source":"def generate_paper_texture(filename, size=(128, 32)):\n    img = Image.new('L', size, color=220)  # Lighter gray\n    noise = np.random.normal(0, 5, size).astype(np.uint8)  # Less noise\n    noise_img = Image.fromarray(noise)\n    img.paste(noise_img, (0, 0), noise_img)\n    # Blur for a softer effect\n    img = img.filter(ImageFilter.GaussianBlur(radius=1.5))\n    img.save(os.path.join(BACKGROUND_DIR, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:10.040633Z","iopub.execute_input":"2025-04-03T15:29:10.040902Z","iopub.status.idle":"2025-04-03T15:29:10.055195Z","shell.execute_reply.started":"2025-04-03T15:29:10.040875Z","shell.execute_reply":"2025-04-03T15:29:10.054561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Creating multiple backgrounds</font>**","metadata":{}},{"cell_type":"code","source":"for i in range(NUMBER_BACKGROUND_IMAGE):\n    generate_gradient_background(f\"gradient_{i}.png\")\n    generate_paper_texture(f\"paper_{i}.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:10.055959Z","iopub.execute_input":"2025-04-03T15:29:10.056241Z","iopub.status.idle":"2025-04-03T15:29:13.770266Z","shell.execute_reply.started":"2025-04-03T15:29:10.056213Z","shell.execute_reply":"2025-04-03T15:29:13.769622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main_logger.info('Background images was generated.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.771006Z","iopub.execute_input":"2025-04-03T15:29:13.771323Z","iopub.status.idle":"2025-04-03T15:29:13.775586Z","shell.execute_reply.started":"2025-04-03T15:29:13.771295Z","shell.execute_reply":"2025-04-03T15:29:13.774401Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Load backgrounds (optional, add images to the \"backgrounds\" folder)</font>**","metadata":{}},{"cell_type":"code","source":"background_files = (\n    [os.path.join(BACKGROUND_DIR, f) for f in os.listdir(BACKGROUND_DIR) \n     if f.endswith(('.png', '.jpg', '.jpeg'))] if os.path.exists(BACKGROUND_DIR) else []\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.780138Z","iopub.execute_input":"2025-04-03T15:29:13.780340Z","iopub.status.idle":"2025-04-03T15:29:13.845507Z","shell.execute_reply.started":"2025-04-03T15:29:13.780322Z","shell.execute_reply":"2025-04-03T15:29:13.844602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main_logger.info('Background images was loaded for useing.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.847195Z","iopub.execute_input":"2025-04-03T15:29:13.847421Z","iopub.status.idle":"2025-04-03T15:29:13.850964Z","shell.execute_reply.started":"2025-04-03T15:29:13.847402Z","shell.execute_reply":"2025-04-03T15:29:13.850172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">List of setting type of formats</font>**","metadata":{}},{"cell_type":"code","source":"formats = {\n    \"Czech Republic\": [\n        {\n            \"regex\": r\"[A-Z]\\d[A-Z]{5}\",\n            \"description\": \"One letter, one digit, five letters\",\n            \"weight\": 0.95  # Kept the same\n        },\n        {\n            \"regex\": r\"[A-Z]\\d-[A-Z]{5}\",\n            \"description\": \"One letter, one digit, dash, five letters\",\n            \"weight\": 0.05  # Kept the same\n        }\n    ],\n    \"Germany\": [\n        {\n            \"regex\": r\"[A-Z]-[A-Z]{2} \\d{3}\",\n            \"description\": \"One letter, dash, two letters, space, three digits\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce space and dash frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{2}\",\n            \"description\": \"Four letters, two digits\",\n            \"weight\": 0.15  # Increased from 0.1 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.7  # Kept the same\n        }\n    ],\n    \"France\": [\n        {\n            \"regex\": r\"[A-Z]{2}-\\d{3}-[A-Z]{2}\",\n            \"description\": \"Two letters, dash, three digits, dash, two letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{2} \\d{3} [A-Z]{2}\",\n            \"description\": \"Two letters, space, three digits, space, two letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce space frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{2}\\d{3}[A-Z]{2}\",\n            \"description\": \"Two letters, three digits, two letters\",\n            \"weight\": 0.15  # Increased from 0.1 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{2}\",\n            \"description\": \"Four letters, two digits\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.45  # Kept the same\n        }\n    ],\n    \"United Kingdom\": [\n        {\n            \"regex\": r\"[A-Z]{2}\\d{2} [A-Z]{3}\",\n            \"description\": \"Two letters, two digits, space, three letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce space frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{2}\\d{2}[A-Z]{3}\",\n            \"description\": \"Two letters, two digits, three letters\",\n            \"weight\": 0.15  # Increased from 0.1 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{1}\",\n            \"description\": \"Four letters, one digit\",\n            \"weight\": 0.7  # Kept the same\n        }\n    ],\n    \"Brazil\": [\n        {\n            \"regex\": r\"\\d{3}[A-Z]{4}\",\n            \"description\": \"Three digits, four letters\",\n            \"weight\": 0.35  # Increased from 0.3 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{2}\",\n            \"description\": \"Four letters, two digits\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{3}-\\d{2}-[A-Z]{2}\",\n            \"description\": \"Three letters, dash, two digits, dash, two letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.4  # Reduced from 0.45 to balance overall distribution\n        }\n    ],\n    \"Australia\": [\n        {\n            \"regex\": r\"[A-Z]{3}-\\d{3}\",\n            \"description\": \"Three letters, dash, three digits\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{3} \\d{3}\",\n            \"description\": \"Three letters, space, three digits\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce space frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{2}\",\n            \"description\": \"Four letters, two digits\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.6  # Increased from 0.55 to maintain letter frequency\n        }\n    ],\n    \"Austria\": [\n        {\n            \"regex\": r\"\\d[A-Z]{4}\\d{2}\",\n            \"description\": \"One digit, four letters, two digits\",\n            \"weight\": 0.35  # Increased from 0.3 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.5  # Kept the same\n        },\n        {\n            \"regex\": r\"\\d-[A-Z]{4}-\\d{2}\",\n            \"description\": \"One digit, dash, four letters, dash, two digits\",\n            \"weight\": 0.05  # Reduced from 0.1 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"\\d [A-Z]{4} \\d{2}\",\n            \"description\": \"One digit, space, four letters, space, two digits\",\n            \"weight\": 0.1  # Kept the same\n        }\n    ],\n    \"Italy\": [\n        {\n            \"regex\": r\"[A-Z]{2} \\d{3}[A-Z]{2}\",\n            \"description\": \"Two letters, space, three digits, space, two letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce space frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{2}-\\d{3}[A-Z]{2}\",\n            \"description\": \"Two letters, dash, three digits, two letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{2}\\d{3}[A-Z]{2}\",\n            \"description\": \"Two letters, three digits, two letters\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{2}\",\n            \"description\": \"Four letters, two digits\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.5  # Kept the same\n        }\n    ],\n    \"Belgium\": [\n        {\n            \"regex\": r\"\\d-[A-Z]{3}-\\d{3}\",\n            \"description\": \"One digit, dash, three letters, dash, three digits\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{2}\",\n            \"description\": \"Four letters, two digits\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.55  # Kept the same\n        },\n        {\n            \"regex\": r\"[A-Z] \\d{3} [A-Z]{3}\",\n            \"description\": \"One letter, space, three digits, space, three letters\",\n            \"weight\": 0.2  # Kept the same\n        }\n    ],\n    \"Spain\": [\n        {\n            \"regex\": r\"[A-Z]{4}\\d{3}\",\n            \"description\": \"Four letters, three digits\",\n            \"weight\": 0.35  # Increased from 0.3 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}-\\d{2}-[A-Z]\",\n            \"description\": \"Four letters, dash, two digits, dash, one letter\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.4  # Kept the same\n        },\n        {\n            \"regex\": r\"[A-Z]{4} \\d{2} [A-Z]\",\n            \"description\": \"Four letters, space, two digits, space, one letter\",\n            \"weight\": 0.1  # Kept the same\n        }\n    ],\n    \"Hungary\": [\n        {\n            \"regex\": r\"[A-Z] \\d{2} [A-Z]{3}\",\n            \"description\": \"One letter, space, two digits, space, three letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce space frequency\n        },\n        {\n            \"regex\": r\"[A-Z]\\d{2}[A-Z]{3}\",\n            \"description\": \"One letter, two digits, three letters\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{1}\",\n            \"description\": \"Four letters, one digit\",\n            \"weight\": 0.65  # Kept the same\n        },\n        {\n            \"regex\": r\"[A-Z]\\d-[A-Z]{3}-\\d\",\n            \"description\": \"One letter, one digit, dash, three letters, dash, one digit\",\n            \"weight\": 0.1  # Kept the same\n        }\n    ],\n    \"Norway\": [\n        {\n            \"regex\": r\"\\d{2}[A-Z]{5}\",\n            \"description\": \"Two digits, five letters\",\n            \"weight\": 0.35  # Increased from 0.3 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{2}-\\d{2}-[A-Z]{3}\",\n            \"description\": \"Two letters, dash, two digits, dash, three letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{2} \\d{2} [A-Z]{3}\",\n            \"description\": \"Two letters, space, two digits, space, three letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce space frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.35  # Increased from 0.3 to maintain letter frequency\n        }\n    ],\n    \"Sweden\": [\n        {\n            \"regex\": r\"[A-Z]{3} \\d{3}\",\n            \"description\": \"Three letters, space, three digits\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce space frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{2}\",\n            \"description\": \"Four letters, two digits\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.75  # Kept the same\n        }\n    ],\n    \"Netherlands\": [\n        {\n            \"regex\": r\"[A-Z]{2}-\\d{2}-[A-Z]{2}\",\n            \"description\": \"Two letters, dash, two digits, dash, two letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{2}\\d{2}[A-Z]{2}\",\n            \"description\": \"Two letters, two digits, two letters\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{1}\",\n            \"description\": \"Four letters, one digit\",\n            \"weight\": 0.55  # Kept the same\n        },\n        {\n            \"regex\": r\"[A-Z]{2} \\d{2} [A-Z]{2}\",\n            \"description\": \"Two letters, space, two digits, space, two letters\",\n            \"weight\": 0.2  # Reduced from 0.2 to maintain balance\n        }\n    ],\n    \"Serbia\": [\n        {\n            \"regex\": r\"[A-Z]{2} \\d{3}-[A-Z]{2}\",\n            \"description\": \"Two letters, space, three digits, dash, two letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce space and dash frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{2}\\d{3}[A-Z]{2}\",\n            \"description\": \"Two letters, three digits, two letters\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{2}\",\n            \"description\": \"Four letters, two digits\",\n            \"weight\": 0.05  # Kept the same\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.7  # Kept the same\n        }\n    ],\n    \"Ukraine\": [\n        {\n            \"regex\": r\"\\d{2}[A-Z]{4}\\d{2}\",\n            \"description\": \"Two digits, four letters, two digits\",\n            \"weight\": 0.35  # Increased from 0.3 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.5  # Kept the same\n        },\n        {\n            \"regex\": r\"\\d{2}-[A-Z]{4}-\\d\",\n            \"description\": \"Two digits, dash, four letters, dash, one digit\",\n            \"weight\": 0.05  # Reduced from 0.1 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"\\d{2} [A-Z]{4} \\d\",\n            \"description\": \"Two digits, space, four letters, space, one digit\",\n            \"weight\": 0.1  # Kept the same\n        }\n    ],\n    \"USA_v1\": [\n        {\n            \"regex\": r\"[A-Z]{4}\\d{2}\",\n            \"description\": \"Four letters, two digits\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.9  # Reduced from 0.95 to balance overall distribution\n        }\n    ],\n    \"USA_v2\": [\n        {\n            \"regex\": r\"\\d{3}-[A-Z]{3}\",\n            \"description\": \"Three digits, dash, three letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce dash frequency\n        },\n        {\n            \"regex\": r\"\\d{3} [A-Z]{3}\",\n            \"description\": \"Three digits, space, three letters\",\n            \"weight\": 0.15  # Reduced from 0.2 to reduce space frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{4}\\d{2}\",\n            \"description\": \"Four letters, two digits\",\n            \"weight\": 0.1  # Increased from 0.05 to boost digit frequency\n        },\n        {\n            \"regex\": r\"[A-Z]{5}\\d{1}\",\n            \"description\": \"Five letters, one digit\",\n            \"weight\": 0.6  # Increased from 0.55 to maintain letter frequency\n        }\n    ]\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.851747Z","iopub.execute_input":"2025-04-03T15:29:13.851978Z","iopub.status.idle":"2025-04-03T15:29:13.868680Z","shell.execute_reply.started":"2025-04-03T15:29:13.851958Z","shell.execute_reply":"2025-04-03T15:29:13.867799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Random text generation function</font>**","metadata":{}},{"cell_type":"code","source":"def generate_random_license_plate_from_regex(regex_format):\n    license_plate = \"\"\n    i = 0\n    \n    while i < len(regex_format):\n        char = regex_format[i]\n\n        # Handle escaped sequences like \\d or \\w\n        if char == '\\\\' and i + 1 < len(regex_format):\n            next_char = regex_format[i + 1]\n            repeat = 1\n            i += 2  # Skip backslash and the next character\n\n            # Check for repetition pattern {n}\n            if i < len(regex_format) and regex_format[i] == '{':\n                match = re.match(r\"\\{(\\d+)\\}\", regex_format[i:])\n                if match:\n                    repeat = int(match.group(1))\n                    i += len(match.group(0))  # Skip {n}\n\n            if next_char == 'd':\n                license_plate += ''.join(random.choice(string.digits) for _ in range(repeat))\n            elif next_char == 'w':\n                license_plate += ''.join(random.choice(string.ascii_uppercase) for _ in range(repeat))\n            else:\n                license_plate += next_char  # Treat as literal if not \\d or \\w\n\n        # Handle character classes like [A-Z] or [0-9]\n        elif char == '[':\n            j = regex_format.find(']', i)\n            if j != -1:\n                options = regex_format[i+1:j]\n                i = j + 1  # Move past ]\n\n                repeat = 1\n                if i < len(regex_format) and regex_format[i] == '{':\n                    match = re.match(r\"\\{(\\d+)\\}\", regex_format[i:])\n                    if match:\n                        repeat = int(match.group(1))\n                        i += len(match.group(0))  # Skip {n}\n\n                if options == 'A-Z':\n                    license_plate += ''.join(random.choice(string.ascii_uppercase) for _ in range(repeat))\n                elif options == '0-9':\n                    license_plate += ''.join(random.choice(string.digits) for _ in range(repeat))\n                else:\n                    license_plate += ''.join(random.choice(options) for _ in range(repeat))\n            else:\n                license_plate += char  # Unmatched [, treat as literal\n\n        # Handle literal characters (e.g., space, dash)\n        else:\n            license_plate += char\n            i += 1\n\n    return license_plate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.869793Z","iopub.execute_input":"2025-04-03T15:29:13.870133Z","iopub.status.idle":"2025-04-03T15:29:13.886513Z","shell.execute_reply.started":"2025-04-03T15:29:13.870074Z","shell.execute_reply":"2025-04-03T15:29:13.885616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_random_license_plate(formats):\n    # Choose a random country\n    country = random.choice(list(formats.keys()))\n    country_formats = formats[country]\n\n    # If the format is a list (multiple options), select by weight\n    if isinstance(country_formats, list):\n        weights = [fmt[\"weight\"] for fmt in country_formats]\n        chosen_format = random.choices(country_formats, weights=weights, k=1)[0]\n        regex_format = chosen_format[\"regex\"]\n    else:\n        regex_format = country_formats[\"regex\"]\n\n    # Generate license plate according to selected regex\n    license_plate = generate_random_license_plate_from_regex(regex_format)\n    return license_plate, country","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.887307Z","iopub.execute_input":"2025-04-03T15:29:13.887604Z","iopub.status.idle":"2025-04-03T15:29:13.903070Z","shell.execute_reply.started":"2025-04-03T15:29:13.887583Z","shell.execute_reply":"2025-04-03T15:29:13.902261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test code\nfor _ in range(10):\n    license_plate, country = generate_random_license_plate(formats)\n    print(f\"Country: {country}, License Plate: {license_plate}\")\n    main_logger.debug(f'Testing generation of Licence Plate text\\n: Country: {country}, License Plate: {license_plate}\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.904026Z","iopub.execute_input":"2025-04-03T15:29:13.904340Z","iopub.status.idle":"2025-04-03T15:29:13.923728Z","shell.execute_reply.started":"2025-04-03T15:29:13.904307Z","shell.execute_reply":"2025-04-03T15:29:13.922912Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Functions for adding noise and distortions</font>**","metadata":{}},{"cell_type":"code","source":"def add_noise_and_distortion(img):\n    \"\"\"\n    Adds random noise and distortion effects to an input image.\n\n    This function takes an image as input and applies random Gaussian noise, rotation, and perspective distortion to simulate real-world imperfections. \n    The modifications are applied randomly to create variability in the output.\n\n    Args:\n        img (PIL.Image.Image): The input image to which noise and distortion will be applied.\n\n    Returns:\n        PIL.Image.Image: The modified image with added noise and distortion effects.\n\n    The function uses NumPy for array manipulations and OpenCV for image transformations.\n    \"\"\"\n    img_array = np.array(img)\n\n    # Add Gaussian noise\n    if random.random() > 0.5:\n        noise = np.random.normal(0, random.randint(10, 25), img_array.shape).astype(np.uint8)\n        img_array = cv2.add(img_array, noise)\n\n    # Apply rotation\n    angle = random.uniform(-5, 5)\n    rows, cols = img_array.shape[:2]\n    M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)\n    img_array = cv2.warpAffine(img_array, M, (cols, rows))\n\n    # Apply perspective distortion\n    src_points = np.float32([[0, 0], [cols-1, 0], [0, rows-1], [cols-1, rows-1]])\n    dst_points = np.float32([\n        [random.uniform(0, 5), random.uniform(0, 5)],\n        [cols-1-random.uniform(0, 5), random.uniform(0, 5)],\n        [random.uniform(0, 5), rows-1-random.uniform(0, 5)],\n        [cols-1-random.uniform(0, 5), rows-1-random.uniform(0, 5)]\n    ])\n    matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n    img_array = cv2.warpPerspective(img_array, matrix, (cols, rows))\n\n    return Image.fromarray(img_array)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.924469Z","iopub.execute_input":"2025-04-03T15:29:13.924747Z","iopub.status.idle":"2025-04-03T15:29:13.933004Z","shell.execute_reply.started":"2025-04-03T15:29:13.924722Z","shell.execute_reply":"2025-04-03T15:29:13.932186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Improved image generation feature</font>**","metadata":{}},{"cell_type":"code","source":"def generate_synthetic_image(text, font_path, img_size=(IMG_WIDTH, IMG_HEIGHT)):\n    \"\"\"\n    Generates a synthetic image with specified text and font, optionally using a background image.\n\n    This function creates an image with a given text rendered using a specified font. \n    The image can have a background selected from available background files or a generated gradient background. \n    The text is positioned randomly within the image, and the function ensures that the text fits within the image dimensions by adjusting the font size or shortening the text if necessary. \n    The image is then modified with noise and distortion effects to simulate real-world conditions.\n\n    Args:\n        text (str): The text to be rendered on the image.\n        font_path (str): The file path to the TrueType font to be used for rendering the text.\n        img_size (tuple, optional): The dimensions (width, height) of the generated image. Defaults to (IMG_WIDTH, IMG_HEIGHT).\n\n    Returns:\n        PIL.Image.Image: The generated synthetic image with the specified text and effects.\n\n    The function uses the Python Imaging Library (PIL) for image creation and manipulation, and it relies on the `add_noise_and_distortion` function to apply additional effects to the image.\n    \"\"\"\n    # Background\n    if background_files:\n        bg_path = random.choice(background_files)\n        img = Image.open(bg_path).convert('L').resize(img_size)\n    else:\n        img = Image.new('L', img_size, color=230)\n        draw = ImageDraw.Draw(img)\n        for y in range(img_size[1]):\n            color = int(230 - 20 * (y / img_size[1]))\n            draw.line([(0, y), (img_size[0], y)], fill=color)\n        img = img.filter(ImageFilter.GaussianBlur(radius=2))\n\n    draw = ImageDraw.Draw(img)\n\n    # Iterative font and text editing\n    font_size = random.randint(20, min(IMG_HEIGHT-2, 28))\n    max_attempts = 5  # Limiting the number of attempts\n    for attempt in range(max_attempts):\n        font = ImageFont.truetype(font_path, font_size)\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width, text_height = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n\n        if text_width <= IMG_WIDTH - 10:  # Text will fit\n            break\n        elif len(text) > 1:  # Shorten the text if it is too long.\n            text = text[:len(text)//2]\n        else:  # Reduce font size\n            font_size = max(10, font_size - 5)  # Minimum size 10\n\n    # If that doesn't work, use a minimal font and single-letter text.\n    if text_width > IMG_WIDTH - 10:\n        text = text[0]  # Use the first letter\n        font_size = 10\n        font = ImageFont.truetype(font_path, font_size)\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width, text_height = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n\n    # Text position\n    x = random.randint(5, max(5, IMG_WIDTH - text_width - 5))\n    y = random.randint(5, max(5, IMG_HEIGHT - text_height - 5))\n\n    # Highlighting text\n    text_color = random.randint(0, 50)\n    outline_color = 200\n    for offset_x in [-1, 0, 1]:\n        for offset_y in [-1, 0, 1]:\n            if offset_x != 0 or offset_y != 0:\n                draw.text((x + offset_x, y + offset_y), text, font=font, fill=outline_color)\n    draw.text((x, y), text, font=font, fill=text_color)\n\n    # Noise and distortion\n    img = add_noise_and_distortion(img)\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.933840Z","iopub.execute_input":"2025-04-03T15:29:13.934081Z","iopub.status.idle":"2025-04-03T15:29:13.948414Z","shell.execute_reply.started":"2025-04-03T15:29:13.934063Z","shell.execute_reply":"2025-04-03T15:29:13.947634Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Function for splitting labels</font>**","metadata":{}},{"cell_type":"code","source":"def split_labels(labels, label_lengths):\n    \"\"\"\n    Splits a sequence of labels into sub-sequences based on specified lengths.\n    \n    This function takes a single concatenated sequence of labels and splits it into multiple sub-sequences. \n    The lengths of these sub-sequences are specified by the `label_lengths` parameter. \n    Each sub-sequence is extracted from the `labels` sequence and appended to a list, which is then returned.\n    \n    Args:\n        labels (str or list): The concatenated sequence of labels to be split. This can be a string or a list of characters/elements.\n        label_lengths (list of int): A list of integers where each integer specifies the length of a corresponding sub-sequence to be extracted from `labels`.\n    \n    Returns:\n        list: A list of sub-sequences extracted from `labels` according to the specified lengths in `label_lengths`.\n    \n    Example:\n        >>> split_labels(\"abcdefgh\", [2, 3, 3])\n        ['ab', 'cde', 'fgh']\n    \"\"\"\n    split_labels = []\n    start = 0\n    for length in label_lengths:\n        split_labels.append(labels[start:start + length])\n        start += length\n    return split_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.949185Z","iopub.execute_input":"2025-04-03T15:29:13.949486Z","iopub.status.idle":"2025-04-03T15:29:13.964765Z","shell.execute_reply.started":"2025-04-03T15:29:13.949458Z","shell.execute_reply":"2025-04-03T15:29:13.964036Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Function of Beam search decoding</font>**","metadata":{}},{"cell_type":"code","source":"def beam_search_decode(output, idx_to_char, target_lengths=None, beam_width=20, blank_penalty=-1.0, length_penalty=-0.5, global_step=None):\n    \"\"\"\n    Decodes the output of a sequence model using beam search to find the most likely sequence of characters.\n\n    This function performs beam search decoding on the output probabilities of a sequence model, such as a Connectionist Temporal Classification (CTC) model. It explores multiple possible sequences (beams) at each time step to find the sequence with the highest probability, applying penalties for blank tokens and sequence length. The decoded sequences are then converted to strings using a character mapping.\n\n    Args:\n        output (torch.Tensor): The output tensor from the model, typically of shape (T, B, C), where T is the time steps, B is the batch size, and C is the number of classes (characters).\n        idx_to_char (dict): A dictionary mapping index values to their corresponding characters.\n        target_lengths (list of int, optional): A list specifying the target lengths for each sequence in the batch. If provided, it is used to determine the maximum length for decoding. Defaults to None.\n        beam_width (int, optional): The number of beams (hypotheses) to consider at each time step during decoding. Defaults to 20.\n        blank_penalty (float, optional): A penalty applied to the log probability of blank tokens to discourage their selection. Defaults to -1.0.\n        length_penalty (float, optional): A penalty applied to the log probability based on the length of the sequence to discourage overly long sequences. Defaults to -0.5.\n        global_step (int, optional): The global step or iteration number, used for logging purposes. Defaults to None.\n\n    Returns:\n        list of str: A list of decoded strings, one for each sequence in the batch.\n\n    The function uses the beam search algorithm to efficiently explore multiple hypotheses and select the most probable sequence. It logs token distributions and prediction lengths for monitoring and debugging purposes.\n    \"\"\"\n    probs = output.softmax(2).cpu().numpy()\n    T, B, C = probs.shape\n    predictions = []\n\n    for b in range(B):\n        sequence_probs = [(0.0, [], 1.0)]  # (log_prob, sequence, probability)\n        max_length = target_lengths[b].item() * 2 if target_lengths is not None else min(T, 16)\n        for t in range(T):\n            new_sequences = []\n            for log_prob, seq, prob in sequence_probs:\n                if len(seq) >= max_length:\n                    new_sequences.append((log_prob, seq, prob))\n                    continue\n                top_k_probs, top_k_idx = torch.topk(torch.tensor(probs[t, b]), beam_width)\n                for k_prob, k_idx in zip(top_k_probs, top_k_idx):\n                    new_seq = seq + [k_idx.item()] if k_idx.item() != 0 else seq  # We only add non-blank tokens\n                    new_prob = prob * k_prob.item()\n                    new_log_prob = log_prob + np.log(k_prob.item() + 1e-10)  # Prevention logs(0)\n                    if k_idx.item() == 0:\n                        new_log_prob += blank_penalty\n                    new_log_prob += length_penalty * len(new_seq)\n                    new_sequences.append((new_log_prob, new_seq, new_prob))\n            sequence_probs = sorted(new_sequences, key=lambda x: x[0], reverse=True)[:beam_width]\n\n        best_seq = sequence_probs[0][1]\n        if all(token == 0 for token in best_seq):  # If all tokens are blank\n            decoded = \"\"\n        else:\n            decoded = []\n            prev = -1\n            for idx in best_seq:\n                if idx != 0 and idx != prev:\n                    decoded.append(idx_to_char.get(idx, ''))\n                prev = idx\n            decoded = ''.join(decoded)\n        predictions.append(decoded if decoded else '<empty>')\n\n        token_counts = Counter(best_seq)\n        pred_length = len(best_seq)\n        for token, count in token_counts.items():\n            writer.add_scalar(f'Token_Distribution/token_{token}', count, global_step)\n        writer.add_scalar('Prediction_Length/mean_length', pred_length, global_step)\n        #print(f\"Token distribution (Batch {b}): {dict(token_counts)}, Pred length: {pred_length}\")\n        function_logger.debug(f\"Token distribution (Batch {b}): {dict(token_counts)}, Pred length: {pred_length}\")\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.965654Z","iopub.execute_input":"2025-04-03T15:29:13.965915Z","iopub.status.idle":"2025-04-03T15:29:13.983428Z","shell.execute_reply.started":"2025-04-03T15:29:13.965896Z","shell.execute_reply":"2025-04-03T15:29:13.982707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Custom collate function</font>**","metadata":{}},{"cell_type":"code","source":"def custom_collate_fn(batch):\n    \"\"\"\n    Custom collate function for combining a batch of data samples into a single batch.\n\n    This function is used to process a batch of data samples, typically for use in a data loader during model training or evaluation. It stacks images, concatenates labels, and converts label lengths into tensors, preparing the data for efficient processing in a neural network.\n\n    Args:\n        batch (list of tuples): A list where each element is a tuple containing:\n            - An image tensor.\n            - A label tensor.\n            - An integer representing the length of the label.\n\n    Returns:\n        tuple: A tuple containing:\n            - images (torch.Tensor): A tensor of stacked images from the batch.\n            - labels (torch.Tensor): A concatenated tensor of labels from the batch.\n            - label_lengths (torch.Tensor): A tensor of label lengths from the batch.\n\n    The function ensures that the images, labels, and label lengths are properly formatted as tensors, making them compatible with PyTorch operations.\n    \"\"\"\n    images, labels, label_lengths = zip(*batch)\n    # Stack images (all same size)\n    images = torch.stack(images, dim=0)\n    # Concatenate labels into a flat tensor\n    labels = torch.cat(labels, dim=0)\n    # Convert label_lengths to tensor\n    label_lengths = torch.tensor(label_lengths, dtype=torch.long)\n    return images, labels, label_lengths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:13.984247Z","iopub.execute_input":"2025-04-03T15:29:13.984468Z","iopub.status.idle":"2025-04-03T15:29:14.001198Z","shell.execute_reply.started":"2025-04-03T15:29:13.984440Z","shell.execute_reply":"2025-04-03T15:29:14.000494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Analyze dataset char frequency</font>**","metadata":{}},{"cell_type":"code","source":"def analyze_dataset_char_frequency(labels_file, charset):\n    \"\"\"\n    Analyzes the character frequency in the generated dataset and displays a progress bar.\n    \n    Args:\n        labels_file (str): Path to the labels file (e.g., LABELS_FILE).\n        charset (str): String containing all possible characters (e.g., CHARSET).\n    \n    Returns:\n        dict: Dictionary with character frequencies.\n    \"\"\"\n    if not os.path.exists(labels_file):\n        print(f\"Error: Labels file '{labels_file}' does not exist!\")\n        function_logger.error(f\"Error: Labels file '{labels_file}' does not exist!\")\n        return {}\n\n    # Read labels and extract texts\n    with open(labels_file, 'r') as f:\n        labels = [line.split('\\t')[1].strip() for line in f if '\\t' in line]\n\n    if not labels:\n        print(\"Error: No valid labels found in the file!\")\n        return {}\n\n    # Count character frequencies\n    all_chars = ''.join(labels)\n    char_counts = Counter(all_chars)\n\n    # Prepare data for the bar plot\n    chars = list(charset)\n    frequencies = [char_counts.get(char, 0) for char in chars]\n\n    # Create a progress bar (bar plot)\n    plt.figure(figsize=(12, 6))\n    plt.bar(chars, frequencies, color='skyblue')\n    plt.xlabel('Characters')\n    plt.ylabel('Frequency')\n    plt.title('Character Frequency in Dataset')\n    plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n    plt.tight_layout()\n\n    # Display the plot\n    plt.show()\n\n    # Print summary\n    total_chars = sum(frequencies)\n    print(f\"Total characters analyzed: {total_chars}\")\n    print(\"Character frequencies:\", dict(char_counts))\n    function_logger.debug(f\"Total characters analyzed: {total_chars}\")\n    function_logger.debug(f\"Character frequencies: {dict(char_counts)}\")\n\n    return dict(char_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.002040Z","iopub.execute_input":"2025-04-03T15:29:14.002371Z","iopub.status.idle":"2025-04-03T15:29:14.015331Z","shell.execute_reply.started":"2025-04-03T15:29:14.002324Z","shell.execute_reply":"2025-04-03T15:29:14.014582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Generování datasetu</font>**","metadata":{}},{"cell_type":"code","source":"def create_synthetic_dataset(num_samples):\n    \"\"\"\n    Creates a synthetic dataset with images and corresponding labels.\n    \n    Args:\n        num_samples (int): Number of images to generate.\n    \n    Notes:\n        It uses global variables: OUTPUT_DIR, LABELS_FILE, font_files, MAX_TEXT_LENGTH, MIN_TEXT_LENGTH.\n        It assumes the existence of the generate_synthetic_image functions and access to font_files.\n    \"\"\"\n    labels = []\n    for i in range(num_samples):\n        # Generating text of License plate\n        text, country = generate_random_license_plate(formats)\n        if not text:\n            continue  # Skip if text is empty (which should not happen with min_length=3)\n        \n        # Random font selection\n        font_path = random.choice(font_files)\n        \n        # Image generation\n        img = generate_synthetic_image(text, font_path)\n        img_name = f\"img_{i:05d}.png\"\n        img_path = os.path.join(OUTPUT_DIR, img_name)\n        img.save(img_path)\n        \n        # Saving a label\n        labels.append(f\"{img_name}\\t{text}\")  # Using the tab as a separator\n        \n        # Progress report\n        if i % 100 == 0:\n            print(f\"Generated {i}/{num_samples} images (Country: {country}, Text: {text})\")\n            function_logger.debug(f\"Generated {i}/{num_samples} images (Country: {country}, Text: {text})\")\n\n    # Saving labels to a file\n    if labels:\n        with open(LABELS_FILE, 'w') as f:\n            f.write(\"\\n\".join(labels))\n        print(f\"Dataset generated! Images saved in '{OUTPUT_DIR}', labels in '{LABELS_FILE}'\")\n        function_logger.debug(f\"Dataset generated! Images saved in '{OUTPUT_DIR}', labels in '{LABELS_FILE}'\")\n        \n        # Analyze and display character frequency\n        analyze_dataset_char_frequency(LABELS_FILE, CHARSET)\n    else:\n        print(\"No labels generated!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.016191Z","iopub.execute_input":"2025-04-03T15:29:14.016441Z","iopub.status.idle":"2025-04-03T15:29:14.032904Z","shell.execute_reply.started":"2025-04-03T15:29:14.016422Z","shell.execute_reply":"2025-04-03T15:29:14.032052Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Mapping characters to indices and back</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"char_to_idx = {char: idx + 1 for idx, char in enumerate(CHARSET)}  # 0 is reserved for blank (CTC)\nidx_to_char = {idx: char for char, idx in char_to_idx.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.033778Z","iopub.execute_input":"2025-04-03T15:29:14.033995Z","iopub.status.idle":"2025-04-03T15:29:14.047019Z","shell.execute_reply.started":"2025-04-03T15:29:14.033977Z","shell.execute_reply":"2025-04-03T15:29:14.046389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main_logger.info('Mapping characters to indices and back was done.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.047755Z","iopub.execute_input":"2025-04-03T15:29:14.047960Z","iopub.status.idle":"2025-04-03T15:29:14.058914Z","shell.execute_reply.started":"2025-04-03T15:29:14.047942Z","shell.execute_reply":"2025-04-03T15:29:14.058161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Test regex generation</font>**\n-------------------","metadata":{}},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Custom dataset</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"class OCRDataset(Dataset):\n    \"\"\"\n    A custom dataset class for Optical Character Recognition (OCR) tasks.\n\n    This class extends PyTorch's Dataset class and is used to load and preprocess images and their corresponding labels for OCR tasks. It reads image paths and labels from a file and provides methods to access the data.\n\n    Args:\n        Dataset (torch.utils.data.Dataset): The base dataset class from PyTorch.\n    \"\"\"\n    def __init__(self, image_dir, labels_file):\n        \"\"\"\n        Initializes the OCRDataset with the directory containing images and the file containing labels.\n\n        Args:\n            image_dir (str): The directory path where the images are stored.\n            labels_file (str): The file path containing the labels corresponding to the images. The file should have each line formatted as 'image_path\\tlabel'.\n        \"\"\"\n        self.image_dir = image_dir\n        self.labels_file = labels_file\n        self.data = []\n        with open(labels_file, 'r') as f:\n            for line in f:\n                if not line.strip():  # Skip empty lines\n                    continue\n                image_path, label = line.strip().split('\\t')\n                label_length = len(label)\n                self.data.append((image_path, label, label_length))\n\n    def __len__(self):\n        \"\"\"_summary_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image_path, label, label_length = self.data[idx]\n        image = Image.open(os.path.join(self.image_dir, image_path)).convert('L')\n        image = transforms.ToTensor()(image)\n        label_encoded = torch.tensor([char_to_idx[c] for c in label], dtype=torch.long)\n        return image, label_encoded, label_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.059841Z","iopub.execute_input":"2025-04-03T15:29:14.060168Z","iopub.status.idle":"2025-04-03T15:29:14.076542Z","shell.execute_reply.started":"2025-04-03T15:29:14.060138Z","shell.execute_reply":"2025-04-03T15:29:14.075791Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Custom CTC Loss with Blank Penalty</font>**\n-------------------\n> CTC Loss with Entropy Regularization","metadata":{}},{"cell_type":"code","source":"class CTCLossWithBlankPenalty(nn.Module):\n    \"\"\"\n    A custom loss module that extends the Connectionist Temporal Classification (CTC) loss with additional penalties.\n\n    This class adds penalties for blank predictions and entropy to the standard CTC loss, which is commonly used in sequence-to-sequence tasks like speech recognition and OCR. \n    The penalties help to regularize the model and improve convergence during training.\n\n    Args:\n        nn (torch.nn.Module): The base module class from PyTorch.\n    \"\"\"\n\n    def __init__(self, blank=0, zero_infinity=True, blank_penalty_weight=1.0, entropy_weight=0.5, label_smoothing=0.1):\n        \"\"\"\n        Initializes the CTCLossWithBlankPenalty with specified parameters.\n\n        Args:\n            blank (int, optional): The index of the blank label. Defaults to 0.\n            zero_infinity (bool, optional): Whether to zero the loss of sequences with infinite loss. Defaults to True.\n            blank_penalty_weight (float, optional): The weight of the blank penalty. Defaults to 1.0.\n            entropy_weight (float, optional): The weight of the entropy penalty. Defaults to 0.5.\n            label_smoothing (float, optional): The amount of label smoothing to apply. Defaults to 0.1.\n        \"\"\"\n        super().__init__()\n        self.ctc_loss = nn.CTCLoss(blank=blank, reduction='mean', zero_infinity=zero_infinity)\n        self.blank = blank  # Add blank index as an attribute\n        self.blank_penalty_weight = blank_penalty_weight\n        self.entropy_weight = entropy_weight  \n        self.label_smoothing = label_smoothing\n        self.global_step = 0\n\n    def update_blank_penalty(self, global_step):\n        \"\"\"\n        Updates the blank penalty weight based on the current global step.\n\n        This method gradually reduces the blank penalty weight over the first 1,000 steps to stabilize training.\n\n        Args:\n            global_step (int): The current global step or iteration number.\n        \"\"\"\n        # Gradual reduction of penalty over the first 1,000 steps\n        self.global_step = global_step\n        decay_factor = max(0.1, 1.0 - self.global_step / 1000.0)\n        self.blank_penalty_weight = 1.0 * decay_factor\n\n    def forward(self, log_probs, targets, input_lengths, target_lengths):\n        \"\"\"\n        Computes the total loss, including CTC loss, blank penalty, and entropy penalty.\n\n        Args:\n            log_probs (torch.Tensor): The log probabilities output by the model, of shape (T, N, C), \n                                      where T is the input sequence length, N is the batch size, and C is the number of classes.\n            targets (torch.Tensor): The target sequences, of shape (N, S), where S is the target sequence length.\n            input_lengths (torch.Tensor): The lengths of the input sequences, of shape (N,).\n            target_lengths (torch.Tensor): The lengths of the target sequences, of shape (N,).\n\n        Returns:\n            torch.Tensor: The total loss, which is the sum of the CTC loss, blank penalty, and entropy penalty.\n        \"\"\"\n        # Standard CTC loss\n        ctc_loss = self.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n\n        # Blank penalty: penalize high blank probability (updated to be proportional)\n        blank_probs = log_probs[:, :, self.blank].exp().mean()\n        blank_penalty = self.blank_penalty_weight * blank_probs  # Changed from -torch.log(1 - blank_probs + 1e-4) * self.blank_penalty_weight\n\n        # Entropy regularization\n        probs = log_probs.exp()\n        entropy = -torch.sum(probs * log_probs, dim=-1).mean()\n        entropy_loss = -self.entropy_weight * entropy\n\n        # Total loss\n        total_loss = ctc_loss + blank_penalty + entropy_loss\n        return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.077295Z","iopub.execute_input":"2025-04-03T15:29:14.077560Z","iopub.status.idle":"2025-04-03T15:29:14.085856Z","shell.execute_reply.started":"2025-04-03T15:29:14.077538Z","shell.execute_reply":"2025-04-03T15:29:14.085251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Model definition (CNN + RNN + CTC)</font>**\n-------------------","metadata":{}},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Configuration file</font>**","metadata":{}},{"cell_type":"code","source":"num_chars = len(CHARSET)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.086682Z","iopub.execute_input":"2025-04-03T15:29:14.086920Z","iopub.status.idle":"2025-04-03T15:29:14.100440Z","shell.execute_reply.started":"2025-04-03T15:29:14.086892Z","shell.execute_reply":"2025-04-03T15:29:14.099645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config_model = {\n    'cnn_layers': [\n        {'type': 'conv', 'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1, 'batchnorm': True, 'activation': 'relu', 'dropout': 0.2},\n        {'type': 'pool', 'kernel_size': 2, 'stride': 2},\n        {'type': 'conv', 'out_channels': 128, 'kernel_size': 3, 'stride': 1, 'padding': 1, 'batchnorm': True, 'activation': 'relu', 'dropout': 0.2},\n        {'type': 'pool', 'kernel_size': 2, 'stride': 2},\n        {'type': 'conv', 'out_channels': 256, 'kernel_size': 3, 'stride': 1, 'padding': 1, 'batchnorm': True, 'activation': 'relu'},\n        {'type': 'pool', 'kernel_size': (2, 2), 'stride': (2, 1), 'padding': (0, 1)},  # CRNN pooling\n        {'type': 'conv', 'out_channels': 512, 'kernel_size': 3, 'stride': 1, 'padding': 1, 'batchnorm': True, 'activation': 'relu'},\n        {'type': 'pool', 'kernel_size': (2, 2), 'stride': (2, 1), 'padding': (0, 1)},  # CRNN pooling\n    ],\n    'rnn_type': 'lstm',\n    'rnn_layers': 2,\n    'hidden_size': 256,\n    'bidirectional': True,\n    'dropout': 0.3,\n    'fc_layers': [num_chars],  # One layer for OCR\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.101192Z","iopub.execute_input":"2025-04-03T15:29:14.101414Z","iopub.status.idle":"2025-04-03T15:29:14.114323Z","shell.execute_reply.started":"2025-04-03T15:29:14.101386Z","shell.execute_reply":"2025-04-03T15:29:14.113662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Model architecture</font>**","metadata":{}},{"cell_type":"code","source":"class OCRModel(nn.Module):\n    \"\"\"\n    A neural network model for Optical Character Recognition (OCR) tasks.\n    \n    This class defines an OCR model that combines Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) to process and recognize text in images. \n    The model architecture is configurable through a configuration dictionary.\n    \n    Args:\n        nn (torch.nn.Module): The base module class from PyTorch.\n    \"\"\"\n    def __init__(self, config):\n        \"\"\"\n        Initializes the OCRModel with a given configuration.\n\n        Args:\n            config (dict): A dictionary containing the configuration parameters for the model, including CNN layers, RNN type, hidden size, and more.\n        \"\"\"\n        super(OCRModel, self).__init__()\n        self.cnn = self.build_cnn(config['cnn_layers'])\n        \n        # Calculating input size for RNN after CNN\n        num_pools = sum(1 for layer in config['cnn_layers'] if layer['type'] == 'pool')\n        height_after_cnn = IMG_HEIGHT // (2 ** num_pools)\n        last_conv_out = [layer['out_channels'] for layer in config['cnn_layers'] if layer['type'] == 'conv'][-1]\n        rnn_input_size = last_conv_out * height_after_cnn\n        \n        self.rnn = self.build_rnn(config, rnn_input_size)\n        rnn_output_size = config['hidden_size'] * 2 if config['bidirectional'] else config['hidden_size']\n        self.fc = self.build_fc(config, rnn_output_size)\n\n    def build_cnn(self, cnn_config):\n        \"\"\"\n        Builds the CNN part of the model based on the provided configuration.\n\n        Args:\n            cnn_config (list of dict): A list of dictionaries, each specifying a layer in the CNN. Each dictionary contains parameters like type, out_channels, kernel_size, etc.\n\n        Returns:\n            nn.Sequential: A sequential container of CNN layers.\n        \"\"\"\n        layers = []\n        in_channels = 1\n        for layer in cnn_config:\n            if layer['type'] == 'conv':\n                layers.append(nn.Conv2d(in_channels, layer['out_channels'], layer['kernel_size'], \n                                        layer['stride'], layer['padding']))\n                if layer.get('batchnorm', False):\n                    layers.append(nn.BatchNorm2d(layer['out_channels']))\n                if layer.get('activation') == 'relu':\n                    layers.append(nn.ReLU())\n                elif layer.get('activation') == 'leaky_relu':\n                    layers.append(nn.LeakyReLU(0.2))\n                if 'dropout' in layer:\n                    layers.append(nn.Dropout2d(layer['dropout']))\n                in_channels = layer['out_channels']\n            elif layer['type'] == 'pool':\n                layers.append(nn.MaxPool2d(layer['kernel_size'], layer['stride'], layer.get('padding', 0)))\n        return nn.Sequential(*layers)\n\n    def build_rnn(self, config, input_size):\n        \"\"\"\n        Creates an RNN part (LSTM or GRU) according to the configuration.\n\n        Args:\n            config (dict): The configuration dictionary containing RNN parameters.\n            input_size (int): The input size for the RNN.\n\n        Raises:\n            ValueError: If the RNN type specified in the configuration is unknown.\n\n        Returns:\n            nn.Module: An RNN module (LSTM or GRU).\n        \"\"\"\n        if config['rnn_type'] == 'lstm':\n            return nn.LSTM(input_size, config['hidden_size'], num_layers=config['rnn_layers'], \n                          bidirectional=config['bidirectional'], dropout=config['dropout'])\n        elif config['rnn_type'] == 'gru':\n            return nn.GRU(input_size, config['hidden_size'], num_layers=config['rnn_layers'], \n                         bidirectional=config['bidirectional'], dropout=config['dropout'])\n        else:\n            raise ValueError(\"Unknown RNN type: {}\".format(config['rnn_type']))\n\n    def build_fc(self, config, input_size):\n        \"\"\"\n        Creates output fully connected layers.\n\n        Args:\n            config (dict): The configuration dictionary containing fully connected layer parameters.\n            input_size (int): The input size for the fully connected layers.\n\n        Returns:\n            nn.Sequential: A sequential container of fully connected layers.\n        \"\"\"\n        layers = []\n        fc_layers = config['fc_layers']\n        for out_features in fc_layers[:-1]:\n            layers.append(nn.Linear(input_size, out_features))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(config['dropout']))\n            input_size = out_features\n        layers.append(nn.Linear(input_size, fc_layers[-1]))\n        return nn.Sequential(*layers)\n\n    def forward(self, x, max_length=None):\n        \"\"\"\n        Performs a forward pass through the OCR model.\n\n        Args:\n            x (torch.Tensor): The input tensor, typically an image.\n            max_length (int, optional): The maximum length for the RNN output. Defaults to None.\n\n        Returns:\n            torch.Tensor: The output tensor after passing through the CNN, RNN, and fully connected layers.\n        \"\"\"\n        x = self.cnn(x)\n        batch, channels, height, width = x.size()\n        x = x.view(batch, channels * height, width).permute(2, 0, 1)  # (width, batch, features)\n        if max_length is not None:\n            x = x[:max_length]\n        x, _ = self.rnn(x)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.114958Z","iopub.execute_input":"2025-04-03T15:29:14.115196Z","iopub.status.idle":"2025-04-03T15:29:14.132141Z","shell.execute_reply.started":"2025-04-03T15:29:14.115178Z","shell.execute_reply":"2025-04-03T15:29:14.131363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Training</font>**\n-------------------","metadata":{}},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Initializing the scales</font>**","metadata":{}},{"cell_type":"code","source":"def initialize_weights(model):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.LSTM) or isinstance(module, nn.GRU):\n            for param_name, param in module.named_parameters():\n                if 'weight' in param_name:\n                    nn.init.orthogonal_(param)\n                elif 'bias' in param_name:\n                    nn.init.zeros_(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.132858Z","iopub.execute_input":"2025-04-03T15:29:14.133082Z","iopub.status.idle":"2025-04-03T15:29:14.147625Z","shell.execute_reply.started":"2025-04-03T15:29:14.133063Z","shell.execute_reply":"2025-04-03T15:29:14.146823Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Calculating the gradient norm before trimming (for logging)</font>**","metadata":{}},{"cell_type":"code","source":"def compute_gradient_norm(parameters: Iterator[torch.Tensor]) -> float:\n    total_norm = 0.0\n    for p in parameters:\n        if p.grad is not None:\n            # Compute the Frobenius norm (default) over all elements in the tensor\n            param_norm = torch.linalg.norm(p.grad.detach(), ord=None)\n            total_norm += param_norm.item() ** 2\n    total_norm = total_norm ** 0.5\n    return total_norm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.148280Z","iopub.execute_input":"2025-04-03T15:29:14.148510Z","iopub.status.idle":"2025-04-03T15:29:14.161313Z","shell.execute_reply.started":"2025-04-03T15:29:14.148492Z","shell.execute_reply":"2025-04-03T15:29:14.160521Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Function of train model</font>**","metadata":{}},{"cell_type":"code","source":"def train_model(model, full_dataset, device, config_model, num_epochs=EPOCHS):\n    # Initializing the scales\n    initialize_weights(model)\n\n    # Using the AdamW optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n    # Learning rate warm-up scheduler\n    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(1.0, step / WARMUP_STEPS))\n    # Scheduler for learning rate decrease based on validation loss\n    plateau_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n\n    # Initializing the TensorBoard writer\n    log_dir = os.path.join(TENSORBOARD_DIR, \"ocr_experiment_fixed\")\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    writer = SummaryWriter(log_dir)\n\n    best_val_loss = float('inf')\n    global_step = 0\n\n    for epoch in range(num_epochs):\n        # Curriculum learning: Filtering data by label length\n        label_lengths = [lbl_len for _, _, lbl_len in full_dataset.data]\n        print(f\"Label lengths: {label_lengths}\")\n        if epoch < 5:\n            filtered_data = [(img, lbl, lbl_len) for img, lbl, lbl_len in full_dataset.data if lbl_len <= 10]\n            sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]\n            print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n        elif epoch < 10:\n            filtered_data = [(img, lbl, lbl_len) for img, lbl, lbl_len in full_dataset.data if lbl_len <= 15]\n            sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]\n            print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n        else:\n            filtered_data = full_dataset.data\n            sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]\n            print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n\n        # Creating a new dataset with filtered data\n        curr_dataset = OCRDataset(image_dir=OUTPUT_DIR, labels_file=LABELS_FILE)\n        curr_dataset.data = filtered_data\n\n        # Splitting into training and validation datasets\n        train_size = int(0.8 * len(curr_dataset))\n        val_size = len(curr_dataset) - train_size\n        train_dataset, val_dataset = torch.utils.data.random_split(curr_dataset, [train_size, val_size])\n        print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n\n        # Creating dataloaders\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n\n        # Training phase\n        model.train()\n        total_loss = 0\n        total_grad_norm = 0\n        total_blank_probs = 0\n        skipped_batches = 0\n\n        for batch_idx, (imgs, labels, label_lengths) in enumerate(train_loader):\n            imgs, labels = imgs.to(device), labels.to(device)\n            label_lengths = label_lengths.to(device)\n\n            # Learning rate warm-up\n            if global_step < WARMUP_STEPS:\n                lr_scale = min(1.0, float(global_step + 1) / WARMUP_STEPS)\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = LEARNING_RATE * lr_scale\n\n            optimizer.zero_grad()\n            max_label_length = label_lengths.max().item()\n            outputs = model(imgs, max_length=max_label_length * 2)\n            outputs = outputs / TEMPERATURE\n            outputs = torch.clamp(outputs, min=-10, max=10)  # Clipping pro stabilitu\n            outputs = outputs.log_softmax(2)\n\n            batch_size = imgs.size(0)\n            seq_length = min(outputs.size(0), max_label_length * 2)\n            input_lengths = torch.full((batch_size,), seq_length, dtype=torch.long).to(device)\n\n            # Dynamic penalty for blank tokens\n            blank_probs = outputs[:, :, 0].exp().mean().item()\n            dynamic_blank_penalty = CTC_BLANK_PENALTY_WEIGHT * (1 + blank_probs)\n            criterion = CTCLossWithBlankPenalty(\n                blank=0, zero_infinity=True, blank_penalty_weight=dynamic_blank_penalty,\n                entropy_weight=CTC_ENTROPY_WEIGHT, label_smoothing=CTC_LABEL_SMOOTHING\n            )\n\n            loss = criterion(outputs, labels, input_lengths, label_lengths)\n            if torch.isnan(loss) or torch.isinf(loss):\n                print(f\"Warning: NaN or Inf loss at batch {batch_idx}. Skipping...\")\n                skipped_batches += 1\n                continue\n\n            if loss.item() > 50:\n                print(f\"Warning: High loss {loss.item():.4f} at batch {batch_idx}. Skipping...\")\n                skipped_batches += 1\n                continue\n\n            loss.backward()\n\n            # Calculating the gradient norm before clipping\n            grad_norm_before = compute_gradient_norm(model.parameters())\n\n            # Gradient clipping with a reasonable max_norm value\n            grad_norm_after = nn_utils.clip_grad_norm_(model.parameters(), max_norm=GRADIENT_CLIPPING_VALUE)\n\n            # Logging gradient norms\n            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Gradient Norm Before: {grad_norm_before:.4f}, After: {grad_norm_after:.4f}\")\n\n            optimizer.step()\n            scheduler.step()\n\n            total_loss += loss.item()\n            total_grad_norm += grad_norm_after\n            total_blank_probs += blank_probs\n            global_step += 1\n\n            if batch_idx % 10 == 0:\n                with torch.no_grad():\n                    pred_texts = beam_search_decode(\n                        output=outputs,\n                        idx_to_char=idx_to_char,\n                        target_lengths=label_lengths,\n                        beam_width=BSD_BEAM_WIDTH,\n                        blank_penalty=BSD_BLANK_PENALTY,\n                        length_penalty=BSD_LENGTH_PENALTY\n                    )\n                    raw_outputs = outputs.argmax(2).cpu().numpy()[:3]\n                    label_sequences = split_labels(labels, label_lengths)\n                    ground_truth = [''.join([idx_to_char.get(idx.item(), '') for idx in label_seq])\n                                    for label_seq in label_sequences[:3]]\n\n                    # Logging into TensorBoard\n                    writer.add_scalar('Loss/train_batch', loss.item(), global_step)\n                    writer.add_scalar('Blank_Probability/train_batch', blank_probs, global_step)\n                    writer.add_scalar('Gradient_Norm/train_batch', grad_norm_after, global_step)\n                    writer.add_histogram('Logits/train_probs', outputs.exp().flatten(), global_step)\n\n                    # Logging raw outputs\n                    writer.add_histogram('Raw_Outputs/train_argmax', raw_outputs.flatten(), global_step)\n                    writer.add_text('Raw_Outputs/train_text', f\"Raw train outputs (argmax):\\n{str(raw_outputs)}\", global_step)\n\n                    # Token frequency logging\n                    token_counts = Counter(raw_outputs.flatten())\n                    writer.add_text(\n                        'Raw_Outputs/train_token_counts',\n                        f\"Token counts: {token_counts}\",\n                        global_step\n                    )\n\n                    print(f\"Batch {batch_idx}, Gradient norm: {grad_norm_after:.4f}\")\n                    print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n                    print(f\"Avg Blank Probability: {blank_probs:.4f}\")\n                    print(f\"Sample predictions: {pred_texts[:3]}\")\n                    print(f\"Ground Truth (first 3): {ground_truth}\")\n                    print(f\"Raw outputs (first 3): {raw_outputs}\")\n                    print(f\"Input length: {seq_length}, Label lengths: {label_lengths[:3].tolist()}\")\n\n        avg_loss = total_loss / len(train_loader)\n        avg_grad_norm = total_grad_norm / len(train_loader)\n        avg_blank_probs = total_blank_probs / len(train_loader)\n\n        writer.add_scalar('Loss/train_epoch', avg_loss, epoch)\n        writer.add_scalar('Gradient_Norm/train_epoch', avg_grad_norm, epoch)\n        writer.add_scalar('Blank_Probability/train_epoch', avg_blank_probs, epoch)\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_blank_probs = 0\n        with torch.no_grad():\n            for batch_idx, (imgs, labels, label_lengths) in enumerate(val_loader):\n                imgs, labels = imgs.to(device), labels.to(device)\n                label_lengths = label_lengths.to(device)\n                outputs = model(imgs)\n                outputs = outputs.log_softmax(2)\n                seq_length = outputs.size(0)\n                input_lengths = torch.full((imgs.size(0),), seq_length, dtype=torch.long).to(device)\n                val_loss += criterion(outputs, labels, input_lengths, label_lengths).item()\n                val_blank_probs += outputs[:, :, 0].exp().mean().item()\n\n                # Logging raw outputs for the first batch\n                if batch_idx == 0:\n                    print(f\"Outputs shape: {outputs.shape}, Outputs[0] shape: {outputs[0].shape}\")\n                    if outputs.numel() == 0 or torch.isnan(outputs).any() or torch.isinf(outputs).any():\n                        print(\"Warning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\")\n                        continue\n\n                    # Histogram of predicted tokens\n                    raw_outputs = outputs.argmax(2).cpu().numpy()\n                    writer.add_histogram(\n                        'Raw_Outputs/val_argmax',\n                        raw_outputs.flatten(),\n                        global_step=epoch\n                    )\n\n                    # Text listing of raw outputs\n                    raw_outputs_text = str(raw_outputs[:5])\n                    writer.add_text(\n                        'Raw_Outputs/val_text',\n                        f\"Raw validation outputs (argmax) for first batch:\\n{raw_outputs_text}\",\n                        global_step=epoch\n                    )\n\n                    # Token frequency\n                    token_counts = Counter(raw_outputs.flatten())\n                    writer.add_text(\n                        'Raw_Outputs/val_token_counts',\n                        f\"Token counts: {token_counts}\",\n                        global_step=epoch\n                    )\n\n                    # Logit histogram for the first time step\n                    logits_first_step = outputs[0].cpu().numpy()\n                    writer.add_histogram(\n                        'Raw_Outputs/val_logits_first_step',\n                        logits_first_step.flatten(),\n                        global_step=epoch\n                    )\n\n                    writer.add_histogram('Logits/val_probs', torch.softmax(outputs[0], dim=-1).flatten(), global_step)\n\n            val_loss /= len(val_loader)\n            val_blank_probs /= len(val_loader)\n            pred_texts = beam_search_decode(\n                output=outputs,\n                idx_to_char=idx_to_char,\n                target_lengths=label_lengths,\n                beam_width=BSD_BEAM_WIDTH,\n                blank_penalty=BSD_BLANK_PENALTY,\n                length_penalty=BSD_LENGTH_PENALTY\n            )\n            label_sequences = split_labels(labels, label_lengths)\n            ground_truth = [''.join([idx_to_char.get(idx.item(), '') for idx in label_seq])\n                            for label_seq in label_sequences[:5]]\n\n            # Calculating edit distance\n            edit_distances = []\n            for pred, gt in zip(pred_texts, ground_truth):\n                edit_distances.append(levenshtein_distance(pred, gt))\n            avg_edit_distance = sum(edit_distances) / len(edit_distances) if edit_distances else 0\n\n            # Logging validation metrics to TensorBoard\n            writer.add_scalar('Loss/val_epoch', val_loss, epoch)\n            writer.add_scalar('Blank_Probability/val_epoch', val_blank_probs, epoch)\n            writer.add_text('Predictions/val', f\"Validation Predictions: {pred_texts[:5]}\", epoch)\n            writer.add_text('Ground_Truth/val', f\"Ground Truth: {ground_truth}\", epoch)\n            print(f\"Validation Loss: {val_loss:.4f}\")\n            print(\"Validation Predictions:\", pred_texts[:5])\n            print(\"Ground Truth:\", ground_truth)\n\n        # Updating learning rate based on validation loss\n        plateau_scheduler.step(val_loss)\n\n        current_lr = scheduler.get_last_lr()[0]\n        print(f\"Current Learning Rate: {current_lr}\")\n        \n\n        # Saving the best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'best_ocr_model.pth'))\n\n    # Saving the final model\n    torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'final_ocr_model.pth'))\n\n    # Closing the TensorBoard writer\n    writer.close()\n\n    return best_val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.162274Z","iopub.execute_input":"2025-04-03T15:29:14.162565Z","iopub.status.idle":"2025-04-03T15:29:14.187934Z","shell.execute_reply.started":"2025-04-03T15:29:14.162538Z","shell.execute_reply":"2025-04-03T15:29:14.187126Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Inference (prediction)</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"def decode_prediction(output, idx_to_char):\n    \"\"\"\n    Decodes the output of an OCR model into readable text.\n\n    This function takes the raw output from an OCR model and converts it into a list of decoded text strings. \n    It applies a softmax function to the output to obtain probabilities for each character at each time step. It then uses a dynamic threshold based on the 75th percentile of these probabilities to filter out low-confidence predictions. The decoded text is constructed by mapping character indices to their corresponding characters using a provided dictionary.\n\n    Args:\n        output (torch.Tensor): The raw output tensor from the OCR model, typically of shape (T, N, C), \n        where T is the time steps, N is the batch size, and C is the number of classes (characters).\n        idx_to_char (dict): A dictionary mapping index values to their corresponding characters.\n\n    Returns:\n        list of str: A list of decoded text strings, one for each sequence in the batch. \n        If a sequence is empty after decoding, it is represented as '<empty>'.\n\n    The function prints the raw predictions and the dynamic threshold used for filtering, \n    providing insights into the decoding process.\n    \"\"\"\n    probs = output.softmax(2)\n    max_probs, preds = probs.max(dim=2)\n    preds = preds.cpu().numpy()\n    max_probs = max_probs.cpu().numpy()\n    texts = []\n    for i, (pred, prob) in enumerate(zip(preds.T, max_probs.T)):\n        #print(f\"Raw prediction {i} (pre-filter): {pred}, Max probs: {prob}\")\n        function_logger.debug(f\"Raw prediction {i} (pre-filter): {pred}, Max probs: {prob}\")\n        # Dynamic threshold: 75th percentile of max probs in this sequence\n        threshold = np.percentile(prob, 75)\n        #print(f\"Dynamic threshold for prediction {i}: {threshold:.4f}\")\n        function_logger.debug(f\"Dynamic threshold for prediction {i}: {threshold:.4f}\")\n        pred_text = []\n        prev = -1\n        for idx, p in zip(pred, prob):\n            if idx != 0 and idx != prev and p > threshold:\n                pred_text.append(idx_to_char.get(idx, ''))\n            prev = idx\n        decoded = ''.join(pred_text)\n        texts.append(decoded if decoded else '<empty>')\n    return texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.188705Z","iopub.execute_input":"2025-04-03T15:29:14.188963Z","iopub.status.idle":"2025-04-03T15:29:14.203191Z","shell.execute_reply.started":"2025-04-03T15:29:14.188945Z","shell.execute_reply":"2025-04-03T15:29:14.202303Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Objective function for Optuna with full TensorBoard logging</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"def objective(trial: optuna.trial.Trial):\n    # Global constants\n    global LEARNING_RATE, WEIGHT_DECAY, WARMUP_STEPS, TEMPERATURE\n    global CTC_ENTROPY_WEIGHT, CTC_LABEL_SMOOTHING, CTC_BLANK_PENALTY_WEIGHT\n    global BSD_BEAM_WIDTH, BSD_BLANK_PENALTY, BSD_LENGTH_PENALTY, GRADIENT_NORM_TRESHOLD\n\n    # Suggest hyperparameters to optimize\n    LEARNING_RATE = trial.suggest_float('learning_rate', 1e-9, 5e-7, log=True)\n    WEIGHT_DECAY = trial.suggest_float('weight_decay', 0.05, 0.2, log=True)\n    WARMUP_STEPS = trial.suggest_int('warmup_steps', 1000, 2000)\n    TEMPERATURE = trial.suggest_float('temperature', 0.5, 2.0)\n    CTC_ENTROPY_WEIGHT = trial.suggest_float('ctc_entropy_weight', 0.5, 1.0)\n    CTC_LABEL_SMOOTHING = trial.suggest_float('ctc_label_smoothing', 0.1, 0.2)\n    CTC_BLANK_PENALTY_WEIGHT = trial.suggest_float('ctc_blank_penalty_weight', 0.2, 0.5)\n    BSD_BEAM_WIDTH = trial.suggest_int('bsd_beam_width', 10, 15)\n    BSD_BLANK_PENALTY = trial.suggest_float('bsd_blank_penalty', -0.15, -0.10)\n    BSD_LENGTH_PENALTY = trial.suggest_float('bsd_length_penalty', -0.2, -0.1)\n    GRADIENT_CLIPPING_VALUE = trial.suggest_float('gradient_clipping_value', 1.0, 10.0)\n    GRADIENT_NORM_TRESHOLD = trial.suggest_int('gradient_norm_treshold', 80, 500)\n\n    optuna_logger.info('Optimalization hyperparameters was set.')\n\n    # Suggest model architecture parameters\n    config_model = {\n        'cnn_layers': [\n            {'type': 'conv', 'out_channels': trial.suggest_int('cnn_out_channels_1', 16, 64), 'kernel_size': 3, 'stride': 1, 'padding': 1, 'batchnorm': True, 'activation': 'relu', 'dropout': trial.suggest_float('cnn_dropout_1', 0.1, 0.3)},\n            {'type': 'pool', 'kernel_size': 2, 'stride': 2},\n            {'type': 'conv', 'out_channels': trial.suggest_int('cnn_out_channels_2', 32, 128), 'kernel_size': 3, 'stride': 1, 'padding': 1, 'batchnorm': True, 'activation': 'relu', 'dropout': trial.suggest_float('cnn_dropout_2', 0.1, 0.3)},\n            {'type': 'pool', 'kernel_size': 2, 'stride': 2},\n            {'type': 'conv', 'out_channels': trial.suggest_int('cnn_out_channels_3', 64, 256), 'kernel_size': 3, 'stride': 1, 'padding': 1, 'batchnorm': True, 'activation': 'relu'},\n            {'type': 'pool', 'kernel_size': (2, 2), 'stride': (2, 1), 'padding': (0, 1)},  # CRNN pooling\n            {'type': 'conv', 'out_channels': trial.suggest_int('cnn_out_channels_4', 128, 512), 'kernel_size': 3, 'stride': 1, 'padding': 1, 'batchnorm': True, 'activation': 'relu'},\n            {'type': 'pool', 'kernel_size': (2, 2), 'stride': (2, 1), 'padding': (0, 1)},  # CRNN pooling\n        ],\n        'rnn_type': 'lstm',\n        'rnn_layers': trial.suggest_int('rnn_layers', 1, 3),\n        'hidden_size': trial.suggest_int('hidden_size', 64, 256),\n        'bidirectional': True,\n        'dropout': trial.suggest_float('rnn_dropout', 0.1, 0.3),\n        'fc_layers': [num_chars],  # One layer for OCR\n    }\n\n    optuna_logger.info('Optimalization parameters of neural net model was set.')\n\n    # Initialize the TensorBoard writer for this trial\n    log_dir = os.path.join(TENSORBOARD_DIR, f\"ocr_experiment_trial_{trial.number}\")\n    optuna_logger.info(f\"Start train of trial number: {trial.number}\")\n\n    global writer  # We use a global writer to make it available in train_model and elsewhere\n\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    writer = SummaryWriter(log_dir)\n\n    # Log the config_model settings to TensorBoard\n    config_text = \"\\n\".join([f\"{k}: {v}\" for k, v in config_model.items()])\n    writer.add_text('Config/model', config_text, 0)\n\n    print(f\"Model configuration:\\n {config_model} \\n\")\n    optuna_logger.debug(f\"Model configuration:\\n {config_model} \\n\")\n\n    # Device setup\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Dataset setup (we assume that synthetic data is already generated)\n    full_dataset = OCRDataset(image_dir=OUTPUT_DIR, labels_file=LABELS_FILE)\n    optuna_logger.info(\"Was load synthetic data of dataset.\")\n\n    for i in range(5):\n        img, label, length = full_dataset[i]\n        plt.imshow(img.squeeze(), cmap='gray')\n        plt.title(''.join([idx_to_char.get(idx.item(), '') for idx in label[:length]]))\n        plt.show()\n\n    if len(full_dataset) == 0:\n        print(\"Dataset is empty! Check labels.txt or image directory.\")\n        optuna_logger.error(\"Dataset is empty! Check labels.txt or image directory.\")\n\n        if not font_files:\n            print(\"Download some TrueType fonts (.ttf) and place them in the 'fonts' folder!\")\n            optuna_logger.error(\"Download some TrueType fonts (.ttf) and place them in the 'fonts' folder!\")\n        else:\n            create_synthetic_dataset(NUM_SAMPLES)\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            full_dataset = OCRDataset(image_dir=OUTPUT_DIR, labels_file=LABELS_FILE)\n    else:\n        for i in range(5):\n            img, label, length = full_dataset[i]\n            plt.imshow(img.squeeze(), cmap='gray')\n            plt.title(''.join([idx_to_char.get(idx.item(), '') for idx in label[:length]]))\n            plt.show()\n\n        optuna_logger.info(\"Samples of data was shown.\")\n        \n        # Curriculum phases with pre-filtering\n        model = OCRModel(config=config_model).to(device)\n        criterion = CTCLossWithBlankPenalty(\n            blank=0, zero_infinity=True, blank_penalty_weight=CTC_BLANK_PENALTY_WEIGHT,\n            entropy_weight=CTC_ENTROPY_WEIGHT, label_smoothing=CTC_LABEL_SMOOTHING\n        )\n\n        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n\n        optuna_logger.info(\"Model, Criterion, Optimizer, Scheduler was set.\")\n        \n        best_val_loss = float('inf')\n        avg_grad_norm = 0\n        avg_blank_probs = 0\n        avg_loss = 0  # Initialize avg_loss\n\n        for epoch in range(EPOCHS):\n            optuna_logger.info(f\"Start epoch number: {epoch}\")\n            label_lengths = [lbl_len for _, _, lbl_len in full_dataset.data]\n            #print(f\"Label lengths: {label_lengths}\")\n            optuna_logger.debug(f\"Label lengths: {label_lengths}\")\n            # Filter full dataset based on curriculum phase\n            if epoch < 5:\n                filtered_data = [(img, lbl, lbl_len) for img, lbl, lbl_len in full_dataset.data if lbl_len <= 10]\n                sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]  # lbl is a string\n                #print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n                optuna_logger.debug(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n            elif epoch < 10:\n                filtered_data = [(img, lbl, lbl_len) for img, lbl, lbl_len in full_dataset.data if lbl_len <= 15]\n                sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]\n                #print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n                optuna_logger.debug(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n            else:\n                filtered_data = full_dataset.data\n                sample_labels = [lbl for _, lbl, _ in filtered_data[:3]]\n                #print(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n                optuna_logger.debug(f\"Epoch {epoch+1}, Filtered data size: {len(filtered_data)}, Sample labels: {sample_labels}\")\n\n            # Create a new dataset with filtered data\n            curr_dataset = OCRDataset(image_dir=OUTPUT_DIR, labels_file=LABELS_FILE)\n            curr_dataset.data = filtered_data  # Overwrite with filtered data\n\n            # Split into train and validation\n            train_size = int(0.8 * len(curr_dataset))\n            val_size = len(curr_dataset) - train_size\n            train_dataset, val_dataset = torch.utils.data.random_split(curr_dataset, [train_size, val_size])\n            #print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n            optuna_logger.debug(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n\n            # Create data loaders\n            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n\n            skipped_batches = 0\n\n            # Training with TensorBoard logging\n            model.train()\n            optuna_logger.info('Start Train process.')\n            total_loss = 0\n            total_grad_norm = 0\n            total_blank_probs = 0\n            max_skipped_batches = max(1, int(0.1 * len(train_loader)))  # Allow up to 10% of batches to be skipped\n            avg_loss = 0  # Track average loss for dynamic threshold\n            num_valid_batches = 0\n            \n            global_step = epoch * len(train_loader)\n            \n            for batch_idx, (imgs, labels, label_lengths) in enumerate(train_loader):\n                imgs, labels = imgs.to(device), labels.to(device)\n                label_lengths = label_lengths.to(device)\n\n                if global_step < WARMUP_STEPS:\n                    lr_scale = min(1.0, float(global_step + 1) / WARMUP_STEPS)\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = LEARNING_RATE * lr_scale\n                        optuna_logger.debug(f\"Learning rate set at value: {param_group['lr']}\")\n\n                optimizer.zero_grad()\n                max_label_length = label_lengths.max().item()\n                outputs = model(imgs, max_length=max_label_length * 2)\n                outputs = outputs / TEMPERATURE\n                outputs = torch.clamp(outputs, min=-10, max=10)  # Clipping for stability\n                outputs = outputs.log_softmax(2)\n\n                batch_size = imgs.size(0)\n                seq_length = min(outputs.size(0), max_label_length * 2)\n                input_lengths = torch.full((batch_size,), seq_length, dtype=torch.long).to(device)\n\n                # Dynamic penalty for blank tokens\n                blank_probs = outputs[:, :, 0].exp().mean().item()\n                dynamic_blank_penalty = CTC_BLANK_PENALTY_WEIGHT * (1 + blank_probs)\n                criterion = CTCLossWithBlankPenalty(\n                    blank=0, zero_infinity=True, blank_penalty_weight=dynamic_blank_penalty,\n                    entropy_weight=CTC_ENTROPY_WEIGHT, label_smoothing=CTC_LABEL_SMOOTHING\n                )\n\n                loss = criterion(outputs, labels, input_lengths, label_lengths)\n\n                # Calculating the gradient norm before clipping\n                grad_norm_before = compute_gradient_norm(model.parameters())\n\n                # Dynamic threshold based on average loss of previous batches\n                dynamic_threshold = max(40, avg_loss * 2) if num_valid_batches > 0 else 40  # Dynamic threshold\n                \n                if torch.isnan(loss) or torch.isinf(loss):\n                    #print(f\"Warning: NaN or Inf loss at batch {batch_idx}. Skipping...\")\n                    optuna_logger.warning(f\"Warning: NaN or Inf loss at batch {batch_idx}. Skipping...\")\n                    writer.add_scalar('Skipped_Batches/NaN_Inf', 1, global_step)\n                    skipped_batches += 1\n                elif grad_norm_before > GRADIENT_NORM_TRESHOLD:  # Gradient norm threshold\n                    #print(f\"Warning: High gradient norm {grad_norm_before:.4f} at batch {batch_idx}. Skipping...\")\n                    optuna_logger.warning(f\"Warning: High gradient norm {grad_norm_before:.4f} at batch {batch_idx}. Skipping...\")\n                    writer.add_scalar('Skipped_Batches/High_Gradient', 1, global_step)\n                    skipped_batches += 1\n                elif loss.item() > dynamic_threshold:\n                    #print(f\"Warning: High loss {loss.item():.4f} at batch {batch_idx}. Adjusting learning rate...\")\n                    optuna_logger.warning(f\"Warning: High loss {loss.item():.4f} at batch {batch_idx}. Adjusting learning rate...\")\n                    # Gradually reduce the learning rate\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] *= 0.9  # Reduce learning rate by 10%\n                        optuna_logger.debug(f\"Learning rate set at value: {param_group['lr']}\")\n                    writer.add_scalar('Skipped_Batches/High_Loss', 1, global_step)\n                    skipped_batches += 1\n                else:\n                    loss.backward()\n\n                    # Gradient clipping with a reasonable max_norm value\n                    grad_norm_after = nn_utils.clip_grad_norm_(model.parameters(), max_norm=GRADIENT_CLIPPING_VALUE)\n\n                    # Logging gradient norms\n                    #print(f\"Epoch {epoch+1}, Batch {batch_idx}, Gradient Norm Before: {grad_norm_before:.4f}, After: {grad_norm_after:.4f}\")\n                    optuna_logger.debug(f\"Epoch {epoch+1}, Batch {batch_idx}, Gradient Norm Before: {grad_norm_before:.4f}, After: {grad_norm_after:.4f}\")\n                    writer.add_scalar('Gradient_Norm/train_batch', grad_norm_after.item(), global_step)\n                \n                    optimizer.step()\n                    total_loss += loss.item()\n                    total_grad_norm += grad_norm_after.item()\n                    total_blank_probs += blank_probs\n                    num_valid_batches += 1\n                    avg_loss = total_loss / num_valid_batches if num_valid_batches > 0 else 0\n\n                \n                # Prune trial if too many batches are skipped\n                if skipped_batches > max_skipped_batches:\n                    #print(f\"Too many skipped batches ({skipped_batches}/{max_skipped_batches}) in epoch {epoch+1}. Pruning trial.\")\n                    optuna_logger.warning(f\"Too many skipped batches ({skipped_batches}/{max_skipped_batches}) in epoch {epoch+1}. Pruning trial.\")\n                    writer.add_text(\n                        'Pruning/Details',\n                        f\"Pruned at epoch {epoch+1}, batch {batch_idx}. Skipped batches: {skipped_batches}, \"\n                        f\"Input lengths: {input_lengths.tolist()}, Label lengths: {label_lengths.tolist()}\",\n                        global_step\n                    )\n                    optuna_logger.debug(f'Trial {trial.number} break in epoch {epoch+1}')\n                    raise optuna.TrialPruned()\n                \n                global_step += 1\n\n                \n                # Log additional metrics\n                if batch_idx % 10 == 0:\n                    with torch.no_grad():\n                        pred_texts = beam_search_decode(\n                                        output=outputs,\n                                        idx_to_char=idx_to_char,\n                                        target_lengths=label_lengths,\n                                        beam_width=BSD_BEAM_WIDTH,\n                                        blank_penalty=BSD_BLANK_PENALTY,\n                                        length_penalty=BSD_LENGTH_PENALTY\n                                    )\n                        raw_outputs = outputs.argmax(2).cpu().numpy()[:3]\n                        blank_probs = outputs[:, :, 0].exp().mean().item()\n                        label_sequences = split_labels(labels, label_lengths)\n                        ground_truth = [''.join([idx_to_char.get(idx.item(), '') for idx in label_seq])\n                                        for label_seq in label_sequences[:3]]\n                        \n                        probs = outputs.exp().flatten()\n                        #print(f\"Probs min: {probs.min().item()}, max: {probs.max().item()}, mean: {probs.mean().item()}\")\n                        optuna_logger.debug(f\"Probs min: {probs.min().item()}, max: {probs.max().item()}, mean: {probs.mean().item()}\")\n                        #print(f\"Probs unique values: {torch.unique(probs).numel()}\")\n                        optuna_logger.debug(f\"Probs unique values: {torch.unique(probs).numel()}\")\n                        if torch.isnan(probs).any() or torch.isinf(probs).any():\n                            #print(\"Warning: NaN or Inf values in probs\")\n                            optuna_logger.warning(\"Warning: NaN or Inf values in probs\")\n\n                        # Logging into TensorBoard\n                        writer.add_scalar('Loss/train_batch', loss.item(), global_step)\n                        writer.add_scalar('Blank_Probability/train_batch', blank_probs, global_step)\n                        writer.add_scalar('Gradient_Norm/train_batch', grad_norm_after.item(), global_step)\n                        \n                        if torch.isnan(probs).any() or torch.isinf(probs).any() or torch.unique(probs).numel() <= 1:\n                            #print(\"Skipping histogram logging due to invalid or degenerate data\")\n                            optuna_logger.warning(\"Skipping histogram logging due to invalid or degenerate data\")\n                        else:\n                            writer.add_histogram('Logits/train_probs', probs, global_step)\n\n                        # Adding raw outputs\n                        writer.add_histogram('Raw_Outputs/train_argmax', raw_outputs.flatten(), global_step)\n                        writer.add_text('Raw_Outputs/train_text', f\"Raw train outputs (argmax):\\n{str(raw_outputs)}\", global_step)\n\n                        # Token frequency calculation and logging\n                        token_counts = Counter(raw_outputs.flatten())\n                        writer.add_text(\n                            'Raw_Outputs/train_token_counts',\n                            f\"Token counts: {token_counts}\",\n                            global_step\n                        )\n\n                        #print(f\"Batch {batch_idx}, Gradient norm: {grad_norm_after.item():.4f}\")\n                        optuna_logger.debug(f\"Batch {batch_idx}, Gradient norm: {grad_norm_after.item():.4f}\")\n                        #print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n                        optuna_logger.debug(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n                        #print(f\"Avg Blank Probability: {blank_probs:.4f}\")\n                        optuna_logger.debug(f\"Avg Blank Probability: {blank_probs:.4f}\")\n                        #print(f\"Sample predictions: {pred_texts[:3]}\")\n                        optuna_logger.debug(f\"Sample predictions: {pred_texts[:3]}\")\n                        #print(f\"Ground Truth (first 3): {ground_truth}\")\n                        optuna_logger.debug(f\"Ground Truth (first 3): {ground_truth}\")\n                        #print(f\"Raw outputs (first 3): {raw_outputs}\")\n                        optuna_logger.debug(f\"Raw outputs (first 3): {raw_outputs}\")\n                        #print(f\"Input length: {seq_length}, Label lengths: {label_lengths[:3].tolist()}\")\n                        optuna_logger.debug(f\"Input length: {seq_length}, Label lengths: {label_lengths[:3].tolist()}\")\n                        #print(f\"Number of skipped batches: {skipped_batches}\")\n                        optuna_logger.debug(f\"Number of skipped batches: {skipped_batches}\")\n\n            # Compute epoch averages\n            if num_valid_batches == 0:\n                #print(\"All batches skipped in this epoch. Pruning trial.\")\n                optuna_logger.warning(\"All batches skipped in this epoch. Pruning trial.\")\n                raise optuna.TrialPruned()\n\n            avg_loss = total_loss / len(train_loader)\n            avg_grad_norm = total_grad_norm / len(train_loader)\n            avg_blank_probs = total_blank_probs / len(train_loader)\n\n            writer.add_scalar('Loss/train_epoch', avg_loss, epoch)\n            writer.add_scalar('Gradient_Norm/train_epoch', avg_grad_norm, epoch)\n            writer.add_scalar('Blank_Probability/train_epoch', avg_blank_probs, epoch)\n            print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n            optuna_logger.info(f\"Train process finished: Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n\n            # Validation with TensorBoard logging\n            model.eval()\n            optuna_logger.info('Start Evaluation process.')\n            val_loss = 0\n            val_blank_probs = 0\n            with torch.no_grad():\n                for batch_idx, (imgs, labels, label_lengths) in enumerate(val_loader):\n                    imgs, labels = imgs.to(device), labels.to(device)\n                    label_lengths = label_lengths.to(device)\n                    outputs = model(imgs)\n                    outputs = outputs.log_softmax(2)\n                    seq_length = outputs.size(0)\n                    input_lengths = torch.full((imgs.size(0),), seq_length, dtype=torch.long).to(device)\n                    val_loss += criterion(outputs, labels, input_lengths, label_lengths).item()\n                    val_blank_probs += outputs[:, :, 0].exp().mean().item()\n\n                    # Logging raw outputs for the first batch\n                    if batch_idx == 0:  # We only log the first batch to save space\n\n                        # Control of the form and content of outputs\n                        #print(f\"Outputs shape: {outputs.shape}, Outputs[0] shape: {outputs[0].shape}\")\n                        optuna_logger.debug(f\"Outputs shape: {outputs.shape}, Outputs[0] shape: {outputs[0].shape}\")\n                        if outputs.numel() == 0 or torch.isnan(outputs).any() or torch.isinf(outputs).any():\n                            #print(\"Warning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\")\n                            optuna_logger.warning(\"Warning: Outputs contains invalid values (empty, NaN, or Inf). Skipping histogram logging.\")\n                            continue\n\n                        # 1. Histogram of the distribution of predicted tokens (argmax)\n                        raw_outputs = outputs.argmax(2).cpu().numpy()  # [seq_length, batch_size]\n                        writer.add_histogram(\n                            'Raw_Outputs/val_argmax',\n                            raw_outputs.flatten(),  # We convert to a 1D array for the histogram\n                            global_step=epoch\n                        )\n\n                        # 2. Text listing of raw outputs (first 5 sequences)\n                        raw_outputs_text = str(raw_outputs[:5])  # First 5 sequences as text\n                        writer.add_text(\n                            'Raw_Outputs/val_text',\n                            f\"Raw validation outputs (argmax) for first batch:\\n{raw_outputs_text}\",\n                            global_step=epoch\n                        )\n\n                        # 3. Calculating token frequency and logging to TensorBoard\n                        token_counts = Counter(raw_outputs.flatten())\n                        writer.add_text(\n                            'Raw_Outputs/val_token_counts',\n                            f\"Token counts: {token_counts}\",\n                            global_step=epoch\n                        )\n\n                        # 4. (Optional) Logit histogram for specific tokens (e.g. first time step)\n                        logits_first_step = outputs[0].cpu().numpy()  # [batch_size, num_chars]\n                        writer.add_histogram(\n                            'Raw_Outputs/val_logits_first_step',\n                            logits_first_step.flatten(),\n                            global_step=epoch\n                        )\n\n                        logits_first_step = outputs[0].cpu().numpy()\n                        writer.add_histogram('Logits/val_probs', torch.softmax(outputs[0], dim=-1).flatten(), global_step)\n\n                val_loss /= len(val_loader)\n                val_blank_probs /= len(val_loader)\n                pred_texts = beam_search_decode(\n                                output=outputs,\n                                idx_to_char=idx_to_char,\n                                target_lengths=label_lengths,\n                                beam_width=BSD_BEAM_WIDTH,\n                                blank_penalty=BSD_BLANK_PENALTY,\n                                length_penalty=BSD_LENGTH_PENALTY\n                            )\n                label_sequences = split_labels(labels, label_lengths)\n                ground_truth = [''.join([idx_to_char.get(idx.item(), '') for idx in label_seq])\n                                for label_seq in label_sequences[:5]]\n\n                # Calculating edit distance\n                edit_distances = []\n                for pred, gt in zip(pred_texts, ground_truth):\n                    edit_distances.append(levenshtein_distance(pred, gt))\n                avg_edit_distance = sum(edit_distances) / len(edit_distances) if edit_distances else 0\n\n                # Validation logging to TensorBoard\n                writer.add_scalar('Loss/val_epoch', val_loss, epoch)\n                writer.add_scalar('Blank_Probability/val_epoch', val_blank_probs, epoch)\n                writer.add_text('Predictions/val', f\"Validation Predictions: {pred_texts[:5]}\", epoch)\n                writer.add_text('Ground_Truth/val', f\"Ground Truth: {ground_truth}\", epoch)\n                #print(f\"Validation Loss: {val_loss:.4f}\")\n                optuna_logger.debug(f\"Validation Loss: {val_loss:.4f}\")\n                #print(\"Validation Predictions:\", pred_texts[:5])\n                optuna_logger.debug(f\"Validation Predictions: {pred_texts[:5]}\")\n                #print(\"Ground Truth:\", ground_truth)\n                optuna_logger.debug(f\"Ground Truth: {ground_truth}\")\n\n            scheduler.step(val_loss)\n\n            current_lr = scheduler.get_last_lr()[0]  # get_last_lr() returns a list.\n            #print(f\"Current Learning Rate: {current_lr}\")\n            optuna_logger.debug(f\"Current Learning Rate: {current_lr}\")\n\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(model.state_dict(), os.path.join(MODEL_DIR, f'best_ocr_model_{trial.number}.pth'))\n                optuna_logger.info(f\"Best model was saved: best_ocr_model_{trial.number}.pth\")\n\n            optuna_logger.info(f\"Validation process finished: Epoch {epoch+1}/{EPOCHS}, Loss: {val_loss:.4f}\")\n            \n        torch.save(model.state_dict(), os.path.join(MODEL_DIR, f'final_ocr_model_{trial.number}.pth'))\n        optuna_logger.info(f\"Final model was saved: final_ocr_model_{trial.number}.pth\")\n\n    avg_grad_norm /= EPOCHS\n    avg_blank_probs /= EPOCHS\n\n    # Closing the TensorBoard writer\n    writer.close()\n\n    return best_val_loss, avg_grad_norm, avg_blank_probs  # Metric to minimize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.204031Z","iopub.execute_input":"2025-04-03T15:29:14.204324Z","iopub.status.idle":"2025-04-03T15:29:14.243920Z","shell.execute_reply.started":"2025-04-03T15:29:14.204306Z","shell.execute_reply":"2025-04-03T15:29:14.242951Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **<font style=\"color:green\">Function to run Optuna Dashboard in the background</font>**","metadata":{}},{"cell_type":"code","source":"def run_optuna_dashboard(storage_path):\n    print(f\"Starting Optuna Dashboard with storage: {storage_path}\")\n    os.system(f\"optuna-dashboard sqlite:///{storage_path} --host 0.0.0.0 --port 8080 &\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.244719Z","iopub.execute_input":"2025-04-03T15:29:14.245183Z","iopub.status.idle":"2025-04-03T15:29:14.258620Z","shell.execute_reply.started":"2025-04-03T15:29:14.245152Z","shell.execute_reply":"2025-04-03T15:29:14.257952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Synthetic data generation (same as originally)\n    if not font_files:\n        print(\"Download some TrueType fonts (.ttf) and place them in the 'fonts' folder!\")\n        main_logger.error(\"Download some TrueType fonts (.ttf) and place them in the 'fonts' folder!\")\n    else:\n        create_synthetic_dataset(NUM_SAMPLES)\n\n    # Path to save the Optuna database\n    storage_path = os.path.join(OPTUNA_DIR, 'optuna_study.db')\n    storage = f\"sqlite:///{storage_path}\"\n    main_logger.info(\"Set database for Optuna optimalization.\")\n\n    # Multi-objective study\n    study = optuna.create_study(\n        directions=[\"minimize\", \"minimize\", \"minimize\"],  # Validation loss, grad_norm, blank_probs\n        storage=storage,\n        study_name=\"ocr_multiobjective\",\n        load_if_exists=True\n    )\n    main_logger.info(\"Set & Run Optuna optimalization.\")\n    study.optimize(objective, n_trials=NUMBER_OF_OPTUNA_TRIALS) # You can adjust the number of trials\n\n    #print(\"Best trials (Pareto front):\")\n    main_logger.info(\"Best trials (Pareto front):\")\n    for trial in study.best_trials:\n        #print(f\"  Trial {trial.number}:\")\n        main_logger.debug(f\"Trial {trial.number}:\")\n        #print(f\"    Values: val_loss={trial.values[0]}, grad_norm={trial.values[1]}, blank_probs={trial.values[2]}\")\n        main_logger.debug(f\"Values: val_loss={trial.values[0]}, grad_norm={trial.values[1]}, blank_probs={trial.values[2]}\")\n        #print(f\"    Params: {trial.params}\")\n        main_logger.debug(f\"Params: {trial.params}\")\n    \n    # Running Optuna Dashboard in the background\n    dashboard_thread = threading.Thread(target=run_optuna_dashboard, args=(storage_path,))\n    dashboard_thread.start()\n\n    # Close the file after logging is complete\n    rotating_json_file_handler.close()\n    \n","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-04-03T15:29:14.259467Z","iopub.execute_input":"2025-04-03T15:29:14.259752Z","iopub.status.idle":"2025-04-03T20:05:50.635689Z","shell.execute_reply.started":"2025-04-03T15:29:14.259723Z","shell.execute_reply":"2025-04-03T20:05:50.634662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def zip_folder_with_shutil(source_folder, output_path):\n    '''Function for zip dir data'''\n    shutil.make_archive(output_path, 'zip', source_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T20:05:50.636990Z","iopub.execute_input":"2025-04-03T20:05:50.637382Z","iopub.status.idle":"2025-04-03T20:05:50.642397Z","shell.execute_reply.started":"2025-04-03T20:05:50.637346Z","shell.execute_reply":"2025-04-03T20:05:50.641571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_folder_with_shutil('/kaggle/working/backgrounds', '/kaggle/working/backgrounds')\nzip_folder_with_shutil('/kaggle/working/synthetic_data', '/kaggle/working/synthetic_data')\nzip_folder_with_shutil('/kaggle/working/model_dir', '/kaggle/working/model_dir')\nzip_folder_with_shutil('/kaggle/working/runs', '/kaggle/working/runs')\nzip_folder_with_shutil('/kaggle/working/optuna', '/kaggle/working/optuna')\nzip_folder_with_shutil('/kaggle/working/logs', '/kaggle/working/logs')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T20:05:50.643329Z","iopub.execute_input":"2025-04-03T20:05:50.643684Z","iopub.status.idle":"2025-04-03T20:06:14.757954Z","shell.execute_reply.started":"2025-04-03T20:05:50.643634Z","shell.execute_reply":"2025-04-03T20:06:14.757066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!tensorboard --logdir=/kaggle/working/runs --port 6006\n#!optuna-dashboard sqlite:///kaggle/working/optuna/optuna_study.db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T20:06:14.758941Z","iopub.execute_input":"2025-04-03T20:06:14.759283Z","iopub.status.idle":"2025-04-03T20:06:14.762836Z","shell.execute_reply.started":"2025-04-03T20:06:14.759243Z","shell.execute_reply":"2025-04-03T20:06:14.762016Z"}},"outputs":[],"execution_count":null}]}