{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6863003,"sourceType":"datasetVersion","datasetId":3944299}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **<font style=\"color:black\">Sequence to Sequence text generation by PyTorch (seq2seq)</font>**\n-------------------\n\n>Note: Apply it to machine translation on a dataset with German to English sentences, specifically the Multi30k dataset.","metadata":{}},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Installation and import libraries</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"!pip install spacy\n!pip install tokenizers\n!pip install sacrebleu\n!pip install tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport spacy\nimport random\nfrom torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\nfrom torch.utils.data import Dataset, DataLoader\nfrom sacrebleu import corpus_bleu\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tqdm import tqdm  # Import tqdm for the progress bar\n\n%matplotlib inline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<font style=\"color:blue\">Utils support function</font>**\n-------------------","metadata":{}},{"cell_type":"code","source":"!python -m spacy download de_core_news_sm\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load spacy models for German and English\nspacy_ger = spacy.load(\"de_core_news_sm\")\nspacy_eng = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_ger(text):\n    return [tok.text for tok in spacy_ger.tokenizer(text)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_eng(text):\n    return [tok.text for tok in spacy_eng.tokenizer(text)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_sentence(model, sentence, german_vocab, english_vocab, device, max_length=50):\n    model.eval()\n\n    tokens = [token.lower() for token in sentence]\n    tokens = [german_vocab.sos_token] + tokens + [german_vocab.eos_token]\n    indices = [german_vocab[token] for token in tokens]\n    sentence_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)  # (1, seq_len)\n\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)  # (num_layers, 1, hidden_size)\n        print(f\"hidden shape: {hidden.shape}, cell shape: {cell.shape}\")\n\n    outputs = [english_vocab[english_vocab.sos_token]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)  # (1,)\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n        outputs.append(best_guess)\n        if best_guess == english_vocab[english_vocab.eos_token]:\n            break\n\n    translated_sentence = [english_vocab.lookup_token(idx) for idx in outputs]\n    return translated_sentence[1:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bleu_score(data, model, german_vocab, english_vocab, device):\n    targets = []\n    outputs = []\n\n    for src, trg in data:\n        prediction = translate_sentence(model, src, german_vocab, english_vocab, device)\n        prediction = prediction[:-1]  # Remove <eos> token\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return corpus_bleu(outputs, [targets])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"/kaggle/working/my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, tokens=None):\n        self.token_to_idx = {}\n        self.idx_to_token = []\n        self.special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n\n        # Add special tokens to the vocabulary\n        for token in self.special_tokens:\n            self.add_token(token)\n\n        if tokens:\n            self.build_vocab(tokens)\n\n        # Set attributes for special tokens\n        self.pad_token = \"<pad>\"\n        self.sos_token = \"<sos>\"\n        self.eos_token = \"<eos>\"\n        self.unk_token = \"<unk>\"\n\n    def build_vocab(self, tokens, min_freq=2, max_size=10000):\n        token_counts = Counter(tokens)\n        for token, count in token_counts.items():\n            if count >= min_freq:\n                self.add_token(token)\n                if len(self.token_to_idx) >= max_size:\n                    break\n\n    def add_token(self, token):\n        if token not in self.token_to_idx:\n            self.token_to_idx[token] = len(self.idx_to_token)\n            self.idx_to_token.append(token)\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, token):\n        return self.token_to_idx.get(token, self.token_to_idx[self.unk_token])\n\n    def lookup_token(self, idx):\n        return self.idx_to_token[idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Multi30kDataset(Dataset):\n    def __init__(self, src_path, trg_path, german_vocab, english_vocab):\n        self.src_sentences = self.load_data(src_path)\n        self.trg_sentences = self.load_data(trg_path)\n        self.german_vocab = german_vocab\n        self.english_vocab = english_vocab\n\n    def load_data(self, data_path):\n        with open(data_path, 'r', encoding='utf-8') as file:\n            return file.readlines()\n\n    def __len__(self):\n        return len(self.src_sentences)\n\n    def __getitem__(self, idx):\n        src = self.src_sentences[idx].strip()\n        trg = self.trg_sentences[idx].strip()\n        src_tokens = tokenize_ger(src)\n        trg_tokens = tokenize_eng(trg)\n        src_indices = [self.german_vocab[token] for token in src_tokens]\n        trg_indices = [self.english_vocab[token] for token in trg_tokens]\n        return torch.tensor(src_indices), torch.tensor(trg_indices)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch, pad_idx):\n    src_batch, trg_batch = zip(*batch)\n    src_batch = pad_sequence(src_batch, padding_value=pad_idx)\n    trg_batch = pad_sequence(trg_batch, padding_value=pad_idx)\n    return src_batch, trg_batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\ntrain_src_path = os.path.join('/kaggle','input','multi30k-de-en','training','train.de')\ntrain_trg_path = os.path.join('/kaggle','input','multi30k-de-en','training','train.en')\nvalid_src_path = os.path.join('/kaggle','input','multi30k-de-en','validation','val.de')\nvalid_trg_path = os.path.join('/kaggle','input','multi30k-de-en','validation','val.en')\ntest_src_path = os.path.join('/kaggle','input','multi30k-de-en','mmt16_task1_test','test.de')\ntest_trg_path = os.path.join('/kaggle','input','multi30k-de-en','mmt16_task1_test','test.en')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build vocabularies\ngerman_tokens_train = []\nenglish_tokens_train = []\ngerman_tokens_valid = []\nenglish_tokens_valid = []\ngerman_tokens_test = []\nenglish_tokens_test = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(train_src_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        german_tokens_train.extend(tokenize_ger(line.strip()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(train_trg_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        english_tokens_train.extend(tokenize_eng(line.strip()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(valid_src_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        german_tokens_valid.extend(tokenize_ger(line.strip()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(valid_trg_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        english_tokens_valid.extend(tokenize_eng(line.strip()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(test_src_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        german_tokens_test.extend(tokenize_ger(line.strip()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(test_trg_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        english_tokens_test.extend(tokenize_eng(line.strip()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"german_vocab_train = Vocabulary()\nenglish_vocab_train = Vocabulary()\ngerman_vocab_valid = Vocabulary()\nenglish_vocab_valid = Vocabulary()\ngerman_vocab_test = Vocabulary()\nenglish_vocab_test = Vocabulary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"german_vocab_train.build_vocab(german_tokens_train)\nenglish_vocab_train.build_vocab(english_tokens_train)\ngerman_vocab_valid.build_vocab(german_tokens_valid)\nenglish_vocab_valid.build_vocab(english_tokens_valid)\ngerman_vocab_test.build_vocab(german_tokens_test)\nenglish_vocab_test.build_vocab(english_tokens_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training hyperparameters\nnum_epochs = 80\nlearning_rate = 0.001\nbatch_size = 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model hyperparameters\nload_model = False\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_size_encoder = len(german_vocab_train)\ninput_size_decoder = len(english_vocab_train)\noutput_size = len(english_vocab_train)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create data loaders\ntrain_dataset = Multi30kDataset(train_src_path, train_trg_path, german_vocab_train, english_vocab_train)\nvalid_dataset = Multi30kDataset(valid_src_path, valid_trg_path, german_vocab_valid, english_vocab_valid)\ntest_dataset = Multi30kDataset(test_src_path, test_trg_path, german_vocab_test, english_vocab_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the length of the dataset\nprint(f\"Number of samples in train dataset: {len(train_dataset)}\")\nprint(f\"Number of samples in train dataset: {len(valid_dataset)}\")\nprint(f\"Number of samples in train dataset: {len(test_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets_list = [train_dataset, valid_dataset, test_dataset]\nnames_list = ['train dataset', 'validation dataset', 'test dataset']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect a few samples\nfor p, dataset in enumerate(datasets_list):\n    print(f'Show {names_list[p]} samples.\\n')\n    for i in range(min(5, len(dataset))):\n        src, trg = train_dataset[i]\n        print(f\"Source: {src}\")\n        print(f\"Target: {trg}\")\n    print(100*'-')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_idx=german_vocab_train[\"<pad>\"]))\nvalid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_idx=german_vocab_valid[\"<pad>\"]))\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_idx=german_vocab_test[\"<pad>\"]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for batch_idx, (src, trg) in enumerate(train_loader):\n    print(f\"Batch {batch_idx}: src shape={src.shape}, trg shape={trg.shape}\")\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for batch_idx, (src, trg) in enumerate(valid_loader):\n    print(f\"Batch {batch_idx}: src shape={src.shape}, trg shape={trg.shape}\")\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for batch_idx, (src, trg) in enumerate(test_loader):\n    print(f\"Batch {batch_idx}: src shape={src.shape}, trg shape={trg.shape}\")\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch, pad_idx):\n    src_batch, trg_batch = zip(*batch)\n    src_batch = pad_sequence(src_batch, padding_value=pad_idx, batch_first=True)\n    trg_batch = pad_sequence(trg_batch, padding_value=pad_idx, batch_first=True)\n    \n    max_len = max(src_batch.size(1), trg_batch.size(1))\n    \n    if src_batch.size(1) < max_len:\n        src_padding = torch.full((src_batch.size(0), max_len - src_batch.size(1)), pad_idx, dtype=torch.long)\n        src_batch = torch.cat([src_batch, src_padding], dim=1)\n    \n    if trg_batch.size(1) < max_len:\n        trg_padding = torch.full((trg_batch.size(0), max_len - trg_batch.size(1)), pad_idx, dtype=torch.long)\n        trg_batch = torch.cat([trg_batch, trg_padding], dim=1)\n\n    return src_batch, trg_batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n        return hidden, cell","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden, cell):\n        # x: (batch_size,) e.g., (1,)\n        x = x.unsqueeze(1)  # (batch_size, 1) e.g., (1, 1)\n        embedding = self.dropout(self.embedding(x))  # (batch_size, 1, embedding_size) e.g., (1, 1, embedding_size)\n        #print(f\"embedding shape: {embedding.shape}\")\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))  # outputs: (batch_size, 1, hidden_size)\n        #print(f\"outputs shape: {outputs.shape}\")\n        predictions = self.fc(outputs.squeeze(1))  # (batch_size, output_size)\n        return predictions, hidden, cell","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = source.shape[0]\n        target_len = target.shape[1]\n        target_vocab_size = len(english_vocab_train)  # Adjust based on your vocabulary\n\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n\n        hidden, cell = self.encoder(source)  # (batch_size, seq_len) -> (num_layers, batch_size, hidden_size)\n        x = target[:, 0]  # (batch_size,)\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[:, t, :] = output\n            best_guess = output.argmax(1)\n            x = target[:, t] if random.random() < teacher_force_ratio else best_guess\n\n        return outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\ndecoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Seq2Seq(encoder_net, decoder_net, device).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In your model initialization\nfor name, param in model.named_parameters():\n    if param.numel() == 0:\n        print(f\"Warning: Zero-element tensor detected in parameter '{name}'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pad_idx = english_vocab_train[\"<pad>\"]\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if load_model:\n    load_checkpoint(torch.load(\"/kaggle/working/my_checkpoint.pth.tar\"), model, optimizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_sentence(model, sentence, german_vocab, english_vocab, device, max_length=50):\n    model.eval()  # Ensure evaluation mode\n\n    # Tokenize and convert sentence to tensor\n    tokens = [token.lower() for token in sentence]\n    tokens = [german_vocab.sos_token] + tokens + [german_vocab.eos_token]\n    indices = [german_vocab[token] for token in tokens]\n    # Correct shape to (batch_size, seq_len)\n    sentence_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n\n    # Encode sentence\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)  # Shape: (num_layers, 1, hidden_size)\n\n    outputs = [english_vocab[english_vocab.sos_token]]\n\n    for _ in range(max_length):\n        # Correct shape to (batch_size,)\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)  # Shape: (1,)\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n        outputs.append(best_guess)\n        if best_guess == english_vocab[english_vocab.eos_token]:\n            break\n\n    translated_sentence = [english_vocab.lookup_token(idx) for idx in outputs]\n    return translated_sentence[1:]  # Exclude start token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage with a small batch\nexample_batch = [train_dataset[i] for i in range(2)]  # Get a small batch for testing\ncollated_batch = collate_fn(example_batch, pad_idx=german_vocab_train[\"<pad>\"])\nprint(f\"Collated source batch shape: {collated_batch[0].shape}\")\nprint(f\"Collated target batch shape: {collated_batch[1].shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n    translated_sentence = translate_sentence(model, sentence, german_vocab_train, english_vocab_train, device)\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n    for batch_idx, (inp_data, target) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch}\", leave=True)):\n        inp_data, target = inp_data.to(device), target.to(device)\n        #print(f\"inp_data shape: {inp_data.shape}, target shape: {target.shape}\")\n        output = model(inp_data, target)\n        #print(f\"output shape: {output.shape}\")\n        \n        output = output[:, 1:].reshape(-1, output.shape[-1])  # Skip <sos>\n        target = target[:, 1:].reshape(-1)  # Skip <sos>\n        #print(f\"output_flat shape: {output.shape}, target_flat shape: {target.shape}\")\n        \n        optimizer.zero_grad()\n        loss = criterion(output, target)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n        optimizer.step()\n        \n        writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n        step += 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"score = bleu_score(test_dataset, model, german_vocab_train, english_vocab_train, device)\nprint(f\"Bleu score {score * 100:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}